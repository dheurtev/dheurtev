# 1980s 

**In short:**
- Telecommunications : T3 circuits, single-mode optical fibre and cable, ISDN, PON idea, ISPs
- Networking : TCP/IP emergence (IPv4), Ethernet (IEEE 802.3), original standard for Ethernet over fiber (FOIRL), First RFID related patent, RPC, BBS, NSFNET and first internet access
- Cryptography : Age of RSA 
- Computers :
  * Hardware : PC industry, GUI + Mouse
  * CPU : x86 era and IA-32(i386) architecture
  * Motherboard : from published schematic diagrams to single Integrated Circuits with PS/2, ISA, SCSI, PATA/ATA/IDE, UART
  * Storage : 3"1/2 floppy disk, IDE/SCSI hard Drive
  * Screens : SVGA
  * Printers: Color inkjet and laser
- Uses :
  * Servers : DNS, BBS
  * Clients : IBM DOS (Microsft MS DOS), Apple MacOS, Microsoft Windows 2.0
- Consumer Electronics:
  * Distribution media : CD
  * TV : last analog HDTV system
  * Satellite : TVRO/C-band era
  * Video games : Golden age - third generation of video games consoles (Nintendo NES, Sega Master), crash of 1983 (ET Game), fourth generation of video games consoles generations (NEC Home Electronics, Nintendo Gameboy)
- Standards and protocols : Compact Disc Digital Audio, WWW, BMP, GIF, email protocols, IRC, Zip (PKZIP), ISO 9660 file system, EDI (EFIFACT, ODETTE/OFTP), Netbios, SMB, Modems speeds (V22, V22bis, V32)
- Programming languages and frameworks :
- Navigation : Decision to open GPS to civilian use and deployment of modern GPS satellites

## Telecommunications
**[`^        back to top        ^`](#)**

- **Leased lines - T3 circuits**

With the extension of digital services in the 1980s, **leased lines were used to connect customer premises to frame relay or ATM networks**. Access data rates increased from the original T1 option with maximum transmission speed of 1.544 Mbit/s up to **T3 circuits**.

https://en.wikipedia.org/wiki/Leased_line

- **Integrated Services Digital Network (ISDN)** 

ISDN is a set of communication standards for simultaneous digital transmission of voice, video, data, and other network services over the digitalised circuits of the public switched telephone network.[1] Work on the standard began in 1980 at Bell Labs and was formally standardized in 1988 in the CCITT "Red Book".
Prior to ISDN, the telephone system consisted of digital links like T1/E1 on the long-distance lines between telephone company offices and analog signals on copper telephone wires to the customers, the "last mile". 

https://en.wikipedia.org/wiki/Integrated_Services_Digital_Network

- **Passive Optical Networks (PON) (1987) - British Telecommunication**

A passive optical network (PON) is a fiber-optic telecommunications technology for delivering broadband network access to end-customers. Its architecture implements a point-to-multipoint topology in which a single optical fiber serves multiple endpoints by using unpowered (passive) fiber optic splitters to divide the fiber bandwidth among the endpoints. Passive optical networks are often referred to as the last mile between an Internet service provider (ISP) and its customers.[1]

Passive optical networks were first proposed by British Telecommunications in 1987.

https://en.wikipedia.org/wiki/Passive_optical_network

- **ISPs - Internet Service Providers (1980s)**

An Internet service provider (ISP) is an organization that provides services for accessing, using, or participating in the Internet. ISPs can be organized in various forms, such as commercial, community-owned, non-profit, or otherwise privately owned.

Internet services typically provided by ISPs can include Internet access, Internet transit, domain name registration, web hosting, Usenet service, and colocation.

An ISP typically serves as the access point or the gateway that provides a user access to everything available on the Internet.[1] Such a network can also be called as an eyeball network.

During the 1980s, online service providers such as CompuServe and America On Line (AOL) began to offer limited capabilities to access the Internet, such as e-mail interchange, but full access to the Internet was not readily available to the general public.

In 1989, the first Internet service providers, companies offering the public direct access to the Internet for a monthly fee, were established in Australia[4] and the United States. In Brookline, Massachusetts, **The World** became the first commercial ISP in the US. Its first customer was served in November 1989.[5] These companies generally offered dial-up connections, using the public telephone network to provide last-mile connections to their customers. The barriers to entry for dial-up ISPs were low and many providers emerged.

However, cable television companies and the telephone carriers already had wired connections to their customers and could offer Internet connections at much higher speeds than dial-up using broadband technology such as cable modems and digital subscriber line (DSL). As a result, these companies often became the dominant ISPs in their service areas, and what was once a highly competitive ISP market became effectively a monopoly or duopoly in countries with a commercial telecommunications market, such as the United States.

In 1995, NSFNET was decommissioned removing the last restrictions on the use of the Internet to carry commercial traffic and network access points were created to allow peering arrangements between commercial ISPs.

https://en.wikipedia.org/wiki/Internet_service_provider

## Networking
**[`^        back to top        ^`](#)**

- **OSI Model vs. TCP/IP War**

Hubert Zimmermann, and Charles Bachman as chairman, played a key role in the development of the Open Systems Interconnections reference model. Beginning in 1978, this international work led to a draft proposal in 1980 and the final OSI model was published in 1984.

The early research and development of standards for data networks and protocols culminated in the Internet–OSI Standards War in the late 1980s and early 1990s.
Beginning in the early 1980s, ARPA pursued commercial partnerships with the telecommunication and computer industry which enabled the adoption of TCP/IP. In Europe, CERN purchased UNIX machines with TCP/IP for their intranet between 1984 and 1988

At the beginning of the 1990s, academic institutions and organizations in some European countries had adopted TCP/IP.[nb 6] In February 1990 RARE stated "without putting into question its OSI policy, recognizes the TCP/IP family of protocols as an open multivendor suite, well adapted to scientific and technical applications." In the same month, CERN established a transatlantic TCP/IP link with Cornell University in the United States.[53][67] Conversely, starting in August 1990, the NSFNET backbone supported the OSI Connectionless Network Protocol (CLNP) in addition to TCP/IP. 

https://en.wikipedia.org/wiki/Protocol_Wars

- **Ethernet (1980) - IEEE 802.3 (1983)**
Ethernet (/ˈiːθərnɛt/) is a family of wired computer networking technologies commonly used in local area networks (LAN), metropolitan area networks (MAN) and wide area networks (WAN).[1] It was commercially introduced in 1980 and first standardized in 1983 as IEEE 802.3. Ethernet has since been refined to support higher bit rates, a greater number of nodes, and longer link distances, but retains much backward compatibility. Over time, Ethernet has largely replaced competing wired LAN technologies such as Token Ring, FDDI and ARCNET.

The original 10BASE5 Ethernet uses coaxial cable as a shared medium, while the newer Ethernet variants use twisted pair and fiber optic links in conjunction with switches. Over the course of its history, Ethernet data transfer rates have been increased from the original 2.94 Mbit/s[2] to the latest 400 Gbit/s, with rates up to 1.6 Tbit/s under development. The Ethernet standards include several wiring and signaling variants of the OSI physical layer.

https://en.wikipedia.org/wiki/Ethernet

- **RPC - Remote Procedure Call (1981)**

In distributed computing, a remote procedure call (RPC) is when a computer program causes a procedure (subroutine) to execute in a different address space (commonly on another computer on a shared network), which is coded as if it were a normal (local) procedure call, without the programmer explicitly coding the details for the remote interaction. That is, the programmer writes essentially the same code whether the subroutine is local to the executing program, or remote. This is a form of client–server interaction (caller is client, executor is server), typically implemented via a request–response message-passing system. In the object-oriented programming paradigm, RPCs are represented by remote method invocation (RMI). The RPC model implies a level of location transparency, namely that calling procedures are largely the same whether they are local or remote, but usually, they are not identical, so local calls can be distinguished from remote calls. Remote calls are usually orders of magnitude slower and less reliable than local calls, so distinguishing them is important.

RPCs are a form of inter-process communication (IPC), in that different processes have different address spaces: if on the same host machine, they have distinct virtual address spaces, even though the physical address space is the same; while if they are on different hosts, the physical address space is different. Many different (often incompatible) technologies have been used to implement the concept.

Request–response protocols date to early distributed computing in the late 1960s, theoretical proposals of remote procedure calls as the model of network operations date to the 1970s, and practical implementations date to the early 1980s. Bruce Jay Nelson is generally credited with coining the term "remote procedure call" in 1981.[1]

Remote procedure calls used in modern operating systems trace their roots back to the RC 4000 multiprogramming system,[2] which used a request-response communication protocol for process synchronization.[3] The idea of treating network operations as remote procedure calls goes back at least to the 1970s in early ARPANET documents.[4] In 1978, Per Brinch Hansen proposed Distributed Processes, a language for distributed computing based on "external requests" consisting of procedure calls between processes.[5]

One of the earliest practical implementations was in 1982 by Brian Randell and colleagues for their Newcastle Connection between UNIX machines.[6] This was soon followed by "Lupine" by Andrew Birrell and Bruce Nelson in the Cedar environment at Xerox PARC.[7][8][9] Lupine automatically generated stubs, providing type-safe bindings, and used an efficient protocol for communication.[8] One of the first business uses of RPC was by Xerox under the name "Courier" in 1981. The first popular implementation of RPC on Unix was Sun's RPC (now called ONC RPC), used as the basis for Network File System (NFS).

In the 1990s, with the popularity of object-oriented programming, an alternative model of remote method invocation (RMI) was widely implemented, such as in Common Object Request Broker Architecture (CORBA, 1991) and Java remote method invocation. RMIs, in turn, fell in popularity with the rise of the internet, particularly in the 2000s.

https://en.wikipedia.org/wiki/Remote_procedure_call

- **BBS - Bulletin Board System (1980s)**

With the advent of the personal computer (PCs) in the early 1980s, one could use a modem to dial into a remote mainframe computer. In this case, the PC was used like a dumb terminal. But now files could be transferred and one PC could connect to another via modems.

The 1980s saw the rise of the **Bulletin Board System (BBS)**. A BBS was just a computer with a modem listening for incoming calls. The public could dial up a BBS with a modem and then download free software, participate in discussions on various topics, play on-line games, etc. Dialing in to a BBS was something like going to an Internet site. Except that to go to another BBS site, you would need to dial another number (and possible pay long distance telephone charges). Many BBSs would have a monthly charge but some were run by volunteers and were free. Many companies established BBSs for customers that contained support information, catalogs, etc. In the early 1990s, BBSs were booming. By the mid 1990s some even offered Internet connections. For some history of BBSs see Sysops' Corner: History of BBSing

https://tldp.org/HOWTO/Modem-HOWTO-29.html

- **NSFNET (1986)**

In the early 1980s, the National Science Foundation (NSF) funded national supercomputing centers at several universities in the United States, and provided interconnectivity in 1986 with the NSFNET project, thus creating network access to these supercomputer sites for research and academic organizations in the United States. International connections to NSFNET, the emergence of architecture such as the Domain Name System, and the adoption of TCP/IP internationally on existing networks marked the beginnings of the Internet.[6][7][8] Commercial Internet service providers (ISPs) emerged in 1989 in the United States and Australia.[9] The ARPANET was decommissioned in 1990.[10] Limited private connections to parts of the Internet by officially commercial entities emerged in several American cities by late 1989 and 1990.

https://en.wikipedia.org/wiki/History_of_the_Internet

- **Internet access (1989)**

Dial-up Internet has been around since the 1980s via public providers such as NSFNET-linked universities. The BBC established Internet access via Brunel University in the United Kingdom in 1989.

https://en.wikipedia.org/wiki/Dial-up_Internet_access


### NFC

- **First patent with RFID (1983)**

May 17, 1983: The first patent to be associated with the abbreviation "RFID" was granted to Charles Walton.[16]

https://en.wikipedia.org/wiki/Near-field_communication

## Cryptography
- **RSA Patent granted (1983)**

https://en.wikipedia.org/wiki/RSA_(cryptosystem)

- **X.509 (1988)** 

In cryptography, X.509 is an International Telecommunication Union (ITU) standard defining the format of public key certificates.[1] X.509 certificates are used in many Internet protocols, including TLS/SSL, which is the basis for HTTPS,[2] the secure protocol for browsing the web. They are also used in offline applications, like electronic signatures.

https://en.wikipedia.org/wiki/X.509

## Computers
**[`^        back to top        ^`](#)**

### Evolution of mainframes

By the 1980s, many mainframes supported **graphic display terminals**, and **terminal emulation**, but not graphical user interfaces. 

https://en.wikipedia.org/wiki/Mainframe_computer

Shrinking demand and tough competition started a shakeout in the market in the early 1970s—RCA sold out to UNIVAC and GE sold its business to Honeywell; between 1986 and 1990 Honeywell was bought out by Bull; UNIVAC became a division of Sperry, which later merged with Burroughs to form Unisys Corporation in 1986.
In 1984 estimated sales of desktop computers ($11.6 billion) exceeded mainframe computers ($11.4 billion) for the first time. IBM received the vast majority of mainframe revenue.[19] During the 1980s, minicomputer-based systems grew more sophisticated and were able to displace the lower end of the mainframes. These computers, sometimes called **departmental computers**, were typified by the Digital Equipment Corporation VAX series.

https://en.wikipedia.org/wiki/Mainframe_computer

### Evolution of hardware ####

- Notable manufacturers of **superminicomputers** in 1980 included: Digital Equipment Corporation, Perkin-Elmer, and Prime Computer.[14][15] Other makers of systems included SEL/Gould and Data General.[16] Four years later there were about a dozen companies producing a significant number of superminicomputers

https://en.wikipedia.org/wiki/Superminicomputer

- **From published schematic diagrams to single Integrated Circuits (1980s)**
 
The most popular computers of the 1980s such as the Apple II and IBM PC had published schematic diagrams and other documentation which permitted rapid reverse-engineering and third-party replacement motherboards. Usually intended for building new computers compatible with the exemplars, many motherboards offered additional performance or other features and were used to upgrade the manufacturer's original equipment.

During the late 1980s and early 1990s, it became economical to move an increasing number of peripheral functions onto the motherboard. In the late 1980s, personal computer motherboards began to include single ICs (also called Super I/O chips) capable of supporting a set of low-speed peripherals: PS/2 keyboard and mouse, floppy disk drive, serial ports, and parallel ports.

https://en.wikipedia.org/wiki/Motherboard

- **Xerox Star 8010 workstation - First Commercial GUI with a mouse (1981) - Xerox**.

The Xerox PARC GUI consisted of graphical elements such as windows, menus, radio buttons, and check boxes. The concept of icons was later introduced by David Canfield Smith, who had written a thesis on the subject under the guidance of Kay.[19][20][21] The PARC GUI employs a pointing device along with a keyboard. These aspects can be emphasized by using the alternative term and acronym for windows, icons, menus, pointing device (WIMP). This effort culminated in the 1973 Xerox Alto, the first computer with a GUI, though the system never reached commercial production. 
In 1981, Xerox eventually commercialized the Alto in the form of a new and enhanced system – the Xerox 8010 Information System – more commonly known as the Xerox Star.[22][23] These early systems spurred many other GUI efforts, including Lisp machines by Symbolics and other manufacturers, the Apple Lisa (which presented the concept of menu bar and window controls) in 1983, the Apple Macintosh 128K in 1984, and the Atari ST with Digital Research's GEM, and Commodore Amiga in 1985. Visi On was released in 1983 for the IBM PC compatible computers, but was never popular due to its high hardware demands.[24] Nevertheless, it was a crucial influence on the contemporary development of Microsoft Windows.[25]

https://en.wikipedia.org/wiki/Graphical_user_interface

- **IBM PC (1981) powered by Microsoft IBM PC DOS**

The IBM Personal Computer (model 5150, commonly known as the IBM PC) is the first microcomputer released in the IBM PC model line and the basis for the IBM PC compatible de facto standard. Released on August 12, 1981, it was created by a team of engineers and designers directed by Don Estridge in Boca Raton, Florida.
The machine was based on open architecture and third-party peripherals. Over time, expansion cards and software technology increased to support it.
The PC had a substantial influence on the personal computer market. The specifications of the IBM PC became one of the most popular computer design standards in the world. The only significant competition it faced from a non-compatible platform throughout the 1980s was from the Apple Macintosh product line. The majority of modern personal computers are distant descendants of the IBM PC.

https://en.wikipedia.org/wiki/IBM_Personal_Computer

The third marketed version of an integrated mouse shipped as a part of a computer and intended for personal computer navigation came with the Xerox 8010 Star in 1981.
By 1982, the Xerox 8010 was probably the best-known computer with a mouse. The Sun-1 also came with a mouse, and the forthcoming Apple Lisa was rumored to use one, but the peripheral remained obscure; Jack Hawley of The Mouse House reported that one buyer for a large organization believed at first that his company sold lab mice. Hawley, who manufactured mice for Xerox, stated that "Practically, I have the market all to myself right now"; a Hawley mouse cost $415.[49] In 1982, Logitech introduced the P4 Mouse at the Comdex trade show in Las Vegas, its first hardware mouse.[50] That same year Microsoft made the decision to make the MS-DOS program Microsoft Word mouse-compatible, and developed the first PC-compatible mouse. Microsoft's mouse shipped in 1983, thus beginning the Microsoft Hardware division of the company.[51] However, the mouse remained relatively obscure until the appearance of the Macintosh 128K (which included an updated version of the single-button[52] Lisa Mouse) in 1984,[53] and of the Amiga 1000 and the Atari ST in 1985.

https://en.wikipedia.org/wiki/Computer_mouse

**Parallel cable and port**
IBM released the IBM Personal Computer in 1981 and included a variant of the Centronics interface— only IBM logo printers (rebranded from Epson) could be used with the IBM PC.[5] IBM standardized the **parallel cable with a DB25F connector on the PC side and the 36-pin Centronics connector on the printer side**. Vendors soon released printers compatible with both standard Centronics and the IBM implementation.

https://en.wikipedia.org/wiki/Parallel_port

**COM port**
COM (communication port)[1][2] is the original, yet still common, name of the serial port interface on PC-compatible computers. It can refer not only to physical ports, but also to emulated ports, such as ports created by Bluetooth or USB adapters.

The name for the COM port started with the original IBM PC. IBM had called the four well-defined communication RS-232 ports the "COM" ports, starting from COM1 through COM4. In BASICA and PC DOS you can open these ports as "COM1:" through "COM4:", and all PC compatibles using MSDOS used the same denotation.[citation needed] Most PC-compatible computers in the 1980s and 1990s had one or two COM ports.

By 2007, most computers shipped with only one or no physical COM ports. Today few consumer-grade PC-compatible computers include COM ports,[3] though some of them do still include a COM header on the motherboard.[4]

After the RS-232 COM port was removed from most consumer-grade computers, an external USB-to-UART serial adapter cable was used to compensate for the loss. A major supplier of these chips is FTDI.[citation needed]

https://en.wikipedia.org/wiki/COM_(hardware_interface)

- **Intel 286 (1982) - Intel**

The Intel 80286[3] (also marketed as the iAPX 286[4] and often called Intel 286) is a 16-bit microprocessor that was introduced on February 1, 1982. It was the first 8086-based CPU with separate, non-multiplexed address and data buses and also the first with memory management and wide protection abilities. The 80286 used approximately 134,000 transistors in its original nMOS (HMOS) incarnation and, just like the contemporary 80186,[5] it could correctly execute most software written for the earlier Intel 8086 and 8088 processors.[6]

The 80286 was employed for the IBM PC/AT, introduced in 1984, and then widely used in most PC/AT compatible computers until the early 1990s.

https://en.wikipedia.org/wiki/Intel_80286

- **Compact Disc (1982) - Philips and Sony**

The compact disc (CD) is a digital optical disc data storage format that was co-developed by Philips and Sony to store and play digital audio recordings. In August 1982, the first compact disc was manufactured. It was then released in October 1982 and branded as Digital Audio Compact Disc.

The format was later adapted for storage of data (CD-ROM). Several other formats were further derived from these, including write-once audio and data storage (CD-R), rewritable media (CD-RW), Video CD (VCD), Super Video CD (SVCD), Photo CD, Picture CD, Compact Disc-Interactive (CD-i) and Enhanced Music CD.

Standard CDs have a diameter of 120 millimetres (4.7 in) and are designed to hold up to 74 minutes of uncompressed stereo digital audio or about 650 MiB of data. Capacity is routinely extended to 80 minutes and 700 MiB by arranging data more closely on the same sized disc. The Mini CD has various diameters ranging from 60 to 80 millimetres (2.4 to 3.1 in); they are sometimes used for CD singles, storing up to 24 minutes of audio, or delivering device drivers.

At the time of the technology's introduction in 1982, a CD could store much more data than a personal computer hard disk drive, which would typically hold 10 MiB. By 2010, hard drives commonly offered as much storage space as a thousand CDs, while their prices had plummeted to commodity level. In 2004, worldwide sales of audio CDs, CD-ROMs, and CD-Rs reached about 30 billion discs. By 2007, 200 billion CDs had been sold worldwide.[2]

https://en.wikipedia.org/wiki/Compact_disc

- **Color inkjet printer (1982) - Robert Howard**

In 1982, Robert Howard came up with the idea to produce a small color printing system that used piezos to spit drops of ink. He formed the company, R.H. (Robert Howard) Research (named Howtek, Inc. in Feb 1984), and developed the revolutionary technology that led to the Pixelmaster color printer with solid ink[6] using Thermojet technology. This technology consists of a tubular single nozzle acoustical wave drop generator invented originally by Steven Zoltan in 1972 with a glass nozzle and improved by the Howtek inkjet engineer in 1984 with a Tefzel molded nozzle to remove unwanted fluid frequencies.

https://en.wikipedia.org/wiki/Inkjet_printing

- **Apple LISA - First PC with a GUI, OS with protected memory, document-oriented workflow (1983) - Apple**

Lisa is a desktop computer developed by Apple, released on January 19, 1983. It is one of the first personal computers to present a graphical user interface (GUI) in a machine aimed at individual business users. Its development began in 1978.[2] It underwent many changes before shipping at US$9,995 (equivalent to $27,190 in 2021) with a five-megabyte hard drive. It was affected by its high price, insufficient software, unreliable Apple FileWare floppy disks, and the immediate release of the cheaper and faster Macintosh. Only 10,000 were sold in two years.

Considered a commercial failure (albeit one with technical acclaim), Lisa introduced a number of advanced features that reappeared on the Macintosh and eventually IBM PC compatibles. Among them is an operating system with protected memory[3] and a document-oriented workflow. The hardware was more advanced overall than the forthcoming Macintosh 128K; the Lisa included hard disk drive support, capacity for up to 2 megabytes (MB) of random-access memory (RAM), expansion slots, and a larger, higher-resolution display.

https://en.wikipedia.org/wiki/Apple_Lisa

- **IBM PC/XT - Internal 10 MB HDD (1983) - IBM**

Most HDDs in the early 1980s were sold to PC end users as an external, add-on subsystem. The subsystem was not sold under the drive manufacturer's name but under the subsystem manufacturer's name such as Corvus Systems and Tallgrass Technologies, or under the PC system manufacturer's name such as the Apple ProFile. The IBM PC/XT in 1983 included an internal 10 MB HDD, and soon thereafter internal HDDs proliferated on personal computers.

External HDDs remained popular for much longer on the Apple Macintosh. Many Macintosh computers made between 1986 and 1998 featured a SCSI port on the back, making external expansion simple. Older compact Macintosh computers did not have user-accessible hard drive bays (indeed, the Macintosh 128K, Macintosh 512K, and Macintosh Plus did not feature a hard drive bay at all), so on those models external SCSI disks were the only reasonable option for expanding upon any internal storage.

https://en.wikipedia.org/wiki/Hard_disk_drive

- **AT/ISA bus - IBM PC AT (1984) - IBM**

IBM PC AT introduces the AT bus, later known as the ISA bus, a 16-bit bus with backwards compatibility with 8-bit PC-compatible expansion cards. The bus also offered fifteen IRQs and seven DMA channels, expanded from eight IRQs and four DMA channels for the PC, achieved by adding another 8259A IRQ controller and another 8237A DMA controller.[5][6] Some IRQ and DMA channels are used by the motherboard and not exposed on the expansion bus. Both dual IRQ and DMA chipsets are cascading which shares the primary pair. In addition to these chipsets, Intel 82284 Clock Driver and Ready Interface and Intel 82288 Bus Controller are to support the microprocessor.

https://en.wikipedia.org/wiki/IBM_Personal_Computer/AT

Industry Standard Architecture (ISA) is the 16-bit internal bus of IBM PC/AT and similar computers based on the Intel 80286 and its immediate successors during the 1980s. The bus was (largely) backward compatible with the 8-bit bus of the 8088-based IBM PC, including the IBM PC/XT as well as IBM PC compatibles.
Originally referred to as the PC bus (8-bit) or AT bus (16-bit), it was also termed I/O Channel by IBM. The ISA term was coined as a retronym by competing PC-clone manufacturers in the late 1980s or early 1990s as a reaction to IBM attempts to replace the AT-bus with its new and incompatible Micro Channel architecture.

https://en.wikipedia.org/wiki/Industry_Standard_Architecture

- **Psion - First personal digital assistant (PDA) / handheld PC (1984) - Psion**

A personal digital assistant (PDA), also known as a handheld PC,[1][2] is a variety mobile device which functions as a personal information manager. PDAs have been mostly displaced by the widespread adoption of highly capable smartphones, in particular those based on iOS and Android.[3]

A PDA has an electronic visual display. Most models also have audio capabilities, allowing usage as a portable media player, and also enabling many of them to be used as telephones. Nearly all modern PDAs can access the Internet, intranets or extranets via Wi-Fi or Wireless WANs, letting them include a web browser. Sometimes, instead of buttons, PDAs employ touchscreen technology.

The first PDA, the Organiser, was released in 1984 by Psion, followed by Psion's Series 3,

https://en.wikipedia.org/wiki/Personal_digital_assistant

- **1984 Apple Commercial**

By 1984 computer dealers saw Apple as the only clear alternative to IBM's influence;[93] some even promoted its products to reduce dependence on the PC.[78] The company announced the Macintosh 128k to the press in October 1983, followed by an 18-page brochure included with magazines in December.[94] Its debut, however, was announced by a single national broadcast of a US$1.5 million television commercial, "1984" (equivalent to $3,900,000 in 2021). It was directed by Ridley Scott, aired during the third quarter of Super Bowl XVIII on January 22, 1984,[95] and is now considered a "watershed event"[96] and a "masterpiece.

https://en.wikipedia.org/wiki/History_of_personal_computers

- **Intel i386 (1985) - Intel **

The Intel 386, originally released as 80386 and later renamed i386, is a 32-bit microprocessor introduced in 1985.[2] The first versions had 275,000 transistors[3] and were the CPU of many workstations and high-end personal computers of the time. As the original implementation of the 32-bit extension of the 80286 architecture,[4] the i386 instruction set, programming model, and binary encodings are still the common denominator for all 32-bit x86 processors, which is termed the i386-architecture, x86, or IA-32, depending on context.

The 32-bit i386 can correctly execute most code intended for the earlier 16-bit processors such as 8086 and 80286 that were ubiquitous in early PCs. (Following the same tradition, modern 64-bit x86 processors are able to run most programs written for older x86 CPUs, all the way back to the original 16-bit 8086 of 1978.) Over the years, successively newer implementations of the same architecture have become several hundreds of times faster than the original 80386 (and thousands of times faster than the 8086).[5] A 33 MHz 80386 was reportedly measured to operate at about 11.4 MIPS.[6]

Development of i386 technology began in 1982 under the internal name of P3.[7] The tape-out of the 80386 development was finalized on July 1985.[8] The 80386 was introduced as pre-production samples for software development workstations in October 1985.[9] Manufacturing of the chips in significant quantities commenced in June 1986,[10][11] along with the first plug-in device that allowed existing 80286-based computers to be upgraded to the 386, the Translator 386 by American Computer and Peripheral.[12][13] Mainboards for 80386-based computer systems were cumbersome and expensive at first, but manufacturing was justified upon the 80386's mainstream adoption. The first personal computer to make use of the 80386 was designed and manufactured by Compaq[14] and marked the first time a fundamental component in the IBM PC compatible de facto standard was updated by a company other than IBM.

https://en.wikipedia.org/wiki/I386

- **IA-32 architecture (1985) - Intel**

IA-32 (short for "Intel Architecture, 32-bit", sometimes also called i386[1][2])[3] is the 32-bit version of the x86 instruction set architecture, designed by Intel and first implemented in the 80386 microprocessor in 1985. IA-32 is the first incarnation of x86 that supports 32-bit computing;[4] as a result, the "IA-32" term may be used as a metonym to refer to all x86 versions that support 32-bit computing.[5][6]

Within various programming language directives, IA-32 is still sometimes referred to as the "i386" architecture. In some other contexts, certain iterations of the IA-32 ISA are sometimes labelled i486, i586 and i686, referring to the instruction supersets offered by the 80486, the P5 and the P6 microarchitectures respectively. These updates offered numerous additions alongside the base IA-32 set including floating-point capabilities and the MMX extensions.

Intel was historically the largest manufacturer of IA-32 processors, with the second biggest supplier having been AMD. During the 1990s, VIA, Transmeta and other chip manufacturers also produced IA-32 compatible processors (e.g. WinChip). In the modern era, Intel still produced IA-32 processors under the Intel Quark microcontroller platform until 2019; however, since the 2000s, the majority of manufacturers (Intel included) moved almost exclusively to implementing CPUs based on the 64-bit variant of x86, x86-64. x86-64, by specification, offers legacy operating modes that operate on the IA-32 ISA for backwards compatibility. Even given the contemporary prevalence of x86-64, as of 2018, IA-32 protected mode versions of many modern operating systems are still maintained, e.g. Microsoft Windows (until Windows 10; Windows 11 requires x86-64-compatible processor for x86 versions)[7] and the Debian Linux distribution.[8] In spite of IA-32's name (and causing some potential confusion), the 64-bit evolution of x86 that originated out of AMD would not be known as "IA-64", that name instead belonging to Intel's Itanium architecture.

https://en.wikipedia.org/wiki/IA-32

- **Comodore Amiga - Home computer with 640x400i interlaced resolution (1985) - Comodore**

The 640 × 400i resolution (720 × 480i with borders disabled) was first introduced by home computers such as the Commodore Amiga and, later, Atari Falcon. These computers used interlace to boost the maximum vertical resolution. These modes were only suited to graphics or gaming, as the flickering interlace made reading text in word processor, database, or spreadsheet software difficult.

- **Apple UniDisk 3.5 - 3"1/2 Floppy Disk (1985) - Apple**

In September 1985, Apple released its first 3+1⁄2-inch drive (A2M2053) for the Apple II series utilizing Sony's new 800-kilobyte double-sided drive mechanism, which would not be released for the Macintosh until four months later. The Apple UniDisk 3.5 drive contained additional circuitry making it an "intelligent" or "smart" drive; this made it incompatible with the Macintosh, despite having the identical mechanism that was to be later used in the Macintosh drive. 

https://en.wikipedia.org/wiki/Macintosh_External_Disk_Drive

- **Apple 800K External Drive - External Drive (1986) - Apple**

In January 1986, Apple introduced the Macintosh Plus which had a Sony double-sided 800-kilobyte capacity disk drive, and used the new HFS disk format providing directories and sub-directories. This drive was fitted into an external case as the Macintosh 800K External Drive (M0131), which was slimmer than the earlier 400-kilobyte drive.

https://en.wikipedia.org/wiki/Macintosh_External_Disk_Drive

- **Apple Desktop Bus - chaining of 16 devices (1986) - Apple**

In 1986 Apple first implemented the Apple Desktop Bus allowing the daisy chaining of up to 16 devices, including mice and other devices on the same bus with no configuration whatsoever. Featuring only a single data pin, the bus used a purely polled approach to device communications and survived as the standard on mainstream models (including a number of non-Apple workstations) until 1998 when Apple's iMac line of computers joined the industry-wide switch to using USB. Beginning with the Bronze Keyboard PowerBook G3 in May 1999, Apple dropped the external ADB port in favor of USB, but retained an internal ADB connection in the PowerBook G4 for communication with its built-in keyboard and trackpad until early 2005.

https://en.wikipedia.org/wiki/Computer_mouse

- **Small Computer System Interface (SCSI) - (1986)**

Small Computer System Interface (SCSI, /ˈskʌzi/ SKUZ-ee)[1] is a set of standards for physically connecting and transferring data between computers and peripheral devices. The SCSI standards define commands, protocols, electrical, optical and logical interfaces. The SCSI standard defines command sets for specific peripheral device types; the presence of "unknown" as one of these types means that in theory it can be used as an interface to almost any device, but the standard is highly pragmatic and addressed toward commercial requirements. The initial Parallel SCSI was most commonly used for hard disk drives and tape drives, but it can connect a wide range of other devices, including scanners and CD drives, although not all controllers can handle all devices.
The ancestral SCSI standard, X3.131-1986, generally referred to as SCSI-1, was published by the X3T9 technical committee of the American National Standards Institute (ANSI) in 1986. SCSI-2 was published in August 1990 as X3.T9.2/86-109, with further revisions in 1994 and subsequent adoption of a multitude of interfaces. Further refinements have resulted in improvements in performance and support for ever-increasing data storage capacity.[2]

https://en.wikipedia.org/wiki/SCSI#Interfaces

- **Parallel ATA (PATA), originally AT Attachment, also known as ATA or IDE (1986) - Western Digital and Compaq**

a standard interface designed for IBM PC-compatible computers. It was first developed by Western Digital and Compaq in 1986 for compatible hard drives and CD or DVD drives

https://en.wikipedia.org/wiki/Parallel_ATA

- **UART serial communication (1987) - National Semiconductor**

The 16550 UART (universal asynchronous receiver/transmitter) is an integrated circuit designed for implementing the interface for serial communications. The corrected -A version was released in 1987 by National Semiconductor.[1] 

https://en.wikipedia.org/wiki/16550_UART

- **IBM PS/2 - PS/2 for mouse and keyboard, UART serial port, VGA (1987) - IBM***

The Personal System/2 or PS/2 is IBM's second generation[2][3] of personal computers. Released in 1987, it officially replaced the IBM PC, XT, AT, and PC Convertible in IBM's lineup. Many of the PS/2's innovations, such as the 16550 UART (serial port), 1440 KB 3.5-inch floppy disk format, 72-pin SIMMs, the PS/2 port, and the VGA video standard, went on to become standards in the broader PC market.[4][5]

https://en.wikipedia.org/wiki/IBM_PS/2

Micro Channel architecture, or the Micro Channel bus, is a proprietary 16- or 32-bit parallel computer bus introduced by IBM in 1987 which was used on PS/2 and other computers until the mid-1990s. Its name is commonly abbreviated as "MCA", although not by IBM. In IBM products, it superseded the ISA bus and was itself subsequently superseded by the PCI bus architecture.

https://en.wikipedia.org/wiki/Extended_Industry_Standard_Architecture

With the arrival of the IBM PS/2 personal-computer series in 1987, IBM introduced the eponymous PS/2 port for mice and keyboards, which other manufacturers rapidly adopted. The most visible change was the use of a round 6-pin mini-DIN, in lieu of the former 5-pin MIDI style full sized DIN 41524 connector. In default mode (called stream mode) a PS/2 mouse communicates motion, and the state of each button, by means of 3-byte packets.[98] For any motion, button press or button release event, a PS/2 mouse sends, over a bi-directional serial port, a sequence of three bytes, with the following format:

https://en.wikipedia.org/wiki/Computer_mouse

- **Commercial flash memory - (1987, Toshiba), (1988, Intel)**

Toshiba commercially launched NAND flash memory in 1987.[1][13] Intel Corporation introduced the first commercial NOR type flash chip in 1988.[22

https://en.wikipedia.org/wiki/Flash_memory

In 1989, Intel employed the FGMOS as an analog nonvolatile memory element** in its **electrically trainable artificial neural network (ETANN) chip**,[3] demonstrating the potential of using FGMOS devices for applications other than digital memory.

https://en.wikipedia.org/wiki/Floating-gate_MOSFET

- **SVGA Graphics Cards (1988)**

In the late 1980s, after the release of IBM's VGA, third-party manufacturers began making graphics cards based on its specifications with extended capabilities. As these cards grew in popularity they began to be referred to as "Super VGA."
This term was not an official standard, but a shorthand for enhanced VGA cards which had become common by 1988.[1] One card that explicitly used the term was **Genoa's SuperVGA HiRes.**[3]
Super VGA cards broke compatibility with the IBM VGA standard, requiring software developers to provide specific display drivers and implementations for each card their software could operate on.

https://en.wikipedia.org/wiki/Super_VGA

- **EISA - Extended Industry Standard Architecture (1988) - IBM PC clones**

(in practice almost always shortened to EISA and frequently pronounced "eee-suh") is a bus standard for IBM PC compatible computers. It was announced in September 1988 by a consortium of PC clone vendors (the Gang of Nine) as an alternative to IBM's proprietary Micro Channel architecture (MCA) in its PS/2 series.[2]
In comparison with the AT bus, which the Gang of Nine retroactively renamed to the ISA bus to avoid infringing IBM's trademark on its PC/AT computer, EISA is extended to 32 bits and allows more than one CPU to share the bus. The bus mastering support is also enhanced to provide access to 4 GB of memory. Unlike MCA, EISA can accept older XT and ISA boards — the lines and slots for EISA are a superset of ISA.
EISA was much favoured by manufacturers due to the proprietary nature of MCA, and even IBM produced some machines supporting it
Gang of 9:
The Gang of Nine was the informal name given to the consortium of personal computer manufacturing companies that together created the EISA bus. Rival members generally acknowledged Compaq's leadership, with one stating in 1989 that within the Gang of Nine "when you have 10 people sit down before a table to write a letter to the president, someone has to write the letter. Compaq is sitting down at the typewriter".[6] The members were:[2]
  * AST Research, Inc.
  * Compaq Computer Corporation
  * Seiko Epson Corporation
  * Hewlett-Packard Company
  * NEC Corporation
  * Olivetti
  * Tandy Corporation
  * WYSE
  * Zenith Data Systems

https://en.wikipedia.org/wiki/Extended_Industry_Standard_Architecture

- **FinFET - Hitachi (1988)** 

FinFET (fin field-effect transistor), a type of 3D multi-gate MOSFET, was developed by Digh Hisamoto and his team of researchers at Hitachi Central Research Laboratory in 1989.[18][19]

https://en.wikipedia.org/wiki/Semiconductor_device


- **Intel i486 (1989) - Intel**

The Intel 486, officially named i486 and also known as 80486, is a microprocessor. It is a higher-performance follow-up to the Intel 386. The i486 was introduced in 1989. It represents the fourth generation of binary compatible CPUs following the 8086 of 1978, the Intel 80286 of 1982, and 1985's i386.

It was the first tightly-pipelined[c] x86 design as well as the first x86 chip to include more than one million transistors. It offered a large on-chip cache and an integrated floating-point unit.

A typical 50 MHz i486 executes around 40 million instructions per second (MIPS), reaching 50 MIPS peak performance. It is approximately twice as fast as the i386 or i286 per clock cycle. The i486's improved performance is thanks to its five-stage pipeline with all stages bound to a single cycle. The enhanced FPU unit on the chip was significantly faster than the i387 FPU per cycle. The intel 80387 FPU ("i387") was a separate, optional math coprocessor that was installed in a motherboard socket alongside the i386.

The i486 was succeeded by the original Pentium.

https://en.wikipedia.org/wiki/I486

- **Laser printers to mass markets based on Canon CX engine (1980s) - HP LaserJet, Apple LaserWriter**

*1981: The first small personal computer designed for office use, the Xerox Star 8010, reached market. The system used a desktop metaphor that was unsurpassed in commercial sales, until the Apple Macintosh. Although it was innovative, the Star workstation was a prohibitively-expensive (US$17,000) system, affordable only to a fraction of the businesses and institutions at which it was targeted.[9]

**1984: The first laser printer intended for mass-market sales, the HP LaserJet**, was released; it used the Canon CX engine, controlled by HP software. The LaserJet was quickly followed by printers from Brother Industries, IBM, and others. First-generation machines had large photosensitive drums, of circumference greater than the loaded paper's length. Once faster-recovery coatings were developed, the drums could touch the paper multiple times in a pass, and therefore be smaller in diameter.

**1985: Apple introduced the LaserWriter (also based on the Canon CX engine)**,[10] but used the newly released PostScript page-description language (up until this point, each manufacturer used its own proprietary page-description language, making the supporting software complex and expensive). PostScript allowed the use of text, fonts, graphics, images, and color largely independent of the printer's brand or resolution.
PageMaker, developed by Aldus for the Macintosh and LaserWriter, was also released in 1985 and the combination became very popular for desktop publishing.[5][6]

https://en.wikipedia.org/wiki/Laser_printing

## Evolution of uses
**[`^        back to top        ^`](#)**

### Servers
- **Bind  (1984) : First DNS server**. Still widely in use (BIND9) 

https://en.wikipedia.org/wiki/Domain_Name_System

- **BBS - Bulletin Board System (1973-1995)**

A bulletin board system or BBS (also called Computer Bulletin Board Service, CBBS[1]) is a computer server running software that allows users to connect to the system using a terminal program. Once logged in, the user can perform functions such as uploading and downloading software and data, reading news and bulletins, and exchanging messages with other users through public message boards and sometimes via direct chatting. In the early 1980s, message networks such as FidoNet were developed to provide services such as NetMail, which is similar to internet-based email.[2]

Many BBSes also offer online games in which users can compete with each other. BBSes with multiple phone lines often provide chat rooms, allowing users to interact with each other. Bulletin board systems were in many ways a precursor to the modern form of the World Wide Web, social networks, and other aspects of the Internet. Low-cost, high-performance asynchronous modems drove the use of online services and BBSes through the early 1990s. InfoWorld estimated that there were 60,000 BBSes serving 17 million users in the United States alone in 1994, a collective market much larger than major online services such as CompuServe.

The introduction of inexpensive dial-up internet service and the Mosaic web browser offered ease of use and global access that BBS and online systems did not provide, and led to a rapid crash in the market starting in late 1994-early 1995. Over the next year, many of the leading BBS software providers went bankrupt and tens of thousands of BBSes disappeared. Today, BBSing survives largely as a nostalgic hobby in most parts of the world, but it is still an extremely popular form of communication for Taiwanese youth (see PTT Bulletin Board System).[3] Most surviving BBSes are accessible over Telnet and typically offer free email accounts, FTP services, IRC and all the protocols commonly used on the Internet. Some offer access through packet switched networks or packet radio connections.[1]

https://en.wikipedia.org/wiki/Bulletin_board_system


### Clients

#### GUI
- **IBM DOS operating system (1981) - IBM / Microsoft**

IBM initially announced intent to support multiple operating systems: CP/M-86, UCSD p-System,[62] and an in-house product called IBM PC DOS, developed by Microsoft.[63][8] In practice, IBM's expectation and intent was for the market to primarily use PC DOS,[64] CP/M-86 was not available for six months after the PC's release[65] and received extremely few orders once it was,[66] and p-System was also not available at release. PC DOS rapidly established itself as the standard OS for the PC and remained the standard for over a decade, with a variant being sold by Microsoft themselves as MS-DOS.

https://en.wikipedia.org/wiki/IBM_Personal_Computer

https://en.wikipedia.org/wiki/IBM_PC_DOS

- **Classic MacOS (1984)**

The "classic" Mac OS is the original Macintosh operating system that was introduced in 1984 alongside the first Macintosh and remained in primary use on Macs until the introduction of Mac OS X in 2001.

https://en.wikipedia.org/wiki/Macintosh_operating_systems

- **Windows 2.0 (1987) - Microsoft**

Windows 2.0 is a major release of Microsoft Windows, a family of graphical operating systems for personal computers developed by Microsoft. It was released to manufacturing on December 9, 1987, as a successor to Windows 1.0.

The product includes two different variants, a base edition for 8086 real mode, and Windows/386, an enhanced edition for i386 protected mode. Windows 2.0 differs from its predecessor by allowing users to overlap and resize application windows, while the operating environment also introduced desktop icons, keyboard shortcuts, and support for 16-color VGA graphics. It also **introduced Microsoft Word and Excel**, and integrated the Control Panel, while the developer support increased substantially.

Noted as an improvement of its predecessor, Microsoft Windows gained more sales and popularity after the release of the operating environment, although it is also considered to be the incarnation that remained a work in progress. Due to the introduction of overlapping windows, Apple Inc. had filed a lawsuit against Microsoft in March 1988 after accusing them of violating copyrights Apple held, although in the end, the judge ruled in favor of Microsoft. The operating environment was succeeded by Windows 2.1 in May 1988, while Microsoft ended its support on December 31, 2001.

https://en.wikipedia.org/wiki/Windows_2.0x

**Dynamic Data Exchange** was first introduced in 1987 with the release of Windows 2.0 as a method of interprocess communication so that one program could communicate with or control another program, somewhat like Sun's RPC (Remote Procedure Call).[1] At the time, the only method for communication between the operating system and client applications was the "Windows Messaging Layer." DDE extended this protocol to allow peer-to-peer communication among client applications, via message broadcasts.

https://en.wikipedia.org/wiki/Dynamic_Data_Exchange

## Consumer electronics
**[`^        back to top        ^`](#)**

### TV
- **HD-MAC - last analog HDTV system (1986)**

In 1986, the European Community proposed HD-MAC, an analog HDTV system with 1,152 lines. A public demonstration took place for the 1992 Summer Olympics in Barcelona. However HD-MAC was scrapped in 1993 and the Digital Video Broadcasting (DVB) project was formed, which would foresee development of a digital HDTV standard.[7]

https://en.wikipedia.org/wiki/High-definition_television

### Satelitte

- **TVRO/C-band satellite era (1980–1986)**

On 26 April 1982, the first satellite channel in the UK, Satellite Television Ltd. (later Sky One), was launched.[68] Its signals were transmitted from the ESA's Orbital Test Satellites
Originally, all channels were broadcast in the clear (ITC) because the equipment necessary to receive the programming was too expensive for consumers. With the growing number of TVRO systems, the program providers and broadcasters had to scramble their signal and develop subscription systems.
In January 1986, HBO began using the now-obsolete VideoCipher II system to encrypt their channels.
On 11 December 1988 Luxembourg launched Astra 1A, the first satellite to provide medium power satellite coverage to Western Europe.[75] This was one of the first medium-powered satellites, transmitting signals in Ku band and allowing reception with small(90 cm) dishes for the first time ever.

https://en.wikipedia.org/wiki/Satellite_television

### Distribution Media

- **Sony CDP-101 - Compact Disc Digital Audio (1982) - Sony**

Compact Disc Digital Audio (CDDA or CD-DA), also known as Digital Audio Compact Disc or simply as Audio CD, is the standard format for audio compact discs. The standard is defined in the Red Book, one of a series of Rainbow Books (named for their binding colors) that contain the technical specifications for all CD formats.

The first commercially available audio CD player, the Sony CDP-101, was released October 1982 in Japan. The format gained worldwide acceptance in 1983–84, selling more than a million CD players in those two years, to play 22.5 million discs.[1]

Beginning in the 2000s, CDs were increasingly being replaced by other forms of digital storage and distribution, with the result that by 2010 the number of audio CDs being sold in the U.S. had dropped about 50% from their peak; however, they remained one of the primary distribution methods for the music industry.

https://en.wikipedia.org/wiki/Compact_Disc_Digital_Audio

### Video games

- **Golden age : Move of arcade games to microprocessors (1978-1983)**

As technology moved from transistor-transistor logic (TTL) integrated circuits to microprocessors, a new wave of arcade video games arose, starting with Taito's **Space Invaders** in 1978 and leading to a golden age of arcade video games that included **Pac-Man** (Namco, 1980), **Missile Command** (Atari, 1980), and **Donkey Kong (Nintendo, 1981)**. The golden age waned in 1983 due to an excess number of arcade games, the growing draw of home video game consoles and computers, and a moral panic on the impact of arcade video games on youth.[22][57] The arcade industry was also partially impacted by the video game crash of 1983.

https://en.wikipedia.org/wiki/Arcade_game#History

Space Invaders led off what is considered to be the golden age of arcade games which lasted from 1978 to 1982. Several influential and best-selling arcade games were released during this period from Atari, Namco, Taito, Williams, and Nintendo, including Asteroids (1979), Galaxian (1979), Defender (1980), Missile Command (1980), Tempest (1981), and Galaga (1981). Pac-Man, released in 1980, became a popular culture icon, and a new wave of games appeared that focused on identifiable characters and alternate mechanics such as navigating a maze or traversing a series of platforms. Aside from Pac-Man and its sequel, Ms. Pac-Man (1982), the most popular games in this vein during the golden age were Donkey Kong (1981) and Q*bert (1982).[14] Games like Pac-Man, Donkey Kong and Q*bert also introduced the **concept of narratives and characters to video games**, which led companies to adopt these later as mascots for marketing purposes.[17][18]

https://en.wikipedia.org/wiki/History_of_video_games

- **Second generation consoles, shift to Japan and new gaming industry model**

At the start of the second generation, all games were developed and produced in-house. Four former Atari programmers, having left from conflicts in management style after Atari was purchased by Warner Communications in 1976, established Activision in 1979 to develop their own VCS games, which included Kaboom! and Pitfall!. Atari sued Activision on the basis of theft of trade secrets and violation of their non-disclosure agreements, but the two companies settled out of court in 1982, with Activision agreeing to pay a small license fee to Atari for every game of theirs their sold. This established Activision as the first third-party developer for a console. This also established a working model for licensing other third-party developers, which several companies followed in Activision's wake, partially contributing to the video game crash of 1983 due to oversaturation.[19]

As the second generation of consoles coincided with the golden age of arcade video games, a common trend that emerged during the generation was licensing arcade video games for consoles. Many of them were increasingly licensed from Japanese video game companies by 1980, which led to Jonathan Greenberg of Forbes predicting in early 1981 that Japanese companies would eventually dominate the North American video game industry later in the decade.[20]

At this stage, both consoles and game cartridges were intended to be sold for profit by manufacturers. However, by segregating games from the console, this approach established the use of the razorblade business model in future console generations, where consoles would be sold at or below cost while licensing fees from third-party games would bring in profits.[21][22]

https://en.wikipedia.org/wiki/Second_generation_of_video_game_consoles

- **8 bits game consoles - Third generation video games consoles (1983)**

In the history of video games, the third generation of game consoles, commonly referred to as the 8-bit era, began on July 15, 1983 with the Japanese release of two systems: Nintendo's Family Computer (commonly abbreviated to **Famicom**) and **Sega's SG-1000**.[1][2] When the Famicom was released outside of Japan it was remodelled and marketed as the **Nintendo Entertainment System (NES)**. This generation marked the end of the video game crash of 1983, and a shift in the dominance of home video game manufacturers from the United States to Japan.[3] Handheld consoles were not a major part of this generation, although the Game & Watch line from Nintendo (which started in 1980) and the Milton Bradley Microvision (which came out in 1979) were sold at the time. However, both are considered second generation hardware.
The best-selling console of this generation was the **NES/Famicom** from Nintendo, followed by the **Sega Master System** (the improved successor to the SG-1000), and the **Atari 7800**. Although the previous generation of consoles had also used 8-bit processors, it was at the end of the third generation that home consoles were first labeled and marketed by their "bits". 

https://en.wikipedia.org/wiki/Third_generation_of_video_game_consoles

- **16 bits game consoles - Fourth generation video games consoles (1987)**

In the history of video games, the fourth generation of game consoles, more commonly referred to as the 16-bit era, began on October 30, 1987, with the Japanese release of **NEC Home Electronics**' PC Engine (known as the TurboGrafx-16 in North America).
Although NEC released the first console of this era, sales were mostly dominated by the rivalry between Sega and Nintendo across most markets: the Sega Mega Drive (named the Sega Genesis in North America) and the **Super Nintendo Entertainment System **(SNES; the Super Famicom in Japan). Cartridge-based handheld consoles became prominent during this time, dominated by the **Nintendo Game Boy** (1989). Color handhelds were also released, including the Atari Lynx (1989) and Sega Game Gear (1990).

The first handheld game console released in the fourth generation was the Game Boy, on April 21, 1989. It went on to dominate handheld sales by an extremely large margin, despite featuring a 8-bit microprocessor and a low-contrast, unlit monochrome screen while all three of its leading competitors had color. Three major franchises made their debut on the Game Boy: Tetris, the Game Boy's killer application; Pokémon; and Kirby. With some design (Game Boy Pocket, Game Boy Light) and hardware (Game Boy Color) changes, it continued in production in some form until 2008, enjoying a better than 18-year run.
https://en.wikipedia.org/wiki/Fourth_generation_of_video_game_consoles

- **Nintendo Gameboy (1989) - Nintendo**
Cartridge-based handheld consoles became prominent during this time, dominated by the Nintendo Game Boy (1989).
Color handhelds were also released, including the Atari Lynx (1989) and Sega Game Gear (1990).
https://en.wikipedia.org/wiki/Fourth_generation_of_video_game_consoles

- **Recovery and decline of the arcade market (1986-1989)**

The arcade market had recovered by 1986, with the help of software conversion kits, the arrival of popular **beat 'em up games** (such as Kung-Fu Master and Renegade), and advanced motion simulator games (such as Sega's "taikan" games including Hang-On, Space Harrier and Out Run). However, the growth of home video game systems such as the Nintendo Entertainment System led to another brief arcade decline towards the end of the 1980s

https://en.wikipedia.org/wiki/Arcade_game#History

### Automation

- **CEBus (1984-1992)**

CEBus(r), short for Consumer Electronics Bus, also known as EIA-600, is a set of electrical standards and communication protocols for electronic devices to transmit commands and data. It is suitable for devices in households and offices to use, and might be useful for utility interface and light industrial applications.
In 1984, members of the Electronic Industries Alliance (EIA) identified a need for standards that included more capability than the de facto home automation standard X10. X10 provided blind transmission of the commands ON, OFF, DIM, BRIGHT, ALL LIGHTS ON, and ALL UNITS OFF over powerline carrier, and later infrared and short range radio mediums. Over a six-year period, engineers representing international companies met on a regular basis and developed a proposed standard. They called this standard CEBus (pronounced "see bus"). The CEBus standard was released in September 1992.
CEBus is an open architecture set of specification documents which define protocols for products to communicate through power line wire, low voltage twisted pair wire, coaxial cable, infrared, RF, and fiber optics.
The CEBus Standard was developed on the foundation of an IR (infrared) protocol developed by GE (General Electric). This work was transferred to the EIA at the beginning of the EIA's involvement, under the plan that it would be expanded then maintained by the EIA.

https://en.wikipedia.org/wiki/CEBus

## Some standards and protocols
**[`^        back to top        ^`](#)**

### Physical layer ###
- **G.652 (1984)** : 

international standard that describes the geometrical, mechanical, and transmission attributes of a **single-mode optical fibre and cable**, developed by the Standardization Sector of the International Telecommunication Union (ITU-T) that specifies the most popular type of single-mode optical fiber (SMF) cable

https://en.wikipedia.org/wiki/G.652

- **IEEE 802.3 (1983) - 10BASE5**
IEEE 802.3 is a working group and a collection of Institute of Electrical and Electronics Engineers (IEEE) standards produced by the working group defining the physical layer and data link layer's media access control (MAC) of wired Ethernet. This is generally a local area network (LAN) technology with some wide area network (WAN) applications. Physical connections are made between nodes and/or infrastructure devices (hubs, switches, routers) by various types of copper or fiber cable.

802.3 is a technology that supports the IEEE 802.1 network architecture.

802.3 also defines LAN access method using CSMA/CD.

IEEE 802.3 standard	1983	10BASE5 10 Mbit/s (1.25 MB/s) over thick coax. Same as Ethernet II (above) except Type field is replaced by Length, and an 802.2 LLC header follows the 802.3 header. Based on the CSMA/CD Process.

https://en.wikipedia.org/wiki/IEEE_802.3

### Link layer ###

- **802.3d	-	Fiber-optic inter-repeater link, replaced by 10BASE-FL in 1993 (1987)**

10BASE-F, or sometimes 10BASE-FX, is a generic term for the family of 10 Mbit/s Ethernet standards using fiber optic cable. In 10BASE-F, the 10 represents a maximum throughput of 10 Mbit/s, BASE indicates its use of baseband transmission, and F indicates that it relies on medium of fiber-optic cable. The technical standard requires two strands of 62.5/125 µm multimode fiber. 

Fiber-optic inter-repeater link (FOIRL) is a specification of Ethernet over optical fiber. It was especially designed as a back-to-back transport between repeater hubs as to decrease latency and collision detection time, thus increasing the possible network radius. It was replaced by 10BASE-FL.[1]

https://en.wikipedia.org/wiki/Classic_Ethernet#FOIRL

- **ARP - Address Resolution Protocol (1982)**

communication protocol used for discovering the link layer address, such as a MAC address, associated with a given internet layer address, typically an IPv4 address.  Essential to the internet.

https://en.wikipedia.org/wiki/Address_Resolution_Protocol

### Network layer ###
- **IPv4 - Internet Protocol version 4 (1981)**

the fourth version of the Internet Protocol (IP). Described in IETF publication RFC 791 (September 1981). First version deployed for production on SATNET in 1982 and on the ARPANET in January 1983. Essential to networking and the internet. 

https://en.wikipedia.org/wiki/IPv4

- **BGP - Border Gateway Protocol (1989)** :  exterior gateway protocol designed to exchange routing and reachability information among autonomous systems (AS) on the Internet. Essential to the internet. https://en.wikipedia.org/wiki/Border_Gateway_Protocol

- **IGMP - Internet Group Management Protocol (1989)** : a communications protocol used by hosts and adjacent routers on IPv4 networks to establish multicast group memberships. IGMP is an integral part of IP multicast and allows the network to direct multicast transmissions only to hosts that have requested them.
IGMP can be used for one-to-many networking applications such as online streaming video and gaming, and allows more efficient use of resources when supporting these types of applications.
https://en.wikipedia.org/wiki/Internet_Group_Management_Protocol

### Transport layer ###
- **UDP - User_Datagram_Protocol (1980)** : Simple message-oriented transport layer protocol (reception is not certain).  Essential to the internet. 
https://en.wikipedia.org/wiki/User_Datagram_Protocol 

### Session layer ###

- **NetBIOS (1983-1986)**

NetBIOS (/ˈnɛtbaɪɒs/) is an acronym for Network Basic Input/Output System. It provides services related to the session layer of the OSI model allowing applications on separate computers to communicate over a local area network. As strictly an API, NetBIOS is not a networking protocol. Older operating systems[clarification needed] ran NetBIOS over IEEE 802.2 and IPX/SPX using the NetBIOS Frames (NBF) and NetBIOS over IPX/SPX (NBX) protocols, respectively. In modern networks, NetBIOS normally runs over TCP/IP via the NetBIOS over TCP/IP (NBT) protocol. This results in each computer in the network having both an IP address and a NetBIOS name corresponding to a (possibly different) host name. NetBIOS is also used for identifying system names in TCP/IP (Windows). Simply saying, it is a protocol that allows communication of files and printers through the Session Layer of the OSI Model in a LAN.[clarification needed]

NetBIOS is a non-routable OSI Session Layer 5 Protocol and a service that allows applications on computers to communicate with one another over a local area network (LAN). NetBIOS was developed in 1983 by Sytek Inc. as an API for software communication over IBM PC Network LAN technology.[1] On IBM PC Network, as an API alone, NetBIOS relied on proprietary Sytek networking protocols for communication over the wire.[citation needed] Despite supporting a maximum of 80 PCs in a LAN, NetBIOS became an industry standard.[1]

In 1985, IBM went forward with the Token Ring network scheme and a NetBIOS emulator was produced to allow NetBIOS-aware applications from the PC-Network era to work over this new design. This emulator, named NetBIOS Extended User Interface (NetBEUI), expanded the base NetBIOS API with, among other things, the ability to deal with the greater node capacity of Token Ring. A new networking protocol, NBF, was simultaneously produced to allow NetBEUI (NetBIOS) to provide its services over Token Ring – specifically, at the IEEE 802.2 Logical Link Control layer.

In 1985, Microsoft created a NetBIOS implementation for its MS-Net networking technology. As in the case of IBM's Token Ring, the services of Microsoft's NetBIOS implementation were provided over the IEEE 802.2 Logical Link Control layer by the NBF protocol.[citation needed] Until Microsoft adopted Domain Name System (DNS) resolution of hostnames, Microsoft operating systems used NetBIOS to resolve names in Windows client-server networks.[1]

In 1986, Novell released Advanced Novell NetWare 2.0 featuring the company's own NetBIOS emulator. Its services were encapsulated within NetWare's IPX/SPX protocol using the NetBIOS over IPX/SPX (NBX) protocol.

In 1987, a method of encapsulating NetBIOS in TCP and UDP packets, NetBIOS over TCP/IP (NBT), was published. It was described in RFC 1001 ("Protocol Standard for a NetBIOS Service on a TCP/UDP Transport: Concepts and Methods") and RFC 1002 ("Protocol Standard for a NetBIOS Service on a TCP/UDP Transport: Detailed Specifications"). The NBT protocol was developed in order to "allow an implementation [of NetBIOS applications] to be built on virtually any type of system where the TCP/IP protocol suite is available," and to "allow NetBIOS interoperation in the Internet."

After the PS/2 computer hit the market in 1987, IBM released the PC LAN Support Program, which included a driver for NetBIOS.

Since its original publishing in a technical reference book from IBM, the NetBIOS API specification has become a de facto standard.

NetBIOS provides three distinct services:
- Name service (NetBIOS-NS) for name registration and resolution.
- Datagram distribution service (NetBIOS-DGM) for connectionless communication.
- Session service (NetBIOS-SSN) for connection-oriented communication.

https://en.wikipedia.org/wiki/NetBIOS

### Application layer ###
- **TFTP - Trivial File Transfer Protoco (1981)**

a simple lockstep File Transfer Protocol which allows a client to get a file from or put a file onto a remote host. One of its primary uses is in the early stages of nodes booting from a local area network. 
Insecure (no login or access control mechanism) but still in use. was mostly superseded by FTP. 

https://en.wikipedia.org/wiki/Trivial_File_Transfer_Protocol

- **NICNAME/WHOIS - WHOIS (1982)** 
 
Who is responsible for the domain, IP. Essential to the internet. 
 
https://en.wikipedia.org/wiki/WHOIS
 
- **NTP - Network_Time_Protocol (1985)** 

used for network time coordination between computers. Essential to networking.

https://en.wikipedia.org/wiki/Network_Time_Protocol

- **DNS - domain name server (1987)**

hierarchical and decentralized naming system used to identify computers reachable through the Internet or other Internet Protocol (IP) networks. The resource records contained in the DNS associate domain names with other forms of information. These are most commonly used to map human-friendly domain names to the numerical IP addresses computers.
Essential to the internet and networking

https://en.wikipedia.org/wiki/Domain_Name_System

- **SNMP - Simple Network Management Protocol (1988)**

protocol for collecting and organizing information about managed devices on IP networks and for modifying that information to change device behaviour. Devices that typically support SNMP include cable modems, routers, switches, servers, workstations, printers, and more. Widely used in network management for network monitoring.

https://en.wikipedia.org/wiki/Simple_Network_Management_Protocol

### Data transmission ###

#### Text transmission
- **SMTP - Email sending protocol (1980)**.

Essential to Emails. 
https://en.wikipedia.org/wiki/Simple_Mail_Transfer_Protocol

- **ASN.1 - Abstract Syntax Notation One (1984)**

A standard interface description language for defining data structures that can be serialized and deserialized in a cross-platform way. It is broadly used in telecommunications and computer networking, and especially in cryptography.[1]

https://en.wikipedia.org/wiki/ASN.1

- **POP1 - Post Office Protocol (1984)**

Email reception protocol. Essential to Emails (POP3). 

https://en.wikipedia.org/wiki/Post_Office_Protocol

- **RARP - Reverse Address Resolution Protocol (1984)**
 
for the configuration of simple devices, replaced by BOOTP 

https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol

- **X.400 (1984)**

a suite of ITU-T Recommendations that defines the ITU-T **Message Handling System (MHS)**. At one time, the designers of X.400 were expecting it to be the predominant form of email, but this role has been taken by the SMTP-based Internet e-mail.

https://en.wikipedia.org/wiki/X.400

- **IMAP2 : Internet Message Access Protocol (1988)** 

email synchronization protocol. does not delete automatically emails on the server. Essential to emails (IMAP4) 
https://en.wikipedia.org/wiki/Internet_Message_Access_Protocol

- **WWW (1989)**

Research at CERN in Switzerland by the British computer scientist Tim Berners-Lee in 1989–90 resulted in the World Wide Web, linking hypertext documents into an information system, accessible from any node on the network.

https://en.wikipedia.org/wiki/History_of_the_Internet

#### Images

- **BMP - Bitmap image file, GIF ancestor (1981) - IBM and Microsoft**

The BMP file format, also known as bitmap image file, device independent bitmap (DIB) file format and bitmap, is a raster graphics image file format used to store bitmap digital images, independently of the display device (such as a graphics adapter), especially on Microsoft Windows[2] and OS/2[3] operating systems.
The BMP file format is capable of storing two-dimensional digital images both monochrome and color, in various color depths, and optionally with data compression, alpha channels, and color profiles. The Windows Metafile (WMF) specification covers the BMP file format.[4]

https://en.wikipedia.org/wiki/BMP_file_format

Microsoft Corporation and IBM Corporation needed to record images in a format that their applications and operating systems could easily  render on low-end machines (Intel 80286).  The resulting "BMP" format contains a single raster graphic with basic header fields that can be
easily mapped (or "blitted") to locations in memory.  As computing moved from 16-bit to 32-bit, BMP evolved to contain 32-bit structures. 

https://datatracker.ietf.org/doc/html/rfc7903

- **ILBM - Interleaved Bitmap (1985) - Electronic Arts**

Interleaved Bitmap (ILBM) is an image file format conforming to the Interchange File Format (IFF) standard. The format originated on the Amiga platform, and on IBM-compatible systems, files in this format or the related PBM (Planar Bitmap) format are typically encountered in games from late 1980s and early 1990s that were either Amiga ports or had their graphical assets designed on Amiga machines.[citation needed]
A characteristic feature of the format is that it stores bitmaps in the form of interleaved bit planes, which gives the format its name; this reflects the way the Amiga graphics hardware natively reads graphics data from memory. A simple form of compression is supported to make ILBM files more compact.[4]
On the Amiga, these files are not associated with a particular file extension, though as they started being used on PC systems where extensions are systematically used, they employed a .lbm or occasionally a .bbm extension.

https://en.wikipedia.org/wiki/ILBM

- **GIF - Bitmap image file, PNG ancestor (1987)**

Graphics Interchange Format (GIF; /ɡɪf/ GHIF or /dʒɪf/ JIF , see pronunciation) is a bitmap image format that was developed by a team at the online services provider CompuServe led by American computer scientist Steve Wilhite and released on 15 June 1987

https://en.wikipedia.org/wiki/GIF

#### Audio and Video

- **IFF - Interchange File Format (1985) - Electronic Arts**

Interchange File Format (IFF), is a generic container file format originally introduced by the Electronic Arts company in 1985 (in cooperation with Commodore) in order to facilitate transfer of data between software produced by different companies.
IFF files do not have any standard extension. On many systems that generate IFF files, file extensions are not important (the OS stores file format metadata separately from the file name). An .iff extension is commonly used for ILBM format files, which use the IFF container format.

https://en.wikipedia.org/wiki/Interchange_File_Format

- **AIFF - Audio Interchange File Format (1988) - Apple**

Audio Interchange File Format (AIFF) is an audio file format standard used for storing sound data for personal computers and other electronic audio devices. The format was developed by Apple Inc. in 1988 based on Electronic Arts' Interchange File Format (IFF, widely used on Amiga systems) and is most commonly used on Apple Macintosh computer systems.

The audio data in most AIFF files is uncompressed pulse-code modulation (PCM). This type of AIFF file uses much more disk space than lossy formats like MP3—about 10 MB for one minute of stereo audio at a sample rate of 44.1 kHz and a bit depth of 16 bits. There is also a compressed variant of AIFF known as AIFF-C or AIFC, with various defined compression codecs.

https://en.wikipedia.org/wiki/Audio_Interchange_File_Format

#### Video transmission ####
- **H.120 (1984)** - **first digital video compression standard**. 

It was developed by COST 211 and published by the CCITT (now the ITU-T) in 1984, with a revision in 1988.

https://en.wikipedia.org/wiki/H.120

the first digital video coding standard. v1 (1984) featured conditional replenishment, differential PCM (DPCM), scalar quantization, variable-length coding and a switch for quincunx sampling. v2 (1988) added motion compensation and background prediction. This standard was little-used and no codecs exist.
https://en.wikipedia.org/wiki/Video_Coding_Experts_Group

- **H.261(1988) - first digital video compression standard of practical use**

H.261 is an ITU-T video compression standard, first ratified in November 1988.[1][2] It is the first member of the H.26x family of video coding standards in the domain of the ITU-T Study Group 16 Video Coding Experts Group (VCEG, then Specialists Group on Coding for Visual Telephony). It was the first video coding standard that was useful in practical terms.

H.261 was originally designed for transmission over ISDN lines on which data rates are multiples of 64 kbit/s. The coding algorithm was designed to be able to operate at video bit rates between 40 kbit/s and 2 Mbit/s. The standard supports two video frame sizes: CIF (352×288 luma with 176×144 chroma) and QCIF (176×144 with 88×72 chroma) using a 4:2:0 sampling scheme. It also has a backward-compatible trick for sending still images with 704×576 luma resolution and 352×288 chroma resolution (which was added in a later revision in 1993).

https://en.wikipedia.org/wiki/H.261

#### File storage

- **ISO 9660 - CD-ROM file system (1988)**

ISO 9660 (also known as ECMA-119) is a file system for optical disc media. Being sold by the International Organization for Standardization (ISO) the file system is considered an international technical standard. Since the specification is available for anybody to purchase,[1] implementations have been written for many operating systems.

https://en.wikipedia.org/wiki/ISO_9660

#### File compression ####

- **LZW Algorithm (1984)**

Lempel–Ziv–Welch (LZW) is a universal lossless data compression algorithm created by Abraham Lempel, Jacob Ziv, and Terry Welch. It was published by Welch in 1984 as an improved implementation of the LZ78 algorithm published by Lempel and Ziv in 1978. The algorithm is simple to implement and has the potential for very high throughput in hardware implementations.[1] It is the algorithm of the widely used Unix file compression utility compress and is used in the GIF image format.

https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Welch

- **ARC (1986)**

ARC is a lossless data compression and archival format by System Enhancement Associates (SEA). The file format and the program were both called ARC. The format is known as the subject of controversy in the 1980s, part of important debates over what would later be known as open formats.

ARC was extremely popular during the early days of the dial-up BBS. ARC was convenient as it combined the functions of the SQ program to compress files and the LU program to create .LBR archives of multiple files. The format was later replaced by the ZIP format, which offered better compression ratios and the ability to retain directory structures through the compression/decompression process.

In 1985, Thom Henderson of System Enhancement Associates wrote a program called ARC,[3] based on earlier programs such as ar, that not only grouped files into a single archive file but also compressed them to save disk space, a feature of great importance on early personal computers, where space was very limited and modem transmission speeds were very slow. The archive files produced by ARC had file names ending in ".ARC" and were thus sometimes called "arc files".

The source code for ARC was released by SEA in 1986 and subsequently ported to Unix and Atari ST in 1987 by Howard Chu. This more portable codebase was subsequently ported to other platforms, including VAX/VMS and IBM System/370 mainframes. Howard's work was also the first to disprove the prevalent belief that Lempel-Ziv encoded files could not be further compressed. Additional compression could be achieved by using Huffman coding on the LZW data, and Howard's version of ARC was the first program to demonstrate this property. This hybrid technique was later used in several other compression schemes by Phil Katz and others.

https://en.wikipedia.org/wiki/ARC_(file_format)

- **PKZIP (1986) - PKWARE**

PKZIP is a file archiving computer program, notable for introducing the popular ZIP file format. PKZIP was first introduced for MS-DOS on the IBM-PC compatible platform in 1989. Since then versions have been released for a number of other architectures and operating systems. PKZIP was originally written by Phil Katz and marketed by his company PKWARE, Inc starting in 1986. The company bears his initials: 'PK'.

https://en.wikipedia.org/wiki/PKZIP

- **ZIP (1989)**

ZIP is an archive file format that supports lossless data compression. A ZIP file may contain one or more files or directories that may have been compressed. The ZIP file format permits a number of compression algorithms, though DEFLATE is the most common. This format was originally created in 1989 and was first implemented in PKWARE, Inc.'s PKZIP utility,[2] as a replacement for the previous ARC compression format by Thom Henderson.

https://en.wikipedia.org/wiki/ZIP_(file_format)

#### Remote access ####
- **X Window System (X11, or simply X) (1984)**

A windowing system for bitmap displays, common on Unix-like operating systems. Originated as part of Project Athena at Massachusetts Institute of Technology (MIT). At version 11 since 1987.

https://en.wikipedia.org/wiki/X_Window_System

#### Instant messaging ####
- **IRC : Internet Relay Chat (1988)**

text-based chat system for instant messaging.

https://en.wikipedia.org/wiki/Internet_Relay_Chat
https://en.wikipedia.org/wiki/Instant_messaging

#### Machine to Machine ####

- **Modem speed protocols**

  * V.22 1200 bps; fallback to 600 bps ; QDPSK = DPSK (1980)
  * V.22bis 2400 bps; QAM (1984)
  * V.32 9600 bps; QAM (1984 but not widely used until years later)

https://tldp.org/HOWTO/Modem-HOWTO-29.html

- **SMB - Server Message Block - files and printers access across the network (1983)**

Server Message Block (SMB) is a communication protocol[1] originally developed in 1983 by Barry A. Feigenbaum at IBM[2] and intended to provide shared access to files and printers across nodes on a network of systems running IBM's OS/2. It also provides an authenticated inter-process communication (IPC) mechanism. In 1987, Microsoft and 3Com implemented SMB in LAN Manager for OS/2, at which time SMB used the NetBIOS service atop the NetBIOS Frames protocol as its underlying transport. Later, Microsoft implemented SMB in Windows NT 3.1 and has been updating it ever since, adapting it to work with newer underlying transports: TCP/IP and NetBT. SMB implementation consists of two vaguely named Windows services: "Server" (ID: LanmanServer) and "Workstation" (ID: LanmanWorkstation).[3] It uses NTLM or Kerberos protocols for user authentication.

https://en.wikipedia.org/wiki/Server_Message_Block

- **BOOTP - Bootstrap Protocol (1985)** 

introduced the concept of a relay agent, which allowed the forwarding of BOOTP packets across networks, allowing one central BOOTP server to serve hosts on many IP subnets, replaced by DHCP

https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol

#### Electronic Data Exchange

- **ODETTE/OFTP (1986+)**

The Odette File Transfer Protocol (OFTP) is a protocol created in 1986, used for EDI (Electronic Data Interchange) between two communications business partners. Its name comes from the Odette Organisation (the Organization for data exchange by teletransmission in Europe).

The ODETTE File Transfer Protocol (ODETTE-FTP) was defined in 1986 by working group four of the Organisation for Data Exchange by Tele-Transmission in Europe (ODETTE) to address the electronic data interchange (EDI) requirements of the European automotive industry. It was designed in the spirit of the Open System Interconnection (OSI) model utilising the Network Service provided by the CCITT X.25 recommendation.

OFTP 2 was written in 2007 by Data Interchange, as a specification for the secure transfer of business documents over the Internet, ISDN and X.25 networks. A description of OFTP 1.3 can be found in RFC 2204, whilst OFTP 2 is defined in RFC 5024.

OFTP 2 can work point-to-point or indirectly via a VAN (Value Added Network). A single OFTP 2 entity can make and receive calls, exchanging files in both directions.[1] This means that OFTP 2 can work in a push or pull mode, as opposed to AS2, which can only work in a push mode.[2]

OFTP 2 can encrypt and digitally sign message data, request signed receipts and also offers high levels of data compression. All of these services are available when using OFTP 2 over TCP/IP, X.25/ISDN or native X.25. When used over a TCP/IP network such as the Internet, additional session level security is available by using OFTP 2 over Transport Layer Security (TLS).

https://en.wikipedia.org/wiki/OFTP

- **EDIFACT (1987+)**

United Nations/Electronic Data Interchange for Administration, Commerce and Transport (UN/EDIFACT) is an international standard for electronic data interchange (EDI) developed for the United Nations and approved and published by UNECE, the UN Economic Commission for Europe.[1]

In 1987, following the convergence of the UN and US/ANSI syntax proposals, the UN/EDIFACT Syntax Rules were approved as the ISO standard ISO 9735 by the International Organization for Standardization.[2]

The EDIFACT standard provides:

a set of syntax rules to structure data
an interactive exchange protocol (I-EDI)
standard messages which allow multi-country and multi-industry exchange
The work of maintenance and further development of this standard is done through the United Nations Centre for Trade Facilitation and Electronic Business (UN/CEFACT) under the UN Economic Commission for Europe, in the Finance Domain working group UN CEFACT TBG5.

https://en.wikipedia.org/wiki/EDIFACT

## Some programming languages and frameworks
**[`^        back to top        ^`](#)**



## Navigation 
**[`^        back to top        ^`](#)**

**Decision to open GPS to civilian use and deployment of modern GPS satellites**

In 1983, after Soviet interceptor aircraft shot down the civilian airliner KAL 007 that strayed into prohibited airspace because of navigational errors, killing all 269 people on board, U.S. President Ronald Reagan announced that GPS would be made available for civilian uses once it was completed,[54][55] although it had been previously published in Navigation magazine, and that the CA code (Coarse/Acquisition code) would be available to civilian users.[citation needed]

By 1985, ten more experimental Block-I satellites had been launched to validate the concept.

Beginning in 1988, command and control of these satellites was moved from Onizuka AFS, California to the 2nd Satellite Control Squadron (2SCS) located at Falcon Air Force Station in Colorado Springs, Colorado.[56][57]

On February 14, 1989, the first modern Block-II satellite was launched.

https://en.wikipedia.org/wiki/Global_Positioning_System
