# 2000s

**In short:**
- IP convergence
- Multi-mode fiber, OTN
- Internet everywhere 
- Legacy free PCs
- Widescreen
- Transition to SSD starts
- Motherboards : USB/SATA/SAS/PCI-E iterations, Southbridge disappears
- Screens : 16:9, 1366x768, Full HD (1080p = 1920x1080) 
- Blu-Ray era
- NginX web server
- HDTV in Europe 
- Sixth generation video games consoles (PlayStation 2, Xbox, Nintendo Wii)
- Formats : DivX, RDP, JSON, SVG, ISO

## Telecommunications

Whilst the following decade saw a convergence of the aforementioned services (frame relay, ATM, Internet for businesses) with the MPLS integrated offerings.

https://en.wikipedia.org/wiki/Leased_line

## Networking

New development focused on:
- Internet everywhere with Wifi networks and Home networking over legacy wires (G.hn).
- IP/power convergence with Power over Ethernet (PoE) and Broadband over power lines (IEEE 1675-2008).

## Cryptography

## Computers

### Evolution of mainframes

In the late 1990s, corporations found new uses for their existing mainframes and as the price of data networking collapsed in most parts of the world, encouraging trends toward more centralized computing. The growth of e-business also dramatically increased the number of back-end transactions processed by mainframe software as well as the size and throughput of databases. Batch processing, such as billing, became even more important (and larger) with the growth of e-business, and mainframes are particularly adept at large-scale batch computing. Another factor currently increasing mainframe use is the development of the Linux operating system, which arrived on IBM mainframe systems in 1999 and is typically run in scores or up to c. 8,000 virtual machines on a single mainframe. Linux allows users to take advantage of open source software combined with mainframe hardware RAS. Rapid expansion and development in emerging markets, particularly People's Republic of China, is also spurring major mainframe investments to solve exceptionally difficult computing problems, e.g. providing unified, extremely high volume online transaction processing databases for 1 billion consumers across multiple industries (banking, insurance, credit reporting, government services, etc.) In late 2000, IBM introduced 64-bit z/Architecture, acquired numerous software companies such as Cognos and introduced those software products to the mainframe. IBM's quarterly and annual reports in the 2000s usually reported increasing mainframe revenues and capacity shipments. However, IBM's mainframe hardware business has not been immune to the recent overall downturn in the server hardware market or to model cycle effects. For example, in the 4th quarter of 2009, IBM's System z hardware revenues decreased by 27% year over year. But MIPS (millions of instructions per second) shipments increased 4% per year over the past two years.[21] Alsop had himself photographed in 2000, symbolically eating his own words ("death of the mainframe").[22]

https://en.wikipedia.org/wiki/Mainframe_computer

By 2000, modern mainframes partially or entirely phased out classic "green screen" and color display terminal access for end-users in favour of Web-style user interface.

https://en.wikipedia.org/wiki/Mainframe_computer


### Evolution of Hardware

- **Transition to legacy free PCs (2000s)**

As the first decade of the 21st century progressed, the legacy-free PC went mainstream, with legacy ports removed from commonly available computer systems in all form factors. However, the PS/2 keyboard connector still retains some use, as it can offer some uses (e.g. implementation of n-key rollover) not offered by USB.[6]

https://en.wikipedia.org/wiki/Legacy-free_PC

- **Screen Transition from 800x600 to eXtended Graphics Array (XGA) 1024x768 (early 2000s)**

In 2002, 1024 × 768 eXtended Graphics Array was the most common display resolution. Many web sites and multimedia products were re-designed from the previous 800 × 600 format to the layouts optimized for 1024 × 768.
The availability of inexpensive LCD monitors made the 5∶4 aspect ratio resolution of 1280 × 1024 more popular for desktop usage during the first decade of the 21st century. 

https://en.wikipedia.org/wiki/Display_resolution

- **USB 2.0 - Hi-Speed USB (2000)**

USB 2.0 was released in April 2000, adding a higher maximum signaling rate of 480 Mbit/s (maximum theoretical data throughput 53 MByte/s[19]) named High Speed or High Bandwidth, in addition to the USB 1.x Full Speed signaling rate of 12 Mbit/s (maximum theoretical data throughput 1.2 MByte/s[20]).

https://en.wikipedia.org/wiki/USB

- **SATA Hard drives (2000)**

SATA was announced in 2000[5][6] in order to provide several advantages over the earlier PATA interface such as reduced cable size and cost (seven conductors instead of 40 or 80), native hot swapping, faster data transfer through higher signaling rates, and more efficient transfer through an (optional) I/O queuing protocol. Revision 1.0 of the specification was released in January 2003.[3]
Serial ATA (SATA, abbreviated from Serial AT Attachment)[3] is a computer bus interface that connects host bus adapters to mass storage devices such as hard disk drives, optical drives, and solid-state drives. Serial ATA succeeded the earlier Parallel ATA (PATA) standard to become the predominant interface for storage devices.
Serial ATA industry compatibility specifications originate from the Serial ATA International Organization (SATA-IO) which are then promulgated by the INCITS Technical Committee T13, AT Attachment (INCITS T13).

https://en.wikipedia.org/wiki/Serial_ATA

- **Blu-ray Disk (2000-2003) - Sony**

The Blu-ray Disc (BD), often known simply as Blu-ray, is a digital optical disc storage format. It is designed to supersede the DVD format, and capable of storing several hours of high-definition video (HDTV 720p and 1080p). The main application of Blu-ray is as a medium for video material such as feature films and for the physical distribution of video games for the PlayStation 3, PlayStation 4, PlayStation 5, Xbox One, and Xbox Series X. The name "Blu-ray" refers to the blue laser (which is actually a violet laser) used to read the disc, which allows information to be stored at a greater density than is possible with the longer-wavelength red laser used for DVDs.
The BD format was developed by the Blu-ray Disc Association, a group representing makers of consumer electronics, computer hardware, and motion pictures. Sony unveiled the first Blu-ray Disc prototypes in October 2000, and the first prototype player was released in Japan in April 2003. Afterward, it continued to be developed until its official worldwide release on June 20, 2006, beginning the high-definition optical disc format war, where Blu-ray Disc competed with the HD DVD format. Toshiba, the main company supporting HD DVD, conceded in February 2008,[8] and later released its own Blu-ray Disc player in late 2009.[9] According to Media Research, high-definition software sales in the United States were slower in the first two years than DVD software sales.[10] Blu-ray faces competition from video on demand (VOD) and the continued sale of DVDs.[11] In January 2016, 44% of U.S. broadband households had a Blu-ray player.[12]

https://en.wikipedia.org/wiki/Blu-ray

- **HyperTransport (HT) - interconnection of computer processors (2001) - AMD**

HyperTransport (HT), formerly known as Lightning Data Transport (LDT), is a technology for interconnection of computer processors. It is a bidirectional serial/parallel high-bandwidth, low-latency point-to-point link that was introduced on April 2, 2001.[1] The HyperTransport Consortium is in charge of promoting and developing HyperTransport technology.

HyperTransport is best known as the system bus architecture of AMD central processing units (CPUs) from Athlon 64 through AMD FX and the associated motherboard chipsets. HyperTransport has also been used by IBM and Apple for the Power Mac G5 machines, as well as a number of modern MIPS systems.

The current specification HTX 3.1 remained competitive for 2014 high-speed (2666 and 3200 MT/s or about 10.4 GB/s and 12.8 GB/s) DDR4 RAM and slower (around 1 GB/s [1] similar to high end PCIe SSDs ULLtraDIMM flash RAM) technology[clarification needed]—a wider range of RAM speeds on a common CPU bus than any Intel front-side bus. Intel technologies require each speed range of RAM to have its own interface, resulting in a more complex motherboard layout but with fewer bottlenecks. HTX 3.1 at 26 GB/s can serve as a unified bus for as many as four DDR4 sticks running at the fastest proposed speeds. Beyond that DDR4 RAM may require two or more HTX 3.1 buses diminishing its value as unified transport.

https://en.wikipedia.org/wiki/HyperTransport

- **PCI Express 1.0a (2003) - PCI-SIG**

Transfer rate is expressed in transfers per second instead of bits per second because the number of transfers includes the overhead bits, which do not provide additional throughput;[48] PCIe 1.x uses an 8b/10b encoding scheme, resulting in a 20% (= 2/10) overhead on the raw channel bandwidth.[49] So in the PCIe terminology, transfer rate refers to the encoded bit rate: 2.5 GT/s is 2.5 Gbps on the encoded serial link. This corresponds to 2.0 Gbps of pre-coded data or 250 MB/s, which is referred to as throughput in PCIe.
While in early development, PCIe was initially referred to as HSI (for High Speed Interconnect), and underwent a name change to 3GIO (for 3rd Generation I/O) before finally settling on its PCI-SIG name PCI Express. A technical working group named the Arapaho Work Group (AWG) drew up the standard. For initial drafts, the AWG consisted only of Intel engineers; subsequently, the AWG expanded to include industry partners.
In 2003, PCI-SIG introduced PCIe 1.0a, with a per-lane data rate of 250 MB/s and a transfer rate of 2.5 gigatransfers per second (GT/s).
Since, PCIe has undergone several large and smaller revisions, improving on performance and other features.
Transfer rate is expressed in transfers per second instead of bits per second because the number of transfers includes the overhead bits, which do not provide additional throughput;[48] PCIe 1.x uses an 8b/10b encoding scheme, resulting in a 20% (= 2/10) overhead on the raw channel bandwidth.[49] So in the PCIe terminology, transfer rate refers to the encoded bit rate: 2.5 GT/s is 2.5 Gbps on the encoded serial link. This corresponds to 2.0 Gbps of pre-coded data or 250 MB/s, which is referred to as throughput in PCIe.

https://en.wikipedia.org/wiki/PCI_Express

- **Non-volatile RAM prototypes (2003-2004)**

Several new types of non-volatile RAM, which preserve data while powered down, are under development. The technologies used include carbon nanotubes and approaches utilizing Tunnel magnetoresistance. Amongst the 1st generation MRAM, a 128 kbit (128 × 210 bytes) chip was manufactured with 0.18 µm technology in the summer of 2003.[citation needed] 

In June 2004, Infineon Technologies unveiled a 16 MB (16 × 220 bytes) prototype again based on 0.18 µm technology. There are two 2nd generation techniques currently in development: thermal-assisted switching (TAS)[28] which is being developed by Crocus Technology, and spin-transfer torque (STT) on which Crocus, Hynix, IBM, and several other companies are working.[29] Nantero built a functioning carbon nanotube memory prototype 10 GB (10 × 230 bytes) array in 2004. Whether some of these technologies can eventually take significant market share from either DRAM, SRAM, or flash-memory technology, however, remains to be seen.

https://en.wikipedia.org/wiki/Random-access_memory#History

- **SAS-1 3GB/s - Serial Attached SCSI (2004)**

In computing, Serial Attached SCSI (SAS) is a point-to-point serial protocol that moves data to and from computer-storage devices such as hard disk drives and tape drives. SAS replaces the older Parallel SCSI (Parallel Small Computer System Interface, usually pronounced "scuzzy" or "sexy"[3][4]) bus technology that first appeared in the mid-1980s. SAS, like its predecessor, uses the standard SCSI command set. SAS offers optional compatibility with Serial ATA (SATA), versions 2 and later. This allows the connection of SATA drives to most SAS backplanes or controllers. The reverse, connecting SAS drives to SATA backplanes, is not possible.[5]

The T10 technical committee of the International Committee for Information Technology Standards (INCITS) develops and maintains the SAS protocol; the SCSI Trade Association (SCSITA) promotes the technology.
SAS-1: 3.0 Gbit/s, introduced in 2004[7]

https://en.wikipedia.org/wiki/Serial_Attached_SCSI

- **DMI - Link between northbridge and southbridge on computer motherboard (2004) - Intel**

In computing, Direct Media Interface (DMI) is Intel's proprietary link between the northbridge and southbridge on a computer motherboard. It was first used between the 9xx chipsets and the ICH6, released in 2004.

https://en.wikipedia.org/wiki/Direct_Media_Interface

- **16:10 widescreens on latops (2004)**

The popular widescreen format (16:10) appeared in 2004 originally in laptops (14″ / 15″ had 1280×800, 17″ had 1440×900), next year desktop monitors appeared in this format (19″ 1440×900, 22″ 1680×1050, 24″ 1920×1200) and high-end laptops received such resolutions. Widescreen monitors come with VGA and DVI ports, running 4:3 resolutions on VGA cable will make image stretching on full screen, while DVI cable maintain aspect ratio and display black bars.

https://www.teoalida.com/webdesign/screen-resolution/

- **NVMHCI 1.0 - NVMe (2007-2009) Intel**

The first details of a new standard for accessing non-volatile memory emerged at the Intel Developer Forum 2007, when NVMHCI was shown as the host-side protocol of a proposed architectural design that had Open NAND Flash Interface Working Group (ONFI) on the memory (flash) chips side.[15] A NVMHCI working group led by Intel was formed that year. The NVMHCI 1.0 specification was completed in April 2008 and released on Intel's web site.[16][17][18]

Technical work on NVMe began in the second half of 2009.[19] The NVMe specifications were developed by the NVM Express Workgroup, which consists of more than 90 companies; Amber Huffman of Intel was the working group's chair. Version 1.0 of the specification was released on 1 March 2011,[20] while version 1.1 of the specification was released on 11 October 2012.[21] Major features added in version 1.1 are multi-path I/O (with namespace sharing) and arbitrary-length scatter-gather I/O. It is expected that future revisions will significantly enhance namespace management.[19] Because of its feature focus, NVMe 1.1 was initially called "Enterprise NVMHCI".[22] An update for the base NVMe specification, called version 1.0e, was released in January 2013.[23] In June 2011, a Promoter Group led by seven companies was formed.

https://en.wikipedia.org/wiki/NVM_Express

- **Southbridge disappears (2008+) - Intel/AMD**

The southbridge is one of the two chips in the core logic chipset on a personal computer (PC) motherboard, the other being the northbridge. The southbridge typically implements the slower capabilities of the motherboard in a northbridge/southbridge chipset computer architecture. In systems with Intel chipsets, the southbridge is named I/O Controller Hub (ICH), while AMD has named its southbridge Fusion Controller Hub (FCH) since the introduction of its Fusion AMD Accelerated Processing Unit (APU) while moving the functions of the Northbridge onto the CPU die, hence making it similar in function to the Platform hub controller.
The southbridge can usually be distinguished from the northbridge by not being directly connected to the CPU. Rather, the northbridge ties the southbridge to the CPU. Through the use of controller integrated channel circuitry, the northbridge can directly link signals from the I/O units to the CPU for data control and access.
Due to the push for system-on-chip (SoC) processors, modern devices increasingly have the northbridge integrated into the CPU die itself;[further explanation needed] examples are Intel's Sandy Bridge[1] and AMD's Fusion processors,[2] both released in 2011. The southbridge became redundant and it was replaced by the Platform Controller Hub (PCH) architecture introduced with the Intel 5 Series chipset in 2008 while AMD did the same with the release of their first APUs in 2011, naming the PCH the Fusion controller hub (FCH), which was only used on AMD's APUs until 2017 when it began to be used on AMD's Zen architecture while dropping the FCH name. On Intel platforms, all southbridge features and remaining I/O functions are managed by the PCH which is directly connected to the CPU via the Direct Media Interface (DMI).[3] Intel low power processor (Haswell-U and onward) and Ultra low power processor (Haswell-Y and onward) also integrated an on-package PCH. Based on its Chiplet design, AMD Ryzen processors also integrated some southbridge functions, such as some USB interface and some SATA/NVMe interface.[4]

https://en.wikipedia.org/wiki/Southbridge_(computing)

- **USB 3.x - SuperSpeed USB logo (2008)**

The USB 3.0 specification was released on 12 November 2008, with its management transferring from USB 3.0 Promoter Group to the USB Implementers Forum (USB-IF), and announced on 17 November 2008 at the SuperSpeed USB Developers Conference.[23]
USB 3.0 adds a SuperSpeed transfer mode, with associated backward compatible plugs, receptacles, and cables. SuperSpeed plugs and receptacles are identified with a distinct logo and blue inserts in standard format receptacles.

https://en.wikipedia.org/wiki/USB

- **Intel QuickPath Interconnect - Replaced Front-side bus (2008) - Intel**

The Intel QuickPath Interconnect (QPI)[1][2] is a point-to-point processor interconnect developed by Intel which replaced the front-side bus (FSB) in Xeon, Itanium, and certain desktop platforms starting in 2008. It increased the scalability and available bandwidth. Prior to the name's announcement, Intel referred to it as Common System Interface (CSI).[3] Earlier incarnations were known as Yet Another Protocol (YAP) and YAP+.
QPI 1.1 is a significantly revamped version introduced with Sandy Bridge-EP (Romley platform).[4]
QPI was replaced by Intel Ultra Path Interconnect (UPI) in Skylake-SP Xeon processors based on LGA 3647 socket.[5]

https://en.wikipedia.org/wiki/Intel_QuickPath_Interconnect

- **Demonstration of 1TB SSD on PCI Express x8 (2009) - OCZ Technology**

At Cebit 2009, OCZ Technology demonstrated a 1 TB[47] flash SSD using a PCI Express ×8 interface. It achieved a maximum write speed of 0.654 gigabytes per second (GB/s) and maximum read speed of 0.712 GB/s.[48] In December 2009, Micron Technology announced an SSD using a 6 gigabits per second (Gbit/s) SATA interface.[49]

https://en.wikipedia.org/wiki/Solid-state_drive

- **Transition from XGA to 1366x768 screens (late 2000s)**

XGA 1024×768 had over 50% marketshare before 2007 and was overtaken by 1366×768 in March 2012 which reached a peak of 27%+ in 2015. Next popular were 1280×800 with a peak of 19.72% in Nov 2009 and 1280×1024 with peak in 2007

https://www.teoalida.com/webdesign/screen-resolution/

### Evolution of uses

#### Servers
- **Nginx - Most popular web server, superseded Apache (2004) - Nginx**

Nginx (pronounced "engine x"[9] /ˌɛndʒɪnˈɛks/ EN-jin-EKS) is a web server that can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache. The software was created by Igor Sysoev and publicly released in 2004.[10

As of June 2022 W3Tech's web server count of all web sites ranked Nginx first with 33.6%. Apache was second at 31.4% and Cloudflare Server third at 21.6%

https://en.wikipedia.org/wiki/Nginx

#### Clients
##### Gui

- **WindowsXP SP2 - Bluetooth mice support (2004) - Microsoft**

Windows XP Service Pack 2 introduced a Bluetooth stack, allowing Bluetooth mice to be used without any USB receivers.[108] 
Windows Vista added native support for horizontal scrolling and standardized wheel movement granularity for finer scrolling.[106]

https://en.wikipedia.org/wiki/Computer_mouse

## Consumer electronics

### TV

- **Commercial TV projectors (2004) - Sony**

Sony was offering 4K projectors as early as 2004

https://en.wikipedia.org/wiki/4K_resolution

- **HDTV in Europe (2004) - Belgium**

The first regular broadcasts started on January 1, 2004, when the Belgian company Euro1080 launched the HD1 channel with the traditional Vienna New Year's Concert. Test transmissions had been active since the IBC exhibition in September 2003, but the New Year's Day broadcast marked the official launch of the HD1 channel, and the official start of direct-to-home HDTV in Europe.[44]

https://en.wikipedia.org/wiki/High-definition_television

- **16:9 widescreen TV standards (2008)**

Television industry adopted 16:9 standard in 2008, and because is cheaper to produce both monitors and TV with same aspect ratio, 14″ / 15″ laptops became 1366×768, 17″ laptops became 1600×900, high-end laptops ($1000+) used 1920×1080. New desktop monitors were 18.5″ 1366×768, 20″ 1600×900, 21.5″ / 23″ 1920×1080. QHD, 27″ 2560×1440 appeared in 2009. LG G3 was in 2013 first smartphone with QHD display. Mini-laptops with 10″ screens used 1024×600, called netbooks (smaller than 14-17″ notebooks).

https://www.teoalida.com/webdesign/screen-resolution/

### Music

Beginning in the 2000s, CDs were increasingly being replaced by other forms of digital storage and distribution, with the result that by 2010 the number of audio CDs being sold in the U.S. had dropped about 50% from their peak; however, they remained one of the primary distribution methods for the music industry.

With the advent and popularity of Internet-based distribution of files in lossy-compressed audio formats such as MP3, sales of CDs began to decline in the 2000s. For example, between 2000 and 2008, despite overall growth in music sales and one anomalous year of increase, major-label CD sales declined overall by 20%,[48] although independent and DIY music sales may be tracking better according to figures released 30 March 2009, and CDs still continue to sell greatl

https://en.wikipedia.org/wiki/Compact_Disc_Digital_Audio

### Video Games

- **128 bits console - Sixth generation video games consoles : Dreamcast, PS2, GameCube, Xbox (1998-2005)**

In the history of video games, the sixth-generation era (sometimes called the 128-bit era; see "bits and system power" below) is the era of computer and video games, video game consoles, and handheld gaming devices available at the turn of the 21st century, starting on November 27, 1998. Platforms in the sixth generation include consoles from four companies: the **Sega Dreamcast (DC)**, **Sony PlayStation 2 (PS2)**, **Nintendo GameCube (GC)**, and **Microsoft Xbox**. This era began on November 27, 1998, with the Japanese release of the Dreamcast, which was joined by the **PlayStation 2 on March 4, 2000**, and the Xbox and Gamecube on November 15 and 18, 2001, respectively. In April 2001, the Dreamcast was the first to be discontinued. Xbox was next in 2006, GameCube in 2007 and PlayStation 2 was the last, in January 2013. Meanwhile, the seventh generation of consoles started on November 22, 2005 with the launch of the Xbox 360.[1]

The major innovation of this generation was of full utilization of the internet to allow a fully online gaming experience. While the prior generation had some systems with internet connectivity, such as the Apple Pippin, these had little market penetration and thus had limited success in the area. Services such as Microsoft's Xbox Live became industry standard in this, and future, generations. Another innovation of the Xbox was the first system to utilize an internal hard disk drive to store game data. This caused many improvements to the gaming experience, including the ability to store program data (rather than just save game data) that allowed for faster load times, as well as the ability to download games directly from the internet rather than to purchase physical media such as a disk or cartridge. Soon after its release other systems, like the Sony PlayStation 2, produced peripheral storage devices to allow similar capabilities, and by the next generation internal storage became industry standard.

Bit ratings (i.e. "64-bit" or "32-bit" for the previous generation) for most consoles largely fell by the wayside during this era, with the notable exceptions being promotions for the Dreamcast[2] and PS2[3] that advertised "128-bit graphics" at the start of the generation. 

The sixth generation of handhelds began with the release of the Neo Geo Pocket Color by SNK in 1998 and Bandai's WonderSwan Color, launched in Japan in 1999. Nintendo maintained its dominant share of the handheld market with the release in 2001 of the Game Boy Advance, which featured many upgrades and new features over the Game Boy. The Game Boy Advance was discontinued around in early 2010. The next generation of handheld consoles began in November 2004, with the North American introduction of the Nintendo DS.

https://en.wikipedia.org/wiki/Sixth_generation_of_video_game_consoles

- **Sony PlayStation 2 - Best selling console (2000) - Sony**

Its successor, the PlayStation 2, was released in 2000. The PlayStation 2 is the best-selling home console to date, having reached over 155 million units sold by the end of 2012.[3]

https://en.wikipedia.org/wiki/PlayStation

The PlayStation 2 (PS2) is a home video game console developed and marketed by Sony Computer Entertainment. It was first released in Japan on 4 March 2000, in North America on 26 October 2000, in Europe on 24 November 2000, and in Australia on 30 November 2000. It is the successor to the original PlayStation, as well as the second installment in the PlayStation brand of consoles. As a sixth-generation console, it competed with Sega's Dreamcast, Nintendo's GameCube, and Microsoft's Xbox.

The PlayStation 2 received widespread critical acclaim upon release, and is the best-selling video game console of all time, having sold over 155 million units worldwide. A total of 4,000 game titles were released, with over 1.5 billion copies sold.[14] In 2004, Sony released a smaller, lighter revision of the console known as the PS2 Slim. Even after the release of its successor, the PlayStation 3, it remained popular well into the seventh generation. It continued to be produced until 2013 when Sony finally announced that it had been discontinued after over twelve years of production, one of the longest lifespans of any video game console. New games for the console continued to be made until the end of its life.

Software for the PlayStation 2 was distributed primarily on DVD-ROMs,[59] with some titles being published on blue-tinted CD-ROM format. In addition, the console can play audio CDs and DVD films and is backward-compatible with almost all original PlayStation games.[56] The PlayStation 2 also supports PlayStation memory cards and controllers, although original PlayStation memory cards will only work with original PlayStation games[60] and the controllers may not support all functions (such as analogue buttons) for PlayStation 2 games.

Later reviews, especially after the launch of the competing GameCube and Xbox systems, continued to praise the PlayStation 2's large game library and DVD playback, while routinely criticizing the PlayStation 2's lesser graphics performance compared to the newer systems and its rudimentary online service compared to Xbox Live. 

https://en.wikipedia.org/wiki/PlayStation_2

- **XBox - Microsoft Game console (2001) - Microsoft** 

Xbox is a video gaming brand created and owned by Microsoft. The brand consists of five video game consoles, as well as applications (games), streaming services, an online service by the name of Xbox network, and the development arm by the name of Xbox Game Studios. The brand was first introduced in the United States in November 2001, with the launch of the original Xbox console.
The original device was the first video game console offered by an American company after the Atari Jaguar stopped sales in 1996. It reached over 24 million units sold by May 2006.[1] Microsoft's second console, the Xbox 360, was released in 2005 and has sold 86 million units as of October 2021. 

https://en.wikipedia.org/wiki/Xbox

- **XBox360 -  well-developped online service (Xbox Live) (2005) - Microsoft** 

The Xbox 360 is a home video game console developed by Microsoft. As the successor to the original Xbox, it is the second console in the Xbox series. It competed with Sony's PlayStation 3 and Nintendo's Wii as part of the seventh generation of video game consoles. It was officially unveiled on MTV on May 12, 2005, with detailed launch and game information announced later that month at the 2005 Electronic Entertainment Expo.[17][18][19][20][21]

The Xbox 360 features an online service, Xbox Live, which was expanded from its previous iteration on the original Xbox and received regular updates during the console's lifetime. Available in free and subscription-based varieties, Xbox Live allows users to: play games online; download games (through Xbox Live Arcade) and game demos; purchase and stream music, television programs, and films through the Xbox Music and Xbox Video portals; and access third-party content services through media streaming applications. In addition to online multimedia features, it allows users to stream media from local PCs. Several peripherals have been released, including wireless controllers, expanded hard drive storage, and the Kinect motion sensing camera. The release of these additional services and peripherals helped the Xbox brand grow from gaming-only to encompassing all multimedia, turning it into a hub for living-room computing entertainment.[22][23][24][25][26]
DVD, CD, digital distribution, Add-on: HD DVD

https://en.wikipedia.org/wiki/Xbox_360

- **Sony PlayStation 3 (2006) - Sony**

Sony's next console, the PlayStation 3, was released in 2006, selling over 87.4 million units by March 2017

https://en.wikipedia.org/wiki/PlayStation

The PlayStation 3 (PS3) is a home video game console developed by Sony Computer Entertainment. The successor to the PlayStation 2, it is part of the PlayStation brand of consoles. It was first released on November 11, 2006, in Japan,[9] November 17, 2006, in North America, and March 23, 2007, in Europe and Australia.[10][11][12] The PlayStation 3 competed primarily against Microsoft's Xbox 360 and Nintendo's Wii as part of the seventh generation of video game consoles.

The console was first officially announced at E3 2005, and was released at the end of 2006. It was the first console to use Blu-ray Disk technology as its primary storage medium.[13] The console was the first PlayStation to integrate social gaming services, including the PlayStation Network, as well as the first to be controllable from a handheld console, through its remote connectivity with PlayStation Portable and PlayStation Vita.[14][15][16] In September 2009, the Slim model of the PlayStation 3 was released. It no longer provided the hardware ability to run PS2 games. It was lighter and thinner than the original version, and featured a redesigned logo and marketing design, as well as a minor start-up change in software. A Super Slim variation was then released in late 2012, further refining and redesigning the console.

During its early years, the system received a mixed reception, due to its high price ($599 for a 60-gigabyte model, $499 for a 20 GB model), a complex processor architecture, and lack of quality games but was praised for its Blu-ray capabilities and "untapped potential". The reception would get more positive over time. The system had a slow start in the market[17] but managed to recover, particularly after the introduction of the Slim model. Its successor, the PlayStation 4, was released later in November 2013

https://en.wikipedia.org/wiki/PlayStation_3

- **Nintendo Wii - motion controlled games with wireless controllers, casual gaming (2006) - Nintendo**

The Wii[g] (/wiː/ WEE) is a home video game console developed and marketed by Nintendo. It was released on November 19, 2006, in North America and in December 2006 for most other regions of the world. It is Nintendo's fifth major home game console, following the GameCube and is a seventh generation home console alongside Microsoft's Xbox 360 and Sony's PlayStation 3.

In developing the Wii, Nintendo president Satoru Iwata directed the company to avoid competing with Microsoft and Sony on computational graphics and power and instead to target a broader demographic of players through novel gameplay. Game designers Shigeru Miyamoto and Genyo Takeda led the console's development under the codename Revolution. The primary controller for the Wii is the Wii Remote, a wireless controller with both motion sensing and traditional controls which can be used as a pointing device towards the television screen or for gesture recognition. The Wii was the first Nintendo console to directly support Internet connectivity, supporting both online games and for digital distribution of games and media applications through the Wii Shop Channel. The Wii also supports wireless connectivity with the Nintendo DS handheld console for selected games. Initial Wii models included full backward compatibility support for the GameCube. Later in its lifecycle, two lower-cost Wii models were produced: a revised model with the same design as the original Wii but removed the GameCube compatibility features and the Wii Mini, a compact, budget redesign of the Wii which further removed features including online connectivity and SD card storage.

Because of Nintendo's reduced focus on computational power, the Wii and its games were less expensive to produce than its competitors. The Wii was extremely popular at launch, causing the system to be in short supply in some markets. A bundled game, Wii Sports, was considered the killer app for the console; other flagship games included entries in the Super Mario, Legend of Zelda, Pokémon, and Metroid series. Within a year of launch, the Wii became the best-selling seventh-generation console, and by 2013, had surpassed over 100 million units sold. Total lifetime sales of the Wii had reached over 101 million units, making it Nintendo's best-selling home console until it was surpassed by the Nintendo Switch in 2021. As of 2022, the Wii is the fifth-best-selling home console of all time.

The Wii repositioned Nintendo as a key player in the video game console marketplace. The introduction of motion-controlled games via the Wii Remote led both Microsoft and Sony to develop their own competing products—the Kinect and PlayStation Move, respectively. Nintendo found that, while the Wii had broadened the demographics that they wanted, the core gamer audience had shunned the Wii. The Wii's successor, the Wii U, sought to recapture the core gamer market with additional features atop the Wii. The Wii U was released in 2012, and Nintendo continued to sell both units through the following year. 

https://en.wikipedia.org/wiki/Wii

- **Kinect - controller free (2009) - Microsoft**

Kinect is a "controller-free gaming and entertainment experience" for the Xbox 360. It was first announced on June 1, 2009, at the Electronic Entertainment Expo, under the codename, Project Natal.[102] The add-on peripheral enables users to control and interact with the Xbox 360 without a game controller by using gestures, spoken commands and presented objects and images

https://en.wikipedia.org/wiki/Xbox_360


### Automation


## Some standards, languages and protocols

- **DivX Codec (1998-2007)**

DivX ;-) (not DivX) 3.11 Alpha and later 3.xx versions refers to a hacked version of the Microsoft MPEG-4 Version 3 video codec (not to be confused with MPEG-4 Part 3) from Windows Media Tools 4 codecs.[4][5] The video codec, which was actually not MPEG-4 compliant, was extracted around 1998 by French hacker Jerome Rota (also known as Gej) at Montpellier. The Microsoft codec originally required that the compressed output be put in an ASF file. It was altered to allow other containers such as Audio Video Interleave (AVI).[6] Rota hacked the Microsoft codec because newer versions of the Windows Media Player would not play his video portfolio and résumé that were encoded with it. Instead of re-encoding his portfolio, Rota and German hacker Max Morice decided to reverse engineer the codec, which "took about a week".[7]
In early 2000, Jordan Greenhall recruited Rota to form a company (originally called DivXNetworks, Inc., renamed to DivX, Inc. in 2005) to develop an MPEG-4 codec, from scratch, that would still be backward-compatible with the Microsoft MPEG-4 Version 3 format. This effort resulted first in the release of the "OpenDivX" codec and source code on 15 January 2001. OpenDivX was hosted as an open-source project on the Project Mayo web site hosted at projectmayo.com[8] (the name comes from "mayonnaise", because, according to Rota, DivX and mayonnaise are both "French and very hard to make."[7]). The company's internal developers and some external developers worked jointly on OpenDivX for the next several months, but the project eventually stagnated.
In early 2001, DivX employee "Sparky" wrote a new and improved version of the codec's encoding algorithm known as "encore2". This code was included in the OpenDivX public source repository for a brief time, but then was abruptly removed. The explanation from DivX at the time was that "the community really wants a Winamp, not a Linux." It was at this point that the project forked. That summer, Rota left the French Riviera and moved to San Diego "with nothing but a pack of cigarettes"[9] where he and Greenhall founded what would eventually become DivX, Inc.[7]
On 4 December 2007, native MPEG-4 ASP playback support was added to the Xbox 360,[27] allowing it to play video encoded with DivX and other MPEG-4 ASP codecs.[28]
On 17 December 2007, firmware upgrade 2.10 was released for the Sony PlayStation 3, which included official DivX Certification. Firmware version 2.50 (released on 15 October 2008) included support for the DivX Video on Demand (DivX VOD) service, and firmware version 2.60 (released on 20 January 2009) included official DivX Certification and updated Profile support to version 3.11.[29]

https://en.wikipedia.org/wiki/DivX

- **G.709 Interfaces for the Optical Transport Network (OTN) (2001)**

ITU-T Recommendation G.709 Interfaces for the Optical Transport Network (OTN) describes a means of communicating data over an optical network.[1] It is a standardized method for transparent transport of services over optical wavelengths in DWDM systems. It is also known as Optical Transport Hierarchy (OTH) standard. The first edition of this protocol was approved in 2001.[2]

https://en.wikipedia.org/wiki/G.709

- **RDP - Remote Desktop Protocol (2001)**

Microsoft proprietary protocol to connect to the windows desktop. Released with Windows XP.

https://en.wikipedia.org/wiki/Remote_Desktop_Protocol


- **SVG - Scalable Vector Graphics - supersedes WMF (2001)**

Scalable Vector Graphics (SVG) is an XML-based vector image format for defining two-dimensional graphics, having support for interactivity and animation. The SVG specification is an open standard developed by the World Wide Web Consortium (W3C) since 1999.

https://en.wikipedia.org/wiki/Scalable_Vector_Graphics

- **JSON - JavaScript Object Notation - simplifies data interchange compared to XML (2001)**
 
open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other serializable values). First message sent in 2001.

Widely used. Later standardized.

https://en.wikipedia.org/wiki/JSON

- **Power over Ethernet, or PoE (2003)**

Describes any of several standards or ad hoc systems that pass electric power along with data on twisted-pair Ethernet cabling. This allows a single cable to provide both data connection and electric power to devices such as wireless access points (WAPs), Internet Protocol (IP) cameras, and voice over Internet Protocol (VoIP) phones.

There are several common techniques for transmitting power over Ethernet cabling. Three of them have been standardized by Institute of Electrical and Electronics Engineers (IEEE) standard IEEE 802.3 since 2003.

https://en.wikipedia.org/wiki/Power_over_Ethernet

- **Advanced Video Coding (AVC) - H.264 - MPEG-4 Part 10 (2003)**

Advanced Video Coding (AVC), also referred to as H.264 or MPEG-4 Part 10, is a video compression standard based on block-oriented, motion-compensated coding.[2] It is by far the most commonly used format for the recording, compression, and distribution of video content, used by 91% of video industry developers as of September 2019.[3][4] It supports resolutions up to and including 8K UHD.[5][6]
The intent of the H.264/AVC project was to create a standard capable of providing good video quality at substantially lower bit rates than previous standards (i.e., half or less the bit rate of MPEG-2, H.263, or MPEG-4 Part 2), without increasing the complexity of design so much that it would be impractical or excessively expensive to implement. This was achieved with features such as a reduced-complexity integer discrete cosine transform (integer DCT),[6][7][8][9] variable block-size segmentation, and multi-picture inter-picture prediction. An additional goal was to provide enough flexibility to allow the standard to be applied to a wide variety of applications on a wide variety of networks and systems, including low and high bit rates, low and high resolution video, broadcast, DVD storage, RTP/IP packet networks, and ITU-T multimedia telephony system
H.264 was standardized by the ITU-T Video Coding Experts Group (VCEG) of Study Group 16 together with the ISO/IEC JTC1 Moving Picture Experts Group (MPEG). The project partnership effort is known as the Joint Video Team (JVT). The ITU-T H.264 standard and the ISO/IEC MPEG-4 AVC standard (formally, ISO/IEC 14496-10 – MPEG-4 Part 10, Advanced Video Coding) are jointly maintained so that they have identical technical content. The final drafting work on the first version of the standard was completed in May 2003, and various extensions of its capabilities have been added in subsequent editions. High Efficiency Video Coding (HEVC), a.k.a. H.265 and MPEG-H Part 2 is a successor to H.264/MPEG-4 AVC developed by the same organizations, while earlier standards are still in common use.
H.264 is perhaps best known as being the most commonly used video encoding format on Blu-ray Discs. It is also widely used by streaming Internet sources, such as videos from Netflix, Hulu, Amazon Prime Video, Vimeo, YouTube, and the iTunes Store, Web software such as the Adobe Flash Player and Microsoft Silverlight, and also various HDTV broadcasts over terrestrial (ATSC, ISDB-T, DVB-T or DVB-T2), cable (DVB-C), and satellite (DVB-S and DVB-S2) systems.

https://en.wikipedia.org/wiki/Advanced_Video_Coding

- **ISO image (2000s)**

An optical disc image (or ISO image, from the ISO 9660 file system used with CD-ROM media) is a disk image that contains everything that would be written to an optical disc, disk sector by disc sector, including the optical disc file system.[2] ISO images are expected to contain the binary image of an optical media file system (usually ISO 9660 and its extensions or UDF), including the data in its files in binary format, copied exactly as they were stored on the disc. The data inside the ISO image will be structured according to the file system that was used on the optical disc from which it was created.

ISO images can be created from optical discs by disk imaging software, or from a collection of files by optical disc authoring software, or from a different disk image file by means of conversion. Software distributed on bootable discs is often available for download in ISO image format. And like any other ISO image, it may be written to an optical disc such as CD, DVD and Blu-Ray.

https://en.wikipedia.org/wiki/Optical_disc_image

- **ISOBMFF - ISO base media file format (2004)**

The ISO base media file format (ISOBMFF) is a container file format that defines a general structure for files that contain time-based multimedia data such as video and audio.[2][3] It is standardized in ISO/IEC 14496-12, a.k.a. MPEG-4 Part 12, and was formerly also published as ISO/IEC 15444-12, a.k.a. JPEG 2000 Part 12.

It is designed as a flexible, extensible format that facilitates interchange, management, editing and presentation of the media. The presentation may be local, or via a network or other stream delivery mechanism. The file format is designed to be independent of any particular network protocol while enabling support for them in general.[3]

The format has become very widely used for media file storage and as the basis for various other media file formats (e.g. the MP4 and 3GP container formats), and its widespread was recognized by a Technology & Engineering Emmy Award presented on 4 November 2021 by the National Academy of Television Arts and Sciences.[4][5][6]

https://en.wikipedia.org/wiki/ISO_base_media_file_format

- **SSH protocol architecture (2006)**

Includes SCP (superseded by SFTP).

https://www.rfc-editor.org/rfc/rfc4251

- **G.651.1 - multi-mode optical fiber (MMF) cable (2007)**

an international standard developed by the Standardization Sector of the International Telecommunication Union (ITU-T) that specifies multi-mode optical fiber (MMF) cable.

https://en.wikipedia.org/wiki/G.651.1

- **DVB-T2 - Digital Video Broadcasting — Second Generation Terrestrial (2007)**

DVB-T2 is an abbreviation for "Digital Video Broadcasting — Second Generation Terrestrial"; it is the extension of the television standard DVB-T, issued by the consortium DVB, devised for the broadcast transmission of digital terrestrial television. DVB has been standardized by ETSI.
This system transmits compressed digital audio, video, and other data in "physical layer pipes" (PLPs), using OFDM modulation with concatenated channel coding and interleaving. The higher offered bit rate, with respect to its predecessor DVB-T, makes it a system suited for carrying HDTV signals on the terrestrial TV channel (though many broadcasters still use plain DVB-T for this purpose). As of 2019, it was implemented in broadcasts in the United Kingdom (Freeview HD, eight channels across two multiplexes, plus an extra multiplex in Northern Ireland carrying three SD channels), Italy (Europa 7 HD, twelve channels), Finland (21 channels, five in HD), Germany (six HD (1080p50) channels, with 40 in planning),[1] the Netherlands (Digitenne, 30 HD (1080p50) channels), Sweden (five channels),[2][3] Thailand (41 SD, 9 HD channels),[4] Flanders (18 SD channels), Serbia (eight channels),[5] Ukraine (32 SD and HD channels in four nationwide multiplexes), Croatia (all national, local and pay-TV channels), Denmark (two pay-TV multiplexes with 20 channels), Romania (8 SD channels, 1 HD channel), and some other countries.
In March 2006 DVB decided to study options for an upgraded DVB-T standard. In June 2006, a formal study group named TM-T2 (Technical Module on Next Generation DVB-T) was established by the DVB Group to develop an advanced modulation scheme that could be adopted by a second generation digital terrestrial television standard, to be named DVB-T2.[6]
According to the commercial requirements and call for technologies[7] issued in April 2007, the first phase of DVB-T2 would be devoted to provide optimum reception for stationary (fixed) and portable receivers (i.e., units which can be nomadic, but not fully mobile) using existing aerials, whereas a second and third phase would study methods to deliver higher payloads (with new aerials) and the mobile reception issue. The novel system should provide a minimum 30% increase in payload, under similar channel conditions already used for DVB-T.
The BBC, ITV, Channel 4 and Channel 5 agreed with the regulator Ofcom to convert one UK multiplex (B, or PSB3) to DVB-T2 to increase capacity for HDTV via DTT.[8] They expected the first TV region to use the new standard would be Granada in November 2009 (with existing switched over regions being changed at the same time). It was expected that over time there would be enough DVB-T2 receivers sold to switch all DTT transmissions to DVB-T2, and H.264.
Ofcom published its final decision on 3 April 2008, for HDTV using DVB-T2 and H.264:[9] BBC HD would have one HD slot after digital switchover (DSO) at Granada. ITV and C4 had, as expected, applied to Ofcom for the 2 additional HD slots available from 2009 to 2012.[10]
Ofcom indicated that it found an unused channel covering 3.7 million households in London, which could be used to broadcast the DVB-T2 HD multiplex from 2010, i.e., before DSO in London. Ofcom indicated that they would look for more unused UHF channels in other parts of the UK, that can be used for the DVB-T2 HD multiplex from 2010 until DSO.[11]

https://en.wikipedia.org/wiki/DVB-T2

- **IEEE 1675-2008 (2008), IEEE 1901 (2011) - broadband over power lines** 

was a standard for broadband over power lines developed by the IEEE Standards Association. It provided electric utility companies with a comprehensive standard for safely installing hardware required for Internet access capabilities over their power lines.
The standard was published 7 January 2008. The IEEE 1901 standard was another related attempt published in 2011.

https://en.wikipedia.org/wiki/IEEE_1675-2008

- **G.hn - 2 Gbit/s over legacy wires (2009)**

Specification for home networking with data rates up to 2 Gbit/s and operation over four types of legacy wires: telephone wiring, coaxial cables, power lines and plastic optical fiber. 
G.hn was developed under the International Telecommunication Union's Telecommunication Standardization sector (the ITU-T) and promoted by the HomeGrid Forum and several other organizations. ITU-T Recommendation (the ITU's term for standard) G.9960, which received approval on October 9, 2009,[2] specified the physical layers and the architecture of G.hn. The Data Link Layer (Recommendation G.9961) was approved on June 11, 2010.[3]

https://en.wikipedia.org/wiki/G.hn



