# Some tech evolutions in the 1950s 

**In short:**
- [Inventions](#inventions) : 
  * early trackball
  * first workable silicon transistor
  * initial research on inkjet printers
  * first LED application ideas
  * first engineering proposal for a flat-panel TV
  * MOSFET
- [Electronics](#electronics) : 
  * PCB commercial use
  * magnetic memory deployed on Eniac
  * first integrated circuit
  * planar process
- [Energy](#energy) : 
- [Telecommunications](#telecommunications) : 
  * SPC telephone exchange invention
  * mass production of telephone lines
  * first satelitte (Sputnik 1)
- [Networking](#networking) : 
  * first commercial modem (Bell 101)
- [Cryptography](#cryptography) : 
  * Government control
- [Computers](#computers) :
  * [Form factor](#form-factor) : 
    - first digital computer for educational purposes
    - first generation mainframes : IBM 700 series, PDP-1
  * [OS](#os) : 
    - Produced by customers : GM-NAA I/O
  * [Peripherals](#peripherals) :
    - IBM 7-track dual magnetic tape reader/recorder
    - D-sub series connectors
    - first UART : PDP-1
  * [Storage](#storage) : 
    - first use of magnetic tape : UNIVAC I
    - first hard drive 
  * [Uses](#uses) : 
    - Translation of Russian to English
    - blackjack calculator
    - artificial intelligence
    - calculation of satellite orbits
    - consumer credit card program
    - hacker culture
    - video games and computer music : PDP-1
- [Consumer Electronics](#consumer-electronics) :
  * [Multimedia](#multimedia) : 
    - clock radio
    - TV remote controls
    - first transistor radio
    - first all transistor phonograph
  * [Screens](#screens) : 
    - Shift from circular CRT to rectangular CRT
    - First color CRT
  * [Broadcast](#broadcast) : 
    - Analog TV systems
    - Cable television
  * [Video games](#video-games) : 
    - Bertie the Brain in 1950 (tic tac toe)
    - Nimrod in 1951 (Nim) on Mainframe computers
- [Standards and protocols](#standards-and-protocols) : 
   * Color TV - NTSC standard
- [Programming languages and frameworks](#programming-languages-and-frameworks) : 
  * Kompiler : language compilers
  * Fortran : numeric/scientific computing
  * Lisp : practical mathematical notation/AI
  * ALGOL : algorithm description/blocks
  * COBOL : business use / procedural 
- [Navigation](#navigation) : 
  * pre-GPS systems

## Inventions
**[`^        back to top        ^`](#)**

- **Early trackball (1952)**

Another early trackball was built by Kenyon Taylor, a British electrical engineer working in collaboration with Tom Cranston and Fred Longstaff. Taylor was part of the original Ferranti Canada, working on the Royal Canadian Navy's DATAR (Digital Automated Tracking and Resolving) system in 1952.[10]

https://en.wikipedia.org/wiki/Computer_mouse

-**First workable silicon transistor (1954) - Bell Telephone Laboratories**

In January 1954, Morris Tanenbaum at Bell Telephone Laboratories created the first workable silicon transistor.[23] This work was reported in the spring of 1954, at the IRE off-the-record conference on solid-state devices, and was later published in the Journal of Applied Physics. Working independently in April 1954, Gordon Teal at TI created the first commercial silicon transistor and tested it on April 14, 1954. On May 10, 1954, at the Institute of Radio Engineers National Conference on Airborne Electronics in Dayton, Ohio, Teal presented a paper: "Some Recent Developments in Silicon and Germanium Materials and Devices".[25]

https://en.wikipedia.org/wiki/Texas_Instruments

- **Initial research on inkjet printers - Canon, HP**

The concept of inkjet printing originated in the 20th century, and the technology was first extensively developed in the early 1950s. While working at Canon in Japan, Ichiro Endo suggested the idea for a "Bubble jet" printer, while around the same time Jon Vaught at HP was developing a similar idea.

https://en.wikipedia.org/wiki/Inkjet_printing

- **first LED applications ideas (1950s)**

Kurt Lehovec, Carl Accardo, and Edward Jamgochian explained these first LEDs in 1951 using an apparatus employing SiC crystals with a current source of a battery or a pulse generator and with a comparison to a variant, pure, crystal in 1953.[17][18]

Rubin Braunstein[19] of the Radio Corporation of America reported on **infrared emission** from gallium arsenide (GaAs) and other semiconductor alloys in 1955.[20] Braunstein observed infrared emission generated by simple diode structures using gallium antimonide (GaSb), GaAs, indium phosphide (InP), and silicon-germanium (SiGe) alloys at room temperature and at 77 kelvins.

In 1957, Braunstein further demonstrated that the rudimentary devices could be used for **non-radio communication across a short distance**. As noted by Kroemer[21] Braunstein "…had set up a simple optical communications link: Music emerging from a record player was used via suitable electronics to modulate the forward current of a GaAs diode. The emitted light was detected by a PbS diode some distance away. This signal was fed into an audio amplifier and played back by a loudspeaker. Intercepting the beam stopped the music. We had a great deal of fun playing with this setup." This setup presaged the use of LEDs for **optical communication applications**.

https://en.wikipedia.org/wiki/Light-emitting_diode

- **first engineering proposal for a flat-panel TV (1954) - General Electric and first flat-panel display - Aiken (1958)**

The first engineering proposal for a flat-panel TV was by General Electric in 1954 as a result of its work on radar monitors. The publication of their findings gave all the basics of future flat-panel TVs and monitors. But GE did not continue with the R&D required and never built a working flat panel at that time.[1] The first production flat-panel display was the Aiken tube, developed in the early 1950s and produced in limited numbers in 1958. This saw some use in military systems as a heads up display and as an oscilloscope monitor, but conventional technologies overtook its development. Attempts to commercialize the system for home television use ran into continued problems and the system was never released commercially.[2][3][4]

The Philco Predicta featured a relatively flat (for its day) cathode ray tube setup and would be the first commercially released "flat panel" upon its launch in 1958; the Predicta was a commercial failure. 

https://en.wikipedia.org/wiki/Flat-panel_display

- **MOSFET (1959) - Bell Labs** 

In the 1950s, Mohamed Atalla investigated the surface properties of silicon semiconductors at Bell Labs, where he proposed a new method of semiconductor device fabrication, coating a silicon wafer with an insulating layer of silicon oxide so that electricity could reliably penetrate to the conducting silicon below, overcoming the surface states that prevented electricity from reaching the semiconducting layer. This is known as surface passivation, a method that became critical to the semiconductor industry as it made possible the mass-production of silicon integrated circuits (ICs). Building on his surface passivation method, he developed the metal oxide semiconductor (MOS) process, which he proposed could be used to build the first working silicon field-effect transistor (FET).[9][10] The led to the invention of the MOSFET (MOS field-effect transistor) by Mohamed Atalla and Dawon Kahng in 1959.[11][12] With its scalability,[13] and much lower power consumption and higher density than bipolar junction transistors,[14] the MOSFET became the most common type of transistor in computers, electronics,[10] and communications technology such as smartphones.[15] The US Patent and Trademark Office calls the MOSFET a "groundbreaking invention that transformed life and culture around the world".[15]

https://en.wikipedia.org/wiki/Semiconductor_device

## Electronics
**[`^        back to top        ^`](#)**

- **Printed Circuit Board (PCB) in commercial use (1948-1952) - Motorola**

**After the war, in 1948, the USA released the invention for commercial use**. Printed circuits did not become commonplace in consumer electronics until the mid-1950s, after the Auto-Sembly process was developed by the United States Army. At around the same time in the UK work along similar lines was carried out by Geoffrey Dummer, then at the RRDE.

**Motorola was an early leader in bringing the process into consumer electronics, announcing in August 1952 the adoption of "plated circuits" in home radios after six years of research and a $1M investment**.[54] Motorola soon began using its trademarked term for the process, PLAcir, in its consumer radio advertisements.[55] Hallicrafters released its first "foto-etch" printed circuit product, a clock-radio, on 1 November 1952.[56]

https://en.wikipedia.org/wiki/Printed_circuit_board

- **Magnetic memory deployed on Eniac (1953)**

In 1953, a 100-word magnetic-core memory built by the Burroughs Corporation was added to ENIAC.

https://en.wikipedia.org/wiki/ENIAC

-**First integrated circuit - Texas Instrument, 1958 - Fairchild Semiconductor, 1960**

Jack Kilby, an employee at TI, invented the integrated circuit in 1958.[26] Kilby recorded his initial ideas concerning the integrated circuit in July 1958, and successfully demonstrated the world's first working integrated circuit on September 12, 1958.[27] Six months later, Robert Noyce of Fairchild Semiconductor (who went on to co-found Intel) independently developed the integrated circuit with integrated interconnect, and is also considered an inventor of the integrated circuit.[28

https://en.wikipedia.org/wiki/Texas_Instruments

Fairchild's Noyce and Texas Instrument's Kilby had independently invented the integrated circuit (IC) based on bipolar technology. 

In 1960, Fairchild built a circuit with four transistors on a single wafer of silicon, thereby creating the first silicon integrated circuit (Texas Instruments' Jack Kilby had developed an integrated circuit made of germanium on September 12, 1958, and was awarded a U.S. patent, however Kilby's method was not scalable and the semiconductor industry adopted Fairchild's process to manufacture integrated circuits). 

https://en.wikipedia.org/wiki/Fairchild_Semiconductor

- **Planar process (1959-1960) - Fairchild Semiconductor**

The planar process is a manufacturing process used in the semiconductor industry to build individual components of a transistor, and in turn, connect those transistors together. It is the primary process by which silicon integrated circuit chips are built. The process utilizes the surface passivation and thermal oxidation methods.

The planar process was developed at Fairchild Semiconductor in 1959.

At a 1958 Electrochemical Society meeting, Mohamed Atalla presented a paper about the surface passivation of PN junctions by thermal oxidation, based on his 1957 BTL memos.[1]

Swiss engineer Jean Hoerni attended the same 1958 meeting, and was intrigued by Atalla's presentation. Hoerni came up with the "planar idea" one morning while thinking about Atalla's device.[1] Taking advantage of silicon dioxide's passivating effect on the silicon surface, Hoerni proposed to make transistors that were protected by a layer of silicon dioxide.[1] This led to the first successful product implementation of the Atalla silicon transistor passivation technique by thermal oxide.[2]

The planar process was developed by Jean Hoerni, one of the "traitorous eight", while working at Fairchild Semiconductor, with a first patent issued 1959.[3][4]

Together with the use of metallization (to join together the integrated circuits), and the concept of p–n junction isolation (from Kurt Lehovec), the researchers at Fairchild were able to create circuits on a single silicon crystal slice (a wafer) from a monocrystalline silicon boule.

In 1959, Robert Noyce built on Hoerni's work with his conception of an integrated circuit (IC), which added a layer of metal to the top of Hoerni's basic structure to connect different components, such as transistors, capacitors, or resistors, located on the same piece of silicon. The planar process provided a powerful way of implementing an integrated circuit that was superior to earlier conceptions of the integrated circuit.[5] Noyce's invention was the first monolithic IC chip.[6][7]

Early versions of the planar process used a photolithography process using near-ultraviolet light from a mercury vapor lamp. As of 2011, small features are typically made with 193 nm "deep" UV lithography.[8] Some researchers use even higher-energy extreme ultraviolet lithography.

https://en.wikipedia.org/wiki/Planar_process

In 1960, Noyce invented the planar integrated circuit. The industry preferred Fairchild's invention over Texas Instruments' because the transistors in planar ICs were interconnected by a thin film deposit, whereas Texas Instruments' invention required fine wires to connect the individual circuits. Noyce's invention was enabled by the planar process developed by Jean Hoerni.[10] In turn, Hoerni's planar process was inspired by the surface passivation method developed by Mohamed Atalla at Bell Labs in 1957.[11]

## Energy
**[`^        back to top        ^`](#)**
## Telecommunications
**[`^        back to top        ^`](#)**

- **SPC - Stored program control (1954) - Bell Labs**

Stored program control (SPC) is a telecommunications technology used for telephone exchanges controlled by a computer program stored in the memory of the switching system. SPC was the enabling technology of electronic switching systems (ESS) developed in the Bell System in the 1950s, and may be considered the third generation of switching technology. Stored program control was invented in 1954 by Bell Labs scientist Erna Schneider Hoover, who reasoned that computer software could control the connection of telephone calls.[1][2][3]

https://en.wikipedia.org/wiki/Stored_program_control

- **Mass production of telephone lines - SAGE (1958)**
 
Mass production of telephone line modems in the United States began as part of the SAGE air-defense system in 1958, connecting terminals at various airbases, radar sites, and command-and-control centers to the SAGE director centers scattered around the United States and Canada.

https://en.wikipedia.org/wiki/Modem

The Semi-Automatic Ground Environment (SAGE) was a system of large computers and associated networking equipment that coordinated data from many radar sites and processed it to produce a single unified image of the airspace over a wide area. SAGE directed and controlled the NORAD response to a possible Soviet air attack, operating in this role from the late 1950s into the 1980s. Its enormous computers and huge displays remain a part of cold war lore, and after decommissioning were common props in movies such as Dr. Strangelove and Colossus, and on science fiction TV series such as The Time Tunnel.

The processing power behind SAGE was supplied by the largest discrete component-based computer ever built, the IBM-manufactured AN/FSQ-7. Each SAGE Direction Center (DC) housed an FSQ-7 which occupied an entire floor, approximately 22,000 square feet (2,000 m2) not including supporting equipment. The FSQ-7 was actually two computers, "A" side and "B" side. Computer processing was switched from "A" side to "B" side on a regular basis, allowing maintenance on the unused side. Information was fed to the DCs from a network of radar stations as well as readiness information from various defense sites. The computers, based on the raw radar data, developed "tracks" for the reported targets, and automatically calculated which defenses were within range. Operators used light guns to select targets on-screen for further information, select one of the available defenses, and issue commands to attack. These commands would then be automatically sent to the defense site via teleprinter.

Connecting the various sites was an enormous network of telephones, modems and teleprinters. Later additions to the system allowed SAGE's tracking data to be sent directly to CIM-10 Bomarc missiles and some of the US Air Force's interceptor aircraft in-flight, directly updating their autopilots to maintain an intercept course without operator intervention. Each DC also forwarded data to a Combat Center (CC) for "supervision of the several sectors within the division"[5] ("each combat center [had] the capability to coordinate defense for the whole nation").[6]: 51 

SAGE became operational in the late 1950s and early 1960s at a combined cost of billions of dollars. It was noted that the deployment cost more than the Manhattan Project—which it was, in a way, defending against. Throughout its development, there were continual concerns about its real ability to deal with large attacks, and the Operation Sky Shield tests showed that only about one-fourth of enemy bombers would have been intercepted.[7] Nevertheless, SAGE was the backbone of NORAD's air defense system into the 1980s, by which time the tube-based FSQ-7s were increasingly costly to maintain and completely outdated. Today the same command and control task is carried out by microcomputers, based on the same basic underlying data.

https://en.wikipedia.org/wiki/Semi-Automatic_Ground_Environment

- **Sputnik 1 - First artificial earth satellite - Soviet Union**
Sputnik 1 (/ˈspʌtnɪk, ˈspʊtnɪk/; see § Etymology) was the first artificial Earth satellite.[5] It was launched into an elliptical low Earth orbit by the Soviet Union on 4 October 1957 as part of the Soviet space program. It sent a radio signal back to Earth for three weeks before its three silver-zinc batteries ran out, and continued in orbit for two months until atmospheric drag caused it to fall back into the atmosphere on the 4th of January 1958.

It was a polished metal sphere 58 cm (23 in) in diameter with four external radio antennas to broadcast radio pulses. Its radio signal was easily detectable by amateur radio operators,[6] and the 65° orbital inclination made its flight path cover virtually the entire inhabited Earth.

The satellite's unanticipated success precipitated the American Sputnik crisis and triggered the Space Race, part of the Cold War. The launch was the beginning of a new era of political, military, technological and scientific developments.[7][8] The word sputnik is Russian for satellite when interpreted in an astronomical context;[9] its other meanings are spouse or traveling companion.[10][11]

https://en.wikipedia.org/wiki/Sputnik_1

## Networking
**[`^        back to top        ^`](#)**

- **BELL 101 - First commercial Modem (1959) - Bell Labs**

Shortly afterwards in 1959, the technology in the SAGE modems was made available commercially as the Bell 101, which provided 110 bit/s speeds. Bell called this and several other early modems "datasets."

https://en.wikipedia.org/wiki/Modem

Prior to 1960, 110 bps modems were used for teletype machines (like an electric typewriter only much more noisy). What one typed at a teletype (or had saved on punched paper tape) could be printed on a remote teletype located far away. No computer was involved.

https://tldp.org/HOWTO/Modem-HOWTO-29.html

## Cryptography
**[`^        back to top        ^`](#)**

- **Government control**

Until the 1960s, secure cryptography was largely the preserve of governments. Two events have since brought it squarely into the public domain: the creation of a public encryption standard (DES), and the invention of public-key cryptography.

https://en.wikipedia.org/wiki/History_of_cryptography

Both events occured in the 1970s.

## Computers
**[`^        back to top        ^`](#)**

### Form factor
**[`^        back to top        ^`](#)**
- **Simon (1950) - Edmund Berkeley - First digital computer for educational purposes**

Simon was a project developed by Edmund Berkeley and presented in a thirteen articles series issued in Radio-Electronics magazine, from October 1950. Although there were far more advanced machines at the time of its construction, the Simon represented the **first experience of building an automatic simple digital computer, for educational purposes**. In fact, its ALU had only 2 bits, and the total memory was 12 bits (2bits x6). In 1950, it was sold for US$600.
https://en.wikipedia.org/wiki/History_of_personal_computers

**First generation mainframes : The IBM 700 series**

https://books.google.fr/books?id=B9l9DwAAQBAJ&pg=PA69&redir_esc=y#v=onepage&q&f=false

In the late 1950s, mainframes had only a rudimentary interactive interface (the **console**) and used sets of **punched cards**, **paper tape**, or **magnetic tape** to transfer data and programs. They operated in batch mode to support back office functions such as payroll and customer billing, most of which were based on repeated tape-based sorting and merging operations followed by line printing to preprinted continuous stationery.

https://en.wikipedia.org/wiki/Mainframe_computer

Several manufacturers and their successors produced mainframe computers from the 1950s until the early 21st century, with gradually decreasing numbers and a gradual transition to simulation on Intel chips rather than proprietary hardware. The US group of manufacturers was first known as "IBM and the Seven Dwarfs":[17]: p.83  usually Burroughs, UNIVAC, NCR, Control Data, Honeywell, General Electric and RCA, although some lists varied. Later, with the departure of General Electric and RCA, it was referred to as IBM and the BUNCH. IBM's dominance grew out of their 700/7000 series and, later, the development of the 360 series mainframes. The latter architecture has continued to evolve into their current zSeries mainframes which, along with the then Burroughs and Sperry (now Unisys) MCP-based and OS1100 mainframes, are among the few mainframe architectures still extant that can trace their roots to this early period. While IBM's zSeries can still run 24-bit System/360 code, the 64-bit zSeries and System z9 CMOS servers have nothing physically in common with the older systems. Notable manufacturers outside the US were Siemens and Telefunken in Germany, ICL in the United Kingdom, Olivetti in Italy, and Fujitsu, Hitachi, Oki, and NEC in Japan. The Soviet Union and Warsaw Pact countries manufactured close copies of IBM mainframes during the Cold War;[18] the BESM series and Strela are examples of an independently designed Soviet computer.

https://en.wikipedia.org/wiki/Mainframe_computer

- **IBM 701 - First generation mainframe (1951) - IBM**

The IBM 701 Electronic Data Processing Machine, known as the Defense Calculator while in development, was IBM’s first commercial scientific computer and its first series production mainframe computer, which was announced to the public on May 21, 1952.[1] It was invented and developed by Jerrier Haddad and Nathaniel Rochester based on the IAS machine at Princeton.[2][3][4]

The IBM 701 was the first computer in the IBM 700/7000 series, which was responsible for bringing electronic computing to the world and for IBM's dominance in the mainframe computer market during the 1960s and 1970s that continues today.[5] The series were IBM’s high-end computers until the arrival of the IBM System/360 in 1964.[5]

https://en.wikipedia.org/wiki/IBM_701

- **IBM 704 - first mass-produced computer with hardware for floating-point arithmetic (1954) - IBM**

The IBM 704 is a large digital mainframe computer introduced by IBM in 1954. It was the first mass-produced computer with hardware for floating-point arithmetic.[1] The IBM 704 Manual of operation states:[2]

The type 704 Electronic Data-Processing Machine is a large-scale, high-speed electronic calculator controlled by an internally stored program of the single address type.

The 704 at that time was thus regarded as "pretty much the only computer that could handle complex math".[3] The 704 was a significant improvement over the earlier IBM 701 in terms of architecture and implementation

The programming languages FORTRAN[5] and LISP[6] were first developed for the 704, as was the SAP assembler—Symbolic Assembly Program, later distributed by SHARE as SHARE Assembly Program.

MUSIC, the first computer music program, was developed on the IBM 704 by Max Mathews.

https://en.wikipedia.org/wiki/IBM_704

- **PDP-1 (1959) - DEC - hacker culture, video games (Spacewar!) and computer music**

The PDP-1 (Programmed Data Processor-1) is the first computer in Digital Equipment Corporation's PDP series and was first produced in 1959. It is famous for being the computer most important in the creation of hacker culture at Massachusetts Institute of Technology, BBN and elsewhere.[2] The PDP-1 is the original hardware for playing history's first game on a minicomputer, Steve Russell's Spacewar!
The PDP-1 uses punched paper tape as its primary storage medium.[15] Unlike punched card decks, which could be sorted and re-ordered, paper tape is difficult to physically edit. This inspired the creation of text-editing programs such as Expensive Typewriter and TECO. Because it is equipped with online and offline printers that were based on IBM electric typewriter mechanisms, it is capable of what, in 1980s terminology, would be called "letter-quality printing" and therefore inspired TJ-2, arguably the first word processor.

https://en.wikipedia.org/wiki/PDP-1

MIT hackers also used the PDP-1 for playing music in four-part harmony, using some special hardware – four flip-flops directly controlled by the processor (the audio signal is filtered with simple RC filters). Music was prepared via Peter Samson's Harmony Compiler, a sophisticated text-based program with some features specifically oriented toward the efficient coding of baroque music. Several hours of music were prepared for it, including Bach fugues, all of Mozart's Eine kleine Nachtmusik, the Ode to Joy movement concluding Beethoven's Symphony No. 9, Christmas carols, and numerous popular songs.

https://en.wikipedia.org/wiki/PDP-1

### OS
**[`^        back to top        ^`](#)**
- **GM-NAA I/O (1956) - General Motors**

The first operating system used for real work was GM-NAA I/O, produced in 1956 by General Motors' Research division for its IBM 704. Most other early operating systems for IBM mainframes were also produced by customers

https://en.wikipedia.org/wiki/History_of_operating_systems

### Peripherals
**[`^        back to top        ^`](#)**
- **IBM 726 - IBM 7-track dual magnetic tape reader/recorder (1952) - IBM**

The IBM 726 dual magnetic tape reader/recorder for the IBM 701 was announced on May 21, 1952.

IBM's first magnetic-tape data storage devices, introduced in 1952, use what is now generally known as 7-track tape. The magnetic tape is 1/2" wide, and there are six data tracks plus one parity track for a total of seven parallel tracks that span the length of the tape. Data is stored as six-bit characters, with each bit of the character and the additional parity bit stored in a different track.

These tape drives were mechanically sophisticated floor-standing drives that used vacuum columns to buffer long U-shaped loops of tape. Between active control of powerful reel motors and vacuum control of these U-shaped tape loops, extremely rapid start and stop of the tape at the tape-to-head interface could be achieved. When active, the two tape reels thus fed tape into or pulled tape out of the vacuum columns, intermittently spinning in rapid, unsynchronized bursts resulting in visually striking action. Stock shots of such vacuum-column tape drives in motion were widely used to represent "the computer" in films and television.

https://en.wikipedia.org/wiki/IBM_7-track#IBM_726

- **D-sub series connectors (1952) - Cannon**

The D-sub series of connectors was introduced by Cannon in 1952.

https://en.wikipedia.org/wiki/D-subminiature

- **PDP-1 (1959) - DEC - first UART**

Gordon Bell of **DEC** designed the **first UART**, occupying an entire circuit board called a line unit, for the PDP series of computers beginning with the PDP-1.[4][5] According to Bell, the main innovation of the UART was its use of sampling to convert the signal into the digital domain, allowing more reliable timing than previous circuits that used analog timing devices with manually adjusted potentiometers.[6] To reduce the cost of wiring, backplane and other components, these computers also pioneered flow control using XON and XOFF characters rather than hardware wires.

https://en.wikipedia.org/wiki/Universal_asynchronous_receiver-transmitter

### Storage
**[`^        back to top        ^`](#)**
- **Eckert-Mauchly UNIVAC I - First use of magnetic tape (1951) - Eckert-Mauchly**

Magnetic tape was first used to record computer data in 1951 on the Eckert-Mauchly UNIVAC I. The system's UNISERVO I tape drive used a thin strip of one half-inch (12.65 mm) wide metal, consisting of nickel-plated bronze (called Vicalloy). Recording density was 100 characters per inch (39.37 characters/cm) on eight tracks.[6]

https://en.wikipedia.org/wiki/Magnetic_tape

- **Introduction of hard drive (1956) - IBM**

Introduced by IBM in 1956,[6] HDDs were the dominant secondary storage device for general-purpose computers beginning in the early 1960s

The first production IBM hard disk drive, the 350 disk storage, shipped in 1957 as a component of the IBM 305 RAMAC system. It was approximately the size of two medium-sized refrigerators and stored five million six-bit characters (3.75 megabytes)[18] on a stack of 52 disks (100 surfaces used).[35] The 350 had a single arm with two read/write heads, one facing up and the other down, that moved both horizontally between a pair of adjacent platters and vertically from one pair of platters to a second set.[36][37][38] Variants of the IBM 350 were the IBM 355, IBM 7300 and IBM 1405.

https://en.wikipedia.org/wiki/Hard_disk_drive

### Uses
**[`^        back to top        ^`](#)**
- **Translation of Russian to English (1952 - 1954) - IBM and Georgetown University**

In 1952 IBM paired with language scholars from Georgetown University to develop a translation software for use on computers. On January 7, 1954, the team developed an experimental software program that allowed the IBM 701 computer to translate from Russian to English. The Mark 1 Translating Device, which was developed for the US Air Force, was able to produce its first automated Russian-to-English translation in 1959 and was shown to the public in 1964.[11]

https://en.wikipedia.org/wiki/IBM_701

- **Blackjack calculator (1954)**

In 1954, a group of scientists ran millions of simulated hands of blackjack on an IBM 701 looking to determine the best playing decision for every combination of cards. The result of the study was the set of correct rules for hitting, standing, doubling or splitting in a blackjack game which are still the same today.[11]

https://en.wikipedia.org/wiki/IBM_701

- **Artificial intelligence (1956) - IBM**

In **1956**, IBM demonstrated the **first practical example of artificial intelligence** when Arthur L. Samuel of IBM's Poughkeepsie, New York, laboratory programmed an IBM 704 not merely to play checkers but "learn" from its own experience. 

https://en.wikipedia.org/wiki/IBM

The IBM 701 can claim to be the first computer displaying the potential of artificial intelligence in Arthur Samuel's Checkers-playing Program on February 24, 1956. 

https://en.wikipedia.org/wiki/IBM_701

- **Calculation of satellite orbits (1957) - IBM, Smithsonian**

The IBM 704 at the MIT Computation Center was used as the official tracker for the Smithsonian Astrophysical Observatory Operation Moonwatch in the fall of 1957. IBM provided four staff scientists to aid Smithsonian Astrophysical Observatory scientists and mathematicians in the calculation of satellite orbits: Dr. Giampiero Rossoni, Dr. John Greenstadt, Thomas Apple and Richard Hatch.

The Los Alamos Scientific Laboratory (LASL) developed an early monitor named SLAM to enable batch processing.[11]

https://en.wikipedia.org/wiki/IBM_704

- **Consumer credit card program (1958) - BankofAmerica / Visa**:  Bank of America launches the first consumer credit card program in the U.S. Later becomes **VISA**.

https://en.wikipedia.org/wiki/Visa_Inc.

- **PDP-1 (1959) - DEC - hacker culture, video games (Spacewar!) and computer music**

See [Computers > Form factor](#form-factor)


## Consumer Electronics
**[`^        back to top        ^`](#)**

### Multimedia
**[`^        back to top        ^`](#)**
- **first TV remote controls**

The first remote intended to control a television was developed by Zenith Radio Corporation in 1950. The remote, called "Lazy Bones,"[13] was connected to the television by a wire. A wireless remote control, the "Flashmatic,"[13][14] was developed in 1955 by Eugene Polley. It worked by shining a beam of light onto one of four photoelectric cells,[15] but the cell did not distinguish between light from the remote and light from other sources.[16] The Flashmatic also had to be pointed very precisely at one of the sensors in order to work.[16][17]

In 1956, Robert Adler developed[18] "Zenith Space Command,"[13] a wireless remote.[19] It was mechanical and used ultrasound to change the channel and volume.[20] When the user pushed a button on the remote control, it struck a bar and clicked, hence they were commonly called a "clicker," but it sounded like a "clink" and the mechanics were similar to a pluck.[21] Each of the four bars emitted a different fundamental frequency with ultrasonic harmonics, and circuits in the television detected these sounds and interpreted them as channel-up, channel-down, sound-on/off, and power-on/off.[22]

Later, the rapid decrease in price of transistors made possible cheaper electronic remotes that contained a piezoelectric crystal that was fed by an oscillating electric current at a frequency near or above the upper threshold of human hearing, though still audible to dogs. The receiver contained a microphone attached to a circuit that was tuned to the same frequency. Some problems with this method were that the receiver could be triggered accidentally by naturally occurring noises or deliberately by metal against glass, for example, and some people could hear the lower ultrasonic harmonics.

https://en.wikipedia.org/wiki/Remote_control#Infrared,_line_of_sight_and_operating_angle

- **clock-radio - first consumer electronice device with PCB - Hallicrafters (1952)**

Motorola was an early leader in bringing the process into consumer electronics, announcing in August 1952 the adoption of "plated circuits" in home radios after six years of research and a $1M investment.[54] Motorola soon began using its trademarked term for the process, PLAcir, in its consumer radio advertisements.[55] Hallicrafters released its first "foto-etch" printed circuit product, a clock-radio, on 1 November 1952.[56]

https://en.wikipedia.org/wiki/Printed_circuit_board

The Hallicrafters Company manufactured, marketed, and sold radio equipment, and to a lesser extent televisions and phonographs, beginning in 1932. The company was founded by William J. Halligan and based in Chicago, Illinois, United States.

https://en.wikipedia.org/wiki/Hallicrafters

- **Regency TR-1 - first transistor radio (1954) - Texas instrument** 

In 1954, Texas Instruments designed and manufactured the first transistor radio. The Regency TR-1 used germanium transistors, as silicon transistors were much more expensive at the time. This was an effort by Haggerty to increase market demand for transistors.

https://en.wikipedia.org/wiki/Texas_Instruments

- **TPA-1 - first all-transistor phonograph (1955) - Philco**

In 1955, Philco developed and produced the world's first all-transistor phonograph models TPA-1 and TPA-2, which were announced in the June 28, 1955 edition of the Wall Street Journal.[54] Philco started to sell these all-transistor phonographs in the fall of 1955, for the price of $59.95. The October 1955 issue of Radio & Television News magazine (page 41), had a full page detailed article on Philco's new consumer product. The all-transistor portable phonograph TPA-1 and TPA-2 models played only 45rpm records and used four 1.5 volt "D" batteries for their power supply. The "TPA" stands for "Transistor Phonograph Amplifier". Their circuitry used three Philco germanium PNP alloy-fused junction audio frequency transistors. After the 1956 season had ended, Philco decided to discontinue both models, for transistors were too expensive compared to vacuum tubes,[55][56] but by 1961 a $49.95 ($452.94 in 2020) portable, battery-powered radio-phonograph with seven transistors was available.[57]

https://en.wikipedia.org/wiki/Phonograph

### Screens
**[`^        back to top        ^`](#)**
- **Shift from circular CRT to rectangular CRT (1950s)**
From 1949 to the early 1960s, there was a shift from circular CRTs to rectangular CRTs, although the first rectangular CRTs were made in 1938 by Telefunken.[36][22][37][38][39][40] While circular CRTs were the norm, European TV sets often blocked portions of the screen to make it appear somewhat rectangular while American sets often left the entire front of the CRT exposed or only blocked the upper and lower portions of the CRT.[41][42]

https://en.wikipedia.org/wiki/Cathode-ray_tube

- **15GP22 - first color CRT (1954) - RCA**

In 1954, RCA produced some of the first color CRTs, the 15GP22 CRTs used in the CT-100,[43] the first color TV set to be mass-produced.[44] The first rectangular color CRTs were also made in 1954.[45][46] However, the first rectangular color CRTs to be offered to the public were made in 1963. One of the challenges that had to be solved to produce the rectangular color CRT was convergence at the corners of the CRT.[39][38] In 1965, brighter rare earth phosphors began replacing dimmer and cadmium-containing red and green phosphors. Eventually blue phosphors were replaced as well.[47][48][49][50][51][52]

https://en.wikipedia.org/wiki/Cathode-ray_tube

### Broadcast
**[`^        back to top        ^`](#)**
- **Analog HD TV systems**

In 1949, France started its transmissions with an 819 lines system (with 737 active lines). The system was monochrome only and was used only on VHF for the first French TV channel. It was discontinued in 1983.

In 1958, the Soviet Union developed Тransformator (Russian: Трансформатор, meaning Transformer), the first high-resolution (definition) television system capable of producing an image composed of 1,125 lines of resolution aimed at providing teleconferencing for military command. It was a research project and the system was never deployed by either the military or consumer broadcasting.[6]

https://en.wikipedia.org/wiki/High-definition_television

- **Cable Television (1950)**

Cable television[5] began in the United States as a commercial business in 1950, although there were small-scale systems by hobbyists in the 1940s.
The early systems simply received weak (broadcast) channels, amplified them, and sent them over unshielded wires to the subscribers, limited to a community or to adjacent communities. The receiving antenna would be taller than any individual subscriber could afford, thus bringing in stronger signals; in hilly or mountainous terrain it would be placed at a high elevation.

https://en.wikipedia.org/wiki/Cable_television

### Video games
**[`^        back to top        ^`](#)**

The history of video games began in the 1950s and 1960s as computer scientists began designing simple games and simulations on minicomputers and mainframes
As early as 1950, computer scientists were using electronic machines to construct relatively simple game systems, such as **Bertie the Brain in 1950 to play tic tac toe, or Nimrod in 1951 for playing Nim**. These systems used either electronic light displays and mainly as demonstration systems at large exhibitions to showcase the power of computers at the time.[1][2] Another early demonstration was Tennis for Two, a game created by William Higinbotham at Brookhaven National Laboratory in 1958 for three-day exhibition, using an analog computer and an oscilloscope for a display.[3]

https://en.wikipedia.org/wiki/History_of_video_games

## Standards and protocols
**[`^        back to top        ^`](#)**

- **Color TV - NTSC standard (1953) - NTSC**
The National Television System Committee (NTSC)[1] developed the analog television format encoding system that was introduced in North America in 1954 and stayed in use until digital conversion.
NTSC color encoding is used with the System M television signal, which consists of 30⁄1.001 (approximately 29.97) interlaced frames of video per second. Each frame is composed of two fields, each consisting of 262.5 scan lines, for a total of 525 scan lines. Initially 486 scan lines make up the visible raster, although this was later standardized to 480. 

In 1953, a second NTSC standard was adopted, which allowed for color television broadcasting which was compatible with the existing stock of black-and-white receivers. NTSC was the first widely adopted broadcast color system and remained dominant until the 2000s, when it started to be replaced with different digital standards such as ATSC and others.
In December 1953, the FCC unanimously approved what is now called the NTSC color television standard (later defined as RS-170a). The compatible color standard retained full backward compatibility with then-existing black-and-white television sets. Color information was added to the black-and-white image by introducing a color subcarrier of precisely 315/88 MHz (usually described as 3.579545 MHz±10 Hz[10] or about 3.58 MHz). The precise frequency was chosen so that horizontal line-rate modulation components of the chrominance signal fall exactly in between the horizontal line-rate modulation components of the luminance signal, thereby enabling the chrominance signal to be filtered out of the luminance signal with minor degradation of the luminance signal. (Also, minimize the visibility on existing sets that do not filter it out.) Due to limitations of frequency divider circuits at the time the color standard was promulgated, the color subcarrier frequency was constructed as composite frequency assembled from small integers, in this case 5×7×9/(8×11) MHz.[11] The horizontal line rate was reduced to approximately 15,734 lines per second (3.579545×2/455 MHz = 9/572 MHz) from 15,750 lines per second, and the frame rate was reduced to 30/1.001 ≈ 29.970 frames per second (the horizontal line rate divided by 525 lines/frame) from 30 frames per second. These changes amounted to 0.1 percent and were readily tolerated by then-existing television receivers.[12][13]
The first publicly announced network television broadcast of a program using the NTSC "compatible color" system was an episode of NBC's Kukla, Fran and Ollie on August 30, 1953, although it was viewable in color only at the network's headquarters.[14] The first nationwide viewing of NTSC color came on the following January 1 with the coast-to-coast broadcast of the Tournament of Roses Parade, viewable on prototype color receivers at special presentations across the country. The first color NTSC television camera was the RCA TK-40, used for experimental broadcasts in 1953; an improved version, the TK-40A, introduced in March 1954, was the first commercially available color television camera. Later that year, the improved TK-41 became the standard camera used throughout much of the 1960s.
https://en.wikipedia.org/wiki/NTSC

In the 1950s, the Western European countries began plans to introduce colour television, and were faced with the problem that the NTSC standard demonstrated several weaknesses, including colour tone shifting under poor transmission conditions, which became a major issue considering Europe's geographical and weather-related particularities.
https://en.wikipedia.org/wiki/PAL

## Programming languages and frameworks
**[`^        back to top        ^`](#)**

- **KOMPILER - language compilation and runtime system (1953) - University of California Radiation Laboratory at Livermore**
The University of California Radiation Laboratory at Livermore developed a language compilation and runtime system called the KOMPILER for their IBM 701. Speedcode was the first high-level programming language created for an IBM computer. The language was developed by John Backus in 1953 for the IBM 701 to support computation with floating-point numbers. The Fortran compiler also developed by Backus was not released by IBM until the IBM 704.

https://en.wikipedia.org/wiki/IBM_701

- **Fortran - scientific programming language (1956)**

Fortran (/ˈfɔːrtræn/; formerly FORTRAN) is a general-purpose, compiled imperative programming language that is especially suited to numeric computation and scientific computing.

Fortran was originally developed by IBM[2] in the 1950s for scientific and engineering applications, and subsequently came to dominate scientific computing. It has been in use for over six decades in computationally intensive areas such as numerical weather prediction, finite element analysis, computational fluid dynamics, geophysics, computational physics, crystallography and computational chemistry. It is a popular language for high-performance computing[3] and is used for programs that benchmark and rank the world's fastest supercomputers.[4][5]

Fortran has had numerous versions, each of which has added extensions while largely retaining compatibility with preceding versions. Successive versions have added support for structured programming and processing of character-based data (FORTRAN 77), array programming, modular programming and generic programming (Fortran 90), High Performance Fortran (Fortran 95), object-oriented programming (Fortran 2003), concurrent programming (Fortran 2008), and native parallel computing capabilities (Coarray Fortran 2008/2018).

Fortran's design was the basis for many other programming languages. Among the better-known is BASIC, which is based on FORTRAN II with a number of syntax cleanups, notably better logical structures,[6] and other changes to work more easily in an interactive environment.[7]

As of August 2021, Fortran was ranked 13th in the TIOBE index, a measure of the popularity of programming languages, climbing 29 positions from its ranking of 42nd in August 2020.[8]
In late 1953, John W. Backus submitted a proposal to his superiors at IBM to develop a more practical alternative to assembly language for programming their IBM 704 mainframe computer.[10]: 69  Backus' historic FORTRAN team consisted of programmers Richard Goldberg, Sheldon F. Best, Harlan Herrick, Peter Sheridan, Roy Nutt, Robert Nelson, Irving Ziller, Harold Stern, Lois Haibt, and David Sayre.[15] Its concepts included easier entry of equations into a computer, an idea developed by J. Halcombe Laning and demonstrated in the Laning and Zierler system of 1952.[16]

The Fortran Automatic Coding System for the IBM 704 (15 October 1956), the first programmer's reference manual for Fortran[9]
A draft specification for The IBM Mathematical Formula Translating System was completed by November 1954.[10]: 71  The first manual for FORTRAN appeared in October 1956,[9][10]: 72  with the first FORTRAN compiler delivered in April 1957.[10]: 75  This was the first optimizing compiler, because customers were reluctant to use a high-level programming language unless its compiler could generate code with performance approaching that of hand-coded assembly language.[17]

While the community was skeptical that this new method could possibly outperform hand-coding, it reduced the number of programming statements necessary to operate a machine by a factor of 20, and quickly gained acceptance. John Backus said during a 1979 interview with Think, the IBM employee magazine, "Much of my work has come from being lazy. I didn't like writing programs, and so, when I was working on the IBM 701, writing programs for computing missile trajectories, I started work on a programming system to make it easier to write programs."[18]

The language was widely adopted by scientists for writing numerically intensive programs, which encouraged compiler writers to produce compilers that could generate faster and more efficient code. The inclusion of a complex number data type in the language made Fortran especially suited to technical applications such as electrical engineering.[19]

By 1960, versions of FORTRAN were available for the IBM 709, 650, 1620, and 7090 computers. Significantly, the increasing popularity of FORTRAN spurred competing computer manufacturers to provide FORTRAN compilers for their machines, so that by 1963 over 40 FORTRAN compilers existed. For these reasons, FORTRAN is considered to be the first widely used cross-platform programming language.

The development of Fortran paralleled the early evolution of compiler technology, and many advances in the theory and design of compilers were specifically motivated by the need to generate efficient code for Fortran programs

https://en.wikipedia.org/wiki/Fortran

- **LISP (1958)**

Lisp (historically LISP) is a family of programming languages with a long history and a distinctive, fully parenthesized prefix notation.[3] Originally specified in 1958, Lisp is the second-oldest high-level programming language still in common use. Only Fortran is older, by one year.[4][5] Lisp has changed since its early days, and many dialects have existed over its history. Today, the best-known general-purpose Lisp dialects are **Racket, Common Lisp, Scheme, and Clojure**.[citation needed]

Lisp was originally created as a practical mathematical notation for computer programs, influenced by (though not originally derived from)[6] the notation of Alonzo Church's lambda calculus. It quickly became the favored programming language for artificial intelligence (AI) research.[7] As one of the earliest programming languages, Lisp pioneered many ideas in computer science, including tree data structures, automatic storage management, dynamic typing, conditionals, higher-order functions, recursion, the self-hosting compiler,[8] and the read–eval–print loop.[9]

The name LISP derives from "LISt Processor".[10] Linked lists are one of Lisp's major data structures, and Lisp source code is made of lists. Thus, Lisp programs can manipulate source code as a data structure, giving rise to the macro systems that allow programmers to create new syntax or new domain-specific languages embedded in Lisp.

The interchangeability of code and data gives Lisp its instantly recognizable syntax. All program code is written as s-expressions, or parenthesized lists. A function call or syntactic form is written as a list with the function or operator's name first, and the arguments following; for instance, a function f that takes three arguments would be called as (f arg1 arg2 arg3).


John McCarthy developed Lisp in 1958 while he was at the Massachusetts Institute of Technology (MIT). McCarthy published its design in a paper in Communications of the ACM in 1960, entitled "Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I".[11] He showed that with a few simple operators and a notation for anonymous functions borrowed from Church, one can build a Turing-complete language for algorithms.

Information Processing Language was the first AI language, from 1955 or 1956, and already included many of the concepts, such as list-processing and recursion, which came to be used in Lisp.

McCarthy's original notation used bracketed "M-expressions" that would be translated into S-expressions. As an example, the M-expression car[cons[A,B]] is equivalent to the S-expression (car (cons A B)). Once Lisp was implemented, programmers rapidly chose to use S-expressions, and M-expressions were abandoned. M-expressions surfaced again with short-lived attempts of MLisp[12] by Horace Enea and CGOL by Vaughan Pratt.

Lisp was first implemented by Steve Russell on an IBM 704 computer using punched cards.[13] Russell had read McCarthy's paper and realized (to McCarthy's surprise) that the Lisp eval function could be implemented in machine code.

According to McCarthy:[14]

Steve Russell said, look, why don't I program this eval ... and I said to him, ho, ho, you're confusing theory with practice, this eval is intended for reading, not for computing. But he went ahead and did it. That is, he compiled the eval in my paper into IBM 704 machine code, fixing bugs, and then advertised this as a Lisp interpreter, which it certainly was. So at that point Lisp had essentially the form that it has today ...

The result was a working Lisp interpreter which could be used to run Lisp programs, or more properly, "evaluate Lisp expressions".

Two assembly language macros for the IBM 704 became the primitive operations for decomposing lists: car (Contents of the Address part of Register number) and cdr (Contents of the Decrement part of Register number),[15] where "register" refers to registers of the computer's central processing unit (CPU). Lisp dialects still use car and cdr (/kɑːr/ and /ˈkʊdər/) for the operations that return the first item in a list and the rest of the list, respectively.

The first complete Lisp compiler, written in Lisp, was implemented in 1962 by Tim Hart and Mike Levin at MIT, and could be compiled by simply having an existing LISP interpreter interpret the compiler code, producing machine code output able to be executed at a 40-fold improvement in speed over that of the interpreter.[16] This compiler introduced the Lisp model of incremental compilation, in which compiled and interpreted functions can intermix freely. The language used in Hart and Levin's memo is much closer to modern Lisp style than McCarthy's earlier code.

Garbage collection routines were developed by MIT graduate student[citation needed] Daniel Edwards, prior to 1962.[17]

During the 1980s and 1990s, a great effort was made to unify the work on new Lisp dialects (mostly successors to Maclisp such as ZetaLisp and NIL (New Implementation of Lisp) into a single language. The new language, Common Lisp, was somewhat compatible with the dialects it replaced (the book Common Lisp the Language notes the compatibility of various constructs). In 1994, ANSI published the Common Lisp standard, "ANSI X3.226-1994 Information Technology Programming Language Common Lisp".

https://en.wikipedia.org/wiki/Lisp_(programming_language)

- **ALGOL (1958)**

ALGOL (/ˈælɡɒl, -ɡɔːl/; short for "Algorithmic Language")[1] is a family of imperative computer programming languages originally developed in 1958. ALGOL heavily influenced many other languages and was the standard method for algorithm description used by the Association for Computing Machinery (ACM) in textbooks and academic sources for more than thirty years.[2]

In the sense that the syntax of most modern languages is "Algol-like",[3] it was arguably more influential than three other high-level programming languages among which it was roughly contemporary: FORTRAN, Lisp, and COBOL.[4] It was designed to avoid some of the perceived problems with FORTRAN and eventually gave rise to many other programming languages, including PL/I, Simula, BCPL, B, Pascal, and C.

ALGOL introduced code blocks and the begin...end pairs for delimiting them. It was also the first language implementing nested function definitions with lexical scope. Moreover, it was the first programming language which gave detailed attention to formal language definition and through the Algol 60 Report introduced Backus–Naur form, a principal formal grammar notation for language design.

There were three major specifications, named after the years they were first published:

ALGOL 58 – originally proposed to be called IAL, for International Algebraic Language.
ALGOL 60 – first implemented as X1 ALGOL 60 in 1961. Revised 1963.[5][6][7]
ALGOL 68 – introduced new elements including flexible arrays, slices, parallelism, operator identification. Revised 1973.[8]
ALGOL 68 is substantially different from ALGOL 60 and was not well received, so in general "Algol" means ALGOL 60 and its dialects.

https://en.wikipedia.org/wiki/ALGOL

- **COBOL (1959)**

COBOL (/ˈkoʊbɒl, -bɔːl/; an acronym for "common business-oriented language") is a compiled English-like computer programming language designed for business use. It is an imperative, procedural and, since 2002, object-oriented language. COBOL is primarily used in business, finance, and administrative systems for companies and governments. COBOL is still widely used in applications deployed on mainframe computers, such as large-scale batch and transaction processing jobs. However, due to its declining popularity and the retirement of experienced COBOL programmers, programs are being migrated to new platforms, rewritten in modern languages or replaced with software packages.[9] Most programming in COBOL is now purely to maintain existing applications; however, many large financial institutions were still developing new systems in COBOL as late as 2006.[10]

COBOL was designed in 1959 by CODASYL and was partly based on the programming language FLOW-MATIC designed by Grace Hopper. It was created as part of a US Department of Defense effort to create a portable programming language for data processing. It was originally seen as a stopgap, but the Department of Defense promptly forced computer manufacturers to provide it, resulting in its widespread adoption.[11] It was standardized in 1968 and has since been revised four times. Expansions include support for structured and object-oriented programming. The current standard is ISO/IEC 1989:2014.[12]

COBOL statements have an English-like syntax, which was designed to be self-documenting and highly readable. However, it is verbose and uses over 300 reserved words. In contrast with modern, succinct syntax like y = x;, COBOL has a more English-like syntax (in this case, MOVE x TO y). COBOL code is split into four divisions (identification, environment, data, and procedure) containing a rigid hierarchy of sections, paragraphs and sentences. Lacking a large standard library, the standard specifies 43 statements, 87 functions and just one class.

Academic computer scientists were generally uninterested in business applications when COBOL was created and were not involved in its design; it was (effectively) designed from the ground up as a computer language for business, with an emphasis on inputs and outputs, whose only data types were numbers and strings of text.[13] COBOL has been criticized throughout its life for its verbosity, design process, and poor support for structured programming. These weaknesses result in monolithic, verbose (intended to be English-like) programs that are not easily comprehensible.

For years, COBOL has been assumed as a programming language for business operations in mainframes,[14] although in recent years an increasing interest has surged on migrating COBOL operations to cloud computing.[15]

In the late 1950s, computer users and manufacturers were becoming concerned about the rising cost of programming. A 1959 survey had found that in any data processing installation, the programming cost US$800,000 on average and that translating programs to run on new hardware would cost $600,000. At a time when new programming languages were proliferating at an ever-increasing rate, the same survey suggested that if a common business-oriented language were used, conversion would be far cheaper and faster.[16]

On 8 April 1959, Mary K. Hawes, a computer scientist at Burroughs Corporation, called a meeting of representatives from academia, computer users, and manufacturers at the University of Pennsylvania to organize a formal meeting on common business languages.[17] Representatives included Grace Hopper (inventor of the English-like data processing language FLOW-MATIC), Jean Sammet and Saul Gorn.[18][19]

At the April meeting, the group asked the Department of Defense (DoD) to sponsor an effort to create a common business language. The delegation impressed Charles A. Phillips, director of the Data System Research Staff at the DoD,[20] who thought that they "thoroughly understood" the DoD's problems. The DoD operated 225 computers, had a further 175 on order and had spent over $200 million on implementing programs to run on them. Portable programs would save time, reduce costs and ease modernization.[21]

Charles Phillips agreed to sponsor the meeting and tasked the delegation with drafting the agenda.[22]

COBOL programs are used globally in governments and businesses and are running on diverse operating systems such as z/OS, z/VSE, VME, Unix, OpenVMS and Windows. In 1997, the Gartner Group reported that 80% of the world's business ran on COBOL with over 200 billion lines of code[c] and 5 billion lines more being written annually.[108]

Near the end of the 20th century, the year 2000 problem (Y2K) was the focus of significant COBOL programming effort, sometimes by the same programmers who had designed the systems decades before. The particular level of effort required to correct COBOL code has been attributed to the large amount of business-oriented COBOL, as business applications use dates heavily, and to fixed-length data fields.[109] Some studies attribute as much as "24% of Y2K software repair costs to Cobol".[110] After the clean-up effort put into these programs for Y2K, a 2003 survey found that many remained in use.[111] The authors said that the survey data suggest "a gradual decline in the importance of COBOL in application development over the [following] 10 years unless ... integration with other languages and technologies can be adopted".[112]

In 2006 and 2012, Computerworld surveys (of 352 readers) found that over 60% of organizations used COBOL (more than C++ and Visual Basic .NET) and that for half of those, COBOL was used for the majority of their internal software.[10][113] 36% of managers said they planned to migrate from COBOL, and 25% said they would like to if it were cheaper. Instead, some businesses have migrated their systems from expensive mainframes to cheaper, more modern systems, while maintaining their COBOL programs.[10]

Testimony before the House of Representatives in 2016 indicated that COBOL is still in use by many federal agencies.[114] Reuters reported in 2017 that 43% of banking systems still used COBOL with over 220 billion lines of COBOL code in use.[115]

By 2019, the number of COBOL programmers was shrinking fast due to retirements, leading to an impending skills gap in business and government organizations which still use mainframe systems for high-volume transaction processing. Efforts to rewrite systems in newer languages have proven expensive and problematic, as has the outsourcing of code maintenance, thus proposals to train more people in COBOL are advocated.[116]

During the COVID-19 pandemic and the ensuing surge of unemployment, several US states reported a shortage of skilled COBOL programmers to support the legacy systems used for unemployment benefit management. Many of these systems had been in the process of conversion to more modern programming languages prior to the pandemic, but the process had to be put on hold.[117] Similarly, the US Internal Revenue Service rushed to patch its COBOL-based Individual Master File in order to disburse the tens of millions of payments mandated by the Coronavirus Aid, Relief, and Economic Security Act.[118]

https://en.wikipedia.org/wiki/COBOL

## Navigation
**[`^        back to top        ^`](#)**

- **Pre-GPS systems: U.S. Department of Defense research, Long-Range Navigation System, TRANSIT (1955-1960)**

The U.S. Department of Defense developed the system, which originally used 24 satellites, for use by the United States military, and became fully operational in 1995. Civilian use was allowed from the 1980s. Roger L. Easton of the Naval Research Laboratory, Ivan A. Getting of The Aerospace Corporation, and Bradford Parkinson of the Applied Physics Laboratory are credited with inventing it.[19] The work of Gladys West is credited as instrumental in the development of computational techniques for detecting satellite positions with the precision needed for GPS.[20]

The design of GPS is based partly on similar ground-based radio-navigation systems, such as LORAN and the Decca Navigator, developed in the early 1940s.

In 1955, Friedwardt Winterberg proposed a test of general relativity—detecting time slowing in a strong gravitational field using accurate atomic clocks placed in orbit inside artificial satellites. Special and general relativity predicted that the clocks on GPS satellites, as observed by those on Earth, run 38 microseconds faster per day than those on the Earth. The design of GPS corrects for this difference; because without doing so, GPS calculated positions would accumulate errors of up to 10 kilometers per day (6 mi/d).[21]

In 1955, Dutch Naval officer Wijnand Langeraar submitted a patent application for a radio-based Long-Range Navigation System, with the US Patent office on February 16, 1955, and was granted Patent US2980907A [22] on April 18, 1961.[original research?]

When the Soviet Union launched the first artificial satellite (Sputnik 1) in 1957, two American physicists, William Guier and George Weiffenbach, at Johns Hopkins University's Applied Physics Laboratory (APL) decided to monitor its radio transmissions.[23] Within hours they realized that, because of the Doppler effect, they could pinpoint where the satellite was along its orbit. The Director of the APL gave them access to their UNIVAC to do the heavy calculations required.

Early the next year, Frank McClure, the deputy director of the APL, asked Guier and Weiffenbach to investigate the inverse problem: pinpointing the user's location, given the satellite's. (At the time, the Navy was developing the submarine-launched Polaris missile, which required them to know the submarine's location.) This led them and APL to develop the TRANSIT system.[24] In 1959, ARPA (renamed DARPA in 1972) also played a role in TRANSIT.[25][26][27]

TRANSIT was first successfully tested in 1960.[28] It used a constellation of five satellites and could provide a navigational fix approximately once per hour.

https://en.wikipedia.org/wiki/Global_Positioning_System





