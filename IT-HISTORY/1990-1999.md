# 1990s

**In short:**
- Broadband internet (ADSL)
- IP traffic (birth of IPv6)
- Web/internet (HTTP servers and browsers, HTML, URL, XML)
- Home computer
- Laptop
- Screens : XVGA
- Storage: SAN, Fiber Channel
- Motherboard: Integrated Circuits to Motherboards : VESA, PCI, USB, EFI
- CPU: PowerPC, Pentium, MMX, Xeon, SSE, dual-CPU
- GPU : OpenGL, Direct3D, Unreal engine 
- Mobile: PDA (PCMIA), Bluetooth development, early-form RFID
- HDTV
- DVD era
- fourth (Super NES) to fifth generation video game consoles (Nintento 64, Playstation)
- Standards : JPEG, MPEG-1, MPEG-2, WAV, MP3, PNG, SIP, SSH, VNC, Rsync, ZIP compression tools and competitors (Winzip, RAR, ARJ, DEFLATE, 7-zip)
- Navigation : GPS is operational, military use, miniaturization and move to dual use

## Telecommunications : Broadband internet ##

In the 1990s, with the advances of the Internet, leased lines were also used to **connect customer premises to ISP point of presence** 

https://en.wikipedia.org/wiki/Leased_line

- **ADSL**

Lechleider also believed this higher-speed standard would be much more attractive to customers than ISDN had proven. Unfortunately, at these speeds, the systems suffered from a type of crosstalk known as "NEXT", for "near-end crosstalk". This made longer connections on customer lines difficult. Lechleider noted that NEXT only occurred when similar frequencies were being used, and could be diminished if one of the directions used a different carrier rate, but doing so would reduce the potential bandwidth of that channel. Lechleider suggested that most consumer use would be asymmetric anyway, and that providing a high-speed channel towards the user and a lower speed return would be suitable for many uses.[6]
This work in the early 1990s eventually led to the ADSL concept, which emerged in 1995. An early supporter of the concept was Alcatel, who jumped on ADSL while many other companies were still devoted to ISDN. Krish Prabu stated that "Alcatel will have to invest one billion dollars in ADSL before it makes a profit, but it is worth it." They introduced the first DSL Access Multiplexers (DSLAM), the large multi-modem systems used at the telephony offices, and later introduced customer ADSL modems under the Thomson brand. Alcatel remained the primary vendor of ADSL systems for well over a decade.[7]
ADSL quickly replaced ISDN as the customer-facing solution for last-mile connectivity. ISDN has largely disappeared on the customer side, remaining in use only in niche roles like dedicated teleconferencing systems and similar legacy systems.

https://en.wikipedia.org/wiki/Integrated_Services_Digital_Network

## Networking

- **Transition to IP traffic**

Beginning in March 1991 the JANET IP Service (JIPS) was set up as a pilot project to host IP traffic on the existing network.[69] Within eight months the IP traffic had exceeded the levels of X.25 traffic, and the IP support became official in November. Also in 1991, Dai Davies introduced Internet technology over X.25 into the pan-European NREN, EuropaNet, although he experienced personal opposition to this approach.[70][71] The European Academic and Research Network (EARN) and RARE adopted IP around the same time,[nb 7] and the European Internet backbone EBONE became operational in 1992.[53] OSI usage on the NSFNET remained low when compared to TCP/IP. There was some talk of moving JANET to OSI protocols in the 1990s, but this never happened. The X.25 service was closed in August 1997

https://en.wikipedia.org/wiki/Protocol_Wars

## Cryptography : Asymetric keys ##

- **PGP (1991)**

Pretty Good Privacy is an encryption program that provides cryptographic privacy and authentication for data communication

https://en.wikipedia.org/wiki/Pretty_Good_Privacy

- **DSA (1991)**

The Digital Signature Algorithm (DSA) is a Federal Information Processing Standard for digital signatures, based on the mathematical concept of modular exponentiation and the discrete logarithm problem. DSA is a variant of the Schnorr and ElGamal signature schemes.[1]: 486 . 
The National Institute of Standards and Technology (NIST) proposed DSA for use in their Digital Signature Standard (DSS) in 1991, and adopted it as FIPS 186 in 1994

https://en.wikipedia.org/wiki/Digital_Signature_Algorithm
- Leads to SSL and TLS development (see below)

- **RSA released in public domain before patent expiration in 2000**

https://en.wikipedia.org/wiki/RSA_(cryptosystem)

## Computers

### Evolution of mainframes

graphic display terminals, and terminal emulation became obsolete in the 1990s due to the advent of personal computers provided with **GUIs**.

https://en.wikipedia.org/wiki/Mainframe_computer

In 1991, AT&T Corporation briefly owned NCR. During the same period, companies found that servers based on microcomputer designs could be deployed at a fraction of the acquisition price and offer local users much greater control over their own systems given the IT policies and practices at that time. Terminals used for interacting with mainframe systems were gradually replaced by personal computers. Consequently, demand plummeted and new mainframe installations were restricted mainly to financial services and government. In the early 1990s, there was a rough consensus among industry analysts that the mainframe was a dying market as mainframe platforms were increasingly replaced by personal computer networks. InfoWorld's Stewart Alsop infamously predicted that the last mainframe would be unplugged in 1996; in 1993, he cited Cheryl Currid, a computer industry analyst as saying that the last mainframe "will stop working on December 31, 1999",[20] a reference to the anticipated Year 2000 problem (Y2K).

https://en.wikipedia.org/wiki/Mainframe_computer

### Evolution of hardware

- **Integrated ICs to Motherboards (1990s)**

By the late 1990s, many personal computer motherboards included consumer-grade embedded audio, video, storage, and networking functions without the need for any expansion cards at all; higher-end systems for 3D gaming and computer graphics typically retained only the graphics card as a separate component. Business PCs, workstations, and servers were more likely to need expansion cards, either for more robust functions, or for higher speeds; those systems often had fewer embedded components.

Laptop and notebook computers that were developed in the 1990s integrated the most common peripherals. This even included motherboards with no upgradeable components, a trend that would continue as smaller systems were introduced after the turn of the century (like the tablet computer and the netbook). Memory, processors, network controllers, power source, and storage would be integrated into some systems.

Motherboards contain a ROM (and later EPROM, EEPROM, NOR flash) to initialize hardware devices and load an operating system from a peripheral device. Microcomputers such as the Apple II and IBM PC used ROM chips mounted in sockets on the motherboard. At power-up, the central processor unit would load its program counter with the address of the Boot ROM and start executing instructions from the Boot ROM. These instructions initialized and tested the system hardware, displays system information on the screen, performed RAM checks, and then loaded an operating system from a peripheral device

https://en.wikipedia.org/wiki/Motherboard

- **front side bus (1990s)**

The front-side bus was used in all Intel Atom, Celeron, Pentium, Core 2, and Xeon processor models through about 2008. Originally, this bus was a central connecting point for all system devices and the CPU.
A front-side bus (FSB) is a computer communication interface (bus) that was often used in Intel-chip-based computers during the 1990s and 2000s. The EV6 bus served the same function for competing AMD CPUs. Both typically carry data between the central processing unit (CPU) and a memory controller hub, known as the northbridge.[1]
Depending on the implementation, some computers may also have a back-side bus that connects the CPU to the cache. This bus and the cache connected to it are faster than accessing the system memory (or RAM) via the front-side bus. The speed of the front side bus is often used as an important measure of the performance of a computer.
The original front-side bus architecture has been replaced by HyperTransport, Intel QuickPath Interconnect or Direct Media Interface in modern volume CPUs.
The term came into use by Intel Corporation about the time the Pentium Pro and Pentium II products were announced, in the 1990s.
"Front side" refers to the external interface from the processor to the rest of the computer system, as opposed to the back side, where the back-side bus connects the cache (and potentially other CPUs).[2]
A front-side bus (FSB) is mostly used on PC-related motherboards (including personal computers and servers). They are seldom used in embedded systems or similar small computers. The FSB design was a performance improvement over the single system bus designs of the previous decades, but these front-side buses are sometimes referred to as the "system bus."
Front-side buses usually connect the CPU and the rest of the hardware via a chipset, which Intel implemented as a northbridge and a southbridge. Other buses like the Peripheral Component Interconnect (PCI), Accelerated Graphics Port (AGP), and memory buses all connect to the chipset in order for data to flow between the connected devices. These secondary system buses usually run at speeds derived from the front-side bus clock, but are not necessarily synchronized to it.
In response to AMD's Torrenza initiative, Intel opened its FSB CPU socket to third party devices.[3] Prior to this announcement, made in Spring 2007 at Intel Developer Forum in Beijing, Intel had very closely guarded who had access to the FSB, only allowing Intel processors in the CPU socket. The first example was field-programmable gate array (FPGA) co-processors, a result of collaboration between Intel-Xilinx-Nallatech[4] and Intel-Altera-XtremeData (which shipped in 2008).[5][6][7]

https://en.wikipedia.org/wiki/Front-side_bus

- **XGA - Extended Graphics Array (1990) - IBM**

The Extended Graphics Array (XGA) is an IBM display standard introduced in 1990. Later it became the most common appellation of the **1024 × 768 pixels** display resolution, but the official definition is broader than that. It was not a new and improved replacement for Super VGA, but rather became one particular subset of the broad range of capabilities covered under the "Super VGA" umbrella.

https://en.wikipedia.org/wiki/Graphics_display_resolution#Extended_Graphics_Array


- **PowerPC (1991) - Apple–IBM–Motorola alliance**

PowerPC (with the backronym Performance Optimization With Enhanced RISC – Performance Computing, sometimes abbreviated as PPC) is a reduced instruction set computer (RISC) instruction set architecture (ISA) created by the 1991 Apple–IBM–Motorola alliance, known as AIM. PowerPC, as an evolving instruction set, has been named Power ISA since 2006, while the old name lives on as a trademark for some implementations of Power Architecture–based processors.

PowerPC was the cornerstone of AIM's PReP and Common Hardware Reference Platform (CHRP) initiatives in the 1990s. Originally intended for personal computers, the architecture is well known for being used by Apple's Power Macintosh, PowerBook, iMac, iBook, eMac, and Xserve lines from 1994 until 2005, when Apple migrated to Intel's x86. It has since become a niche in personal computers, but remains popular for embedded and high-performance processors. Its use in 7th generation of video game consoles and embedded applications provide an array of uses, including satellites, and the Curiosity and Perseverance rovers on Mars. In addition, PowerPC CPUs are still used in AmigaOne and third party AmigaOS 4 personal computers.

PowerPC is largely based on the earlier IBM POWER architecture, and retains a high level of compatibility with it; the architectures have remained close enough that the same programs and operating systems will run on both if some care is taken in preparation; newer chips in the Power series use the Power ISA.

https://en.wikipedia.org/wiki/PowerPC


- **first commercial flash-based SSD (1991) - SanDisk**

The first commercial flash-based SSD was shipped by SanDisk in 1991.[35] It was a 20 MB SSD in a PCMCIA configuration, and sold OEM for around $1,000 and was used by IBM in a ThinkPad laptop.[39] In 1998, SanDisk introduced SSDs in 2.5-inch and 3.5-inch form factors with PATA interfaces.[40]

In 1995, STEC, Inc. entered the flash memory business for consumer electronic devices.[41]

In 1995, M-Systems introduced flash-based solid-state drives[42] as HDD replacements for the military and aerospace industries, as well as for other mission-critical applications. These applications require the SSD's ability to withstand extreme shock, vibration, and temperature ranges.[43]

In 1999, BiTMICRO made a number of introductions and announcements about flash-based SSDs, including an 18 GB[44] 3.5-inch SSD.[45] In 2007, Fusion-io announced a PCIe-based Solid state drive with 100,000 input/output operations per second (IOPS) of performance in a single card, with capacities up to 320 GB.[46]

https://en.wikipedia.org/wiki/Solid-state_drive

- **Psion3 - PDA with full keyboard (1991) - Psion**

The first PDA, the Organiser, was released in 1984 by Psion, followed by Psion's Series 3, in 1991. The latter began to resemble the more familiar PDA style, including a full keyboard.[4][5] The term PDA was first used on January 7, 1992 by Apple Inc. CEO John Sculley at the Consumer Electronics Show in Las Vegas, Nevada, referring to the Apple Newton.[6] In 1994, IBM introduced the first PDA with analog cellular phone functionality, the IBM Simon, which can also be considered the first smartphone. Then in 1996, Nokia introduced a PDA with digital cellphone functionality, the 9000 Communicator. Another early entrant in this market was Palm, with a line of PDA products which began in March 1996. Palm would eventually be the dominant vendor of PDAs until the rising popularity of Pocket PC devices in the early 2000s.[7] By the mid-2000s most PDAs had morphed into smartphones as classic PDAs without cellular radios were increasingly becoming uncommon.

https://en.wikipedia.org/wiki/Personal_digital_assistant

- **PCI 1.0 - Peripheral Component Interconnect (1992) - Intel**

Peripheral Component Interconnect (PCI)[3] is a local computer bus for attaching hardware devices in a computer and is part of the PCI Local Bus standard. The PCI bus supports the functions found on a processor bus but in a standardized format that is independent of any given processor's native bus. Devices connected to the PCI bus appear to a bus master to be connected directly to its own bus and are assigned addresses in the processor's address space.
Work on PCI began at the Intel Architecture Labs (IAL, also Architecture Development Lab) c. 1990. A team of primarily IAL engineers defined the architecture and developed a proof of concept chipset and platform (Saturn) partnering with teams in the company's desktop PC systems and core logic product organizations.

PCI was immediately put to use in servers, replacing Micro Channel architecture (MCA) and Extended Industry Standard Architecture (EISA) as the server expansion bus of choice. In mainstream PCs, PCI was slower to replace VLB, and did not gain significant market penetration until late 1994 in second-generation Pentium PCs. By 1996, VLB was all but extinct, and manufacturers had adopted PCI even for Intel 80486 (486) computers.[11] EISA continued to be used alongside PCI through 2000. Apple Computer adopted PCI for professional Power Macintosh computers (replacing NuBus) in mid-1995, and the consumer Performa product line (replacing LC Processor Direct Slot (PDS)) in mid-1996.

https://en.wikipedia.org/wiki/Peripheral_Component_Interconnect

- **PCMIA (1990-1992)**

In computing, PC Card is a configuration for computer parallel communication peripheral interface, designed for laptop computers. Originally introduced as PCMCIA, the PC Card standard as well as its successors like CardBus were defined and developed by the Personal Computer Memory Card International Association (PCMCIA).
It was originally designed as a standard for memory-expansion cards for computer storage. The existence of a usable general standard for notebook peripherals led to many kinds of devices being made available based on its configurability, including network cards, modems, and hard disks.
The PCMCIA 1.0 card standard was published by the Personal Computer Memory Card International Association in November 1990 and was soon adopted by more than eighty vendors.[1] [2] It corresponds with the Japanese JEIDA memory card 4.0 standard.[2]
**SanDisk** (operating at the time as "SunDisk") launched its PCMCIA card in October 1992. The company was the first to introduce a writeable Flash RAM card for the HP 95LX (the first MS-DOS pocket computer). These cards conformed to a supplemental PCMCIA-ATA standard that allowed them to appear as more conventional IDE hard drives to the 95LX or a PC. This had the advantage of raising the upper limit on capacity to the full 32M available under DOS 3.22 on the 95LX.[3]
Type II PC Card: IBM V.34 data/fax modem, manufactured by TDK
It soon became clear that the PCMCIA card standard needed expansion to support "smart" I/O cards to address the emerging need for fax, modem, LAN, harddisk and floppy disk cards.[1] It also needed interrupt facilities and hot plugging, which required the definition of new BIOS and operating system interfaces.[1] This led to the introduction of release 2.0 of the PCMCIA standard and JEIDA 4.1 in September 1991,[1][2] which saw corrections and expansion with Card Services (CS) in the PCMCIA 2.1 standard in November 1992.[1][2]

https://en.wikipedia.org/wiki/PC_Card

- **Storage area network (SAN) (1992-1993)"

A storage area network (SAN) or storage network is a computer network which provides access to consolidated, block-level data storage. SANs are primarily used to access data storage devices, such as disk arrays and tape libraries from servers so that the devices appear to the operating system as direct-attached storage. A SAN typically is a dedicated network of storage devices not accessible through the local area network (LAN).

Although a SAN provides only block-level access, file systems built on top of SANs do provide file-level access and are known as shared-disk file systems.

Newer SAN configurations enable hybrid SAN[1] and allow traditional block storage that appears as local storage but also object storage for web services through APIs.

Storage area networks (SANs) are sometimes referred to as network behind the servers[2]: 11  and historically developed out of a centralized data storage model, but with its own data network. A SAN is, at its simplest, a dedicated network for data storage. 

https://en.wikipedia.org/wiki/Storage_area_network

Storage Systems were evolving since when we were introduced to this technology. It was likely 1993 or 1994 when a new storage technology emerged named Storage Area Network aka SAN. At first, it was not that popular as you know it can happen with any new technology. 

Also, the cost of developing a newly arrived storage system was a crucial factor. As you know the users need various types of components to build a SAN. For instance, you will need SAN switches, servers, storage disks, tape libraries, and JBODS, etc. So, overall the price of this technology was very expensive at that time.

Due to that, only the big companies could take advantage of this brand new storage system. With the passing of time, a lot of data storage system providers from all around the world started to offer pre-built SAN storage to businesses and organizations. As a result, the competition between those companies grew and the price of the components also came to a lower level. And, SAN became a popular alternative to store and manage data.

Added to that, when people saw that it uses fiber channel technology and fiber channel protocol for transferring data, it was a hit. We will talk about the popularity of SAN in the later section of the post in detail. But overall, we can say that SAN came into the scene back in the 90s and dominated the storage system market for over a decade and a half.

https://www.reviewplan.com/evolution-of-storage-area-network/

- **Fibre Channel (FC) (1993) - IBM**

Fibre Channel (FC) is a high-speed data transfer protocol providing in-order, lossless[1] delivery of raw block data.[2] Fibre Channel is primarily used to connect computer data storage to servers[3][4] in storage area networks (SAN) in commercial data centers.
Fibre Channel networks form a switched fabric because the switches in a network operate in unison as one big switch. Fibre Channel typically runs on optical fiber cables within and between data centers, but can also run on copper cabling.[3][4] Supported data rates include 1, 2, 4, 8, 16, 32, 64, and 128 gigabit per second resulting from improvements in successive technology generations. The industry now notates this as Gigabit Fibre Channel (GFC).
There are various upper-level protocols for Fibre Channel, including two for block storage. Fibre Channel Protocol (FCP) is a protocol that transports SCSI commands over Fibre Channel networks.[3][4] FICON is a protocol that transports ESCON commands, used by IBM mainframe computers, over Fibre Channel. Fibre Channel can be used to transport data from storage systems that use solid-state flash memory storage medium by transporting NVMe protocol commands.
133 Mbit/s	0.1328125	8b10b	12.5	1993

https://en.wikipedia.org/wiki/Fibre_Channel

- **Intel Pentium (1993) - Intel**

The Pentium (also referred to as P5, its microarchitecture) is a fifth generation, 32-bit x86 microprocessor that was introduced by Intel on March 22, 1993, as the very first CPU in the Pentium brand.[2][3] It was instruction set compatible with the 80486 but was a new and very different microarchitecture design from previous iterations. The P5 Pentium was the first superscalar x86 microarchitecture and the world's first superscalar microprocessor to be in mass production-meaning it generally executes at least 2 instructions per clock mainly because of a design-first dual integer pipeline design previously thought impossible to implement on a CISC microarchitecture. Additonal features include a faster floating-point unit, wider data bus, separate code and data caches, and many other techniques and features to enhance performance and support security, encryption, and multiprocessing, for workstations and servers when compared to the next best previous industry standard processor implementation before it; the Intel 80486.

Considered the fifth main generation in the 8086 compatible line of processors, its implementation and microarchitecture was called P5. As with all new processors from Intel since the Pentium, some new instructions were added to enhance performance for specific types of workloads.

The Pentium was the first Intel x86 to build in robust hardware support for multiprocessing similar to that of large IBM mainframe computers. Intel worked closely with IBM to define this ability and then Intel designed it into the P5 microarchitecture. This new ability was absent in prior x86 generations and x86 copies from competitors.

To realize its greatest potential, compilers had to be optimized to exploit the instruction level parallelism provided by the new superscalar dual pipelines and applications needed to be recompiled. Intel spent substantial effort and resources working with development tool vendors, and major independent software vendor (ISV) and operating system (OS) companies to optimize their products for Pentium before product launch.

https://en.wikipedia.org/wiki/Pentium_(original)

- **VESA Card (1993)**

The VESA Local Bus (usually abbreviated to VL-Bus or VLB) is a short-lived expansion bus introduced during the i486 generation of x86 IBM-compatible personal computers. Created by VESA (Video Electronics Standards Association), the VESA Local Bus worked alongside the then-dominant ISA bus to provide a standardized high-speed conduit intended primarily to accelerate video (graphics) operations. VLB provides a standardized fast path that add-in (video) card makers could tap for greatly accelerated memory-mapped I/O and DMA, while still using the familiar ISA bus to handle basic device duties such as interrupts and port-mapped I/O. Some high-end 386dx motherboards also had a VL-Bus slot.

https://en.wikipedia.org/wiki/VESA_Local_Bus

- **IEEE 1284 - bidirectional printer cable (1994)**

In 1991 the Network Printing Alliance was formed to develop a new standard. In March 1994, the IEEE 1284 specification was released. 1284 included all of these modes, and allowed operation in any of them.
An IEEE 1284 compliant printer cable, with both DB-25 and 36-pin Centronics connectors
The IEEE 1284 standard allows for faster throughput and bidirectional data flow with a theoretical maximum throughput of 4 megabytes per second; actual throughput is around 2 megabytes/second depending on hardware. In the printer venue, this allows for faster printing and back-channel status and management. Since the new standard allowed the peripheral to send large amounts of data back to the host, devices that had previously used SCSI interfaces could be produced at a much lower cost. This included scanners, tape drives, hard disks, computer networks connected directly via parallel interface, network adapters and other devices. No longer was the consumer required to purchase an expensive SCSI card—they could simply use their built-in parallel interface.
The parallel interface has since been mostly displaced by local area network interfaces and USB 2.0.

https://en.wikipedia.org/wiki/IEEE_1284

- **CompactFlash (CF) - flash memory mass storage (1994) - Sandisk**

CompactFlash (CF) is a flash memory mass storage device used mainly in portable electronic devices. The format was specified and the devices were first manufactured by SanDisk in 1994.[3] CompactFlash became one of the most successful of the early memory card formats, surpassing Miniature Card and SmartMedia. Subsequent formats, such as MMC/SD, various Memory Stick formats, and xD-Picture Card offered stiff competition

https://en.wikipedia.org/wiki/CompactFlash

- **DVD (1996)**

The DVD (common abbreviation for Digital Video Disc or Digital Versatile Disc)[8][9] is a digital optical disc data storage format invented and developed in 1995 and released in late 1996. Currently allowing up to 17.08 GB of storage,[10] the medium can store any kind of digital data and was widely used for software and other computer files as well as video programs watched using DVD players. DVDs offer higher storage capacity than compact discs while having the same dimensions.

https://en.wikipedia.org/wiki/DVD

- **USB 1.X - Universal Serial Bus (1996)**

Universal Serial Bus (USB) is an industry standard that establishes specifications for cables, connectors and protocols for connection, communication and power supply (interfacing) between computers, peripherals and other computers.[2] A broad variety of USB hardware exists, including 14 different connector types, of which USB-C is the most recent and the only one not currently deprecated.
First released in 1996, the USB standards are maintained by the USB Implementers Forum (USB-IF). The four generations of USB are: USB 1.x, USB 2.0, USB 3.x, and USB4.[3]
 * Compaq, DEC, IBM, Intel, Microsoft, NEC, Nortel
USB 1.x
Released in January 1996, USB 1.0 specified signaling rates of 1.5 Mbit/s (Low Bandwidth or Low Speed) and 12 Mbit/s (Full Speed).[15] It did not allow for extension cables or pass-through monitors, due to timing and power limitations. Few USB devices made it to the market until USB 1.1 was released in August 1998. USB 1.1 was the earliest revision that was widely adopted and led to what Microsoft designated the "Legacy-free PC".[16][17][18]

https://en.wikipedia.org/wiki/USB

- **Intel Pentium MMX - MMX instruction set (1996) - Intel**

In October 1996, the similar Pentium MMX[4] was introduced, complementing the same basic microarchitecture with the MMX instruction set, larger caches, and some other enhancements.

https://en.wikipedia.org/wiki/Pentium_(original)

MMX is a single instruction, multiple data (SIMD) instruction set architecture designed by Intel, introduced on January 8, 1997[1][2] with its Pentium P5 (microarchitecture) based line of microprocessors, named "Pentium with MMX Technology".[3] It developed out of a similar unit introduced on the Intel i860,[4] and earlier the Intel i750 video pixel processor. MMX is a processor supplementary capability that is supported on IA-32 processors by Intel and other vendors as of 1997.

https://en.wikipedia.org/wiki/MMX_(instruction_set)

- **CardBus (1995-1997)**

CardBus are PCMCIA 5.0 or later (JEIDA 4.2 or later) 32-bit PCMCIA devices, introduced in 1995 and present in laptops from late 1997 onward. CardBus is effectively a 32-bit, 33 MHz PCI bus in the PC Card design. CardBus supports bus mastering, which allows a controller on the bus to talk to other devices or memory without going through the CPU. Many chipsets, such as those that support Wi-Fi, are available for both PCI and CardBus

https://en.wikipedia.org/wiki/PC_Card

- **AGP - Accelerated Graphics Port (1997)**

Accelerated Graphics Port (AGP) is a parallel expansion card standard, designed for attaching a video card to a computer system to assist in the acceleration of 3D computer graphics. It was originally designed as a successor to PCI-type connections for video cards. Since 2004, AGP was progressively phased out in favor of PCI Express (PCIe), which is serial, as opposed to parallel; by mid-2008, PCI Express cards dominated the market and only a few AGP models were available,[1] with GPU manufacturers and add-in board partners eventually dropping support for the interface in favor of PCI Express.

https://en.wikipedia.org/wiki/Accelerated_Graphics_Port

The AGP slot first appeared on x86-compatible system boards based on Socket 7 Intel P5 Pentium and Slot 1 P6 Pentium II processors. Intel introduced AGP support with the i440LX Slot 1 chipset on August 26, 1997, and a flood of products followed from all the major system board vendors.[3]
The first Socket 7 chipsets to support AGP were the VIA Apollo VP3, SiS 5591/5592, and the ALI Aladdin V. Intel never released an AGP-equipped Socket 7 chipset. FIC demonstrated the first Socket 7 AGP system board in November 1997 as the FIC PA-2012 based on the VIA Apollo VP3 chipset, followed very quickly by the EPoX P55-VP3 also based on the VIA VP3 chipset which was first to market.[4]
Early video chipsets featuring AGP support included the Rendition Vérité V2200, 3dfx Voodoo Banshee, Nvidia RIVA 128, 3Dlabs PERMEDIA 2, Intel i740, ATI Rage series, Matrox Millennium II, and S3 ViRGE GX/2. Some early AGP boards used graphics processors built around PCI and were simply bridged to AGP. This resulted in the cards benefiting little from the new bus, with the only improvement used being the 66 MHz bus clock, with its resulting doubled bandwidth over PCI, and bus exclusivity. Examples of such cards were the Voodoo Banshee, Vérité V2200, Millennium II, and S3 ViRGE GX/2. Intel's i740 was explicitly designed to exploit the new AGP feature set; in fact it was designed to texture only from AGP memory, making PCI versions of the board difficult to implement (local board RAM had to emulate AGP memory.)
Microsoft first introduced AGP support into Windows 95 OEM Service Release 2 (OSR2 version 1111 or 950B) via the USB SUPPLEMENT to OSR2 patch.[5] After applying the patch the Windows 95 system became Windows 95 version 4.00.950 B. The first Windows NT-based operating system to receive AGP support was Windows NT 4.0 with Service Pack 3, introduced in 1997. Linux support for AGP enhanced fast data transfers was first added in 1999 with the implementation of the AGPgart kernel module.

https://en.wikipedia.org/wiki/Accelerated_Graphics_Port

- **Apple iMac G3 - Legacy free PC (1998) - Apple**

In 1998, Apple's iMac G3 was introduced as the first widely known example of a legacy-free PC,[9][10][11] and drew much criticism for its lack of legacy peripherals such as a floppy drive and Apple Desktop Bus (ADB) connector; [12] However, its success popularizd USB ports.
Compaq released the iPaq desktop in 1999.

From November 1999 to July 2000, Dell's WebPC was an early less-successful Wintel legacy-free PC.
A legacy-free PC is a type of personal computer that lacks a floppy and/or optical disc drive, legacy ports, and an Industry Standard Architecture (ISA) bus (or sometimes, any internal expansion bus at all). According to Microsoft, "The basic goal for these requirements is that the operating system, devices, and end users cannot detect the presence of the following: ISA slots or devices; legacy floppy disk controller (FDC); and PS/2, serial, parallel, and game ports."[1] The legacy ports are usually replaced with Universal Serial Bus (USB) ports. A USB adapter may be used if an older device must be connected to a PC lacking these ports.[2] According to the 2001 edition of Microsoft's PC System Design Guide, a legacy-free PC must be able to boot from a USB device.[3]

https://en.wikipedia.org/wiki/Legacy-free_PC

- **iSCSI - Internet Small Computer Systems Interface (1998) - IBM / Cisco**

Internet Small Computer Systems Interface or iSCSI (/ˈaɪskʌzi/ (listen) EYE-skuz-ee) is an Internet Protocol-based storage networking standard for linking data storage facilities. iSCSI provides block-level access to storage devices by carrying SCSI commands over a TCP/IP network. iSCSI facilitates data transfers over intranets and to manage storage over long distances. It can be used to transmit data over local area networks (LANs), wide area networks (WANs), or the Internet and can enable location-independent data storage and retrieval.
The protocol allows clients (called initiators) to send SCSI commands (CDBs) to storage devices (targets) on remote servers. It is a storage area network (SAN) protocol, allowing organizations to consolidate storage into storage arrays while providing clients (such as database and web servers) with the illusion of locally attached SCSI disks.[1] It mainly competes with Fibre Channel, but unlike traditional Fibre Channel which usually requires dedicated cabling,[a] iSCSI can be run over long distances using existing network infrastructure.[2] iSCSI was pioneered by IBM and Cisco in 1998 and submitted as a draft standard in March 2000.[3]

https://en.wikipedia.org/wiki/ISCSI

- **EFI - BIOS replacement for servers, superseded by UEFI (1998) - Intel**

The original motivation for EFI came during early development of the first Intel–HP Itanium systems in the mid-1990s. BIOS limitations (such as 16-bit real mode, 1MB addressable memory space,[7] assembly language programming, and PC AT hardware) had become too restrictive for the larger server platforms Itanium was targeting.[8] The effort to address these concerns began in 1998 and was initially called Intel Boot Initiative.[9] It was later renamed to Extensible Firmware Interface (EFI).[10][11]

https://en.wikipedia.org/wiki/UEFI

- **Intel Xeon (1998) - Intel**

Xeon (/ˈziːɒn/ ZEE-on) is a brand of x86 microprocessors designed, manufactured, and marketed by Intel, targeted at the non-consumer workstation, server, and embedded system markets. It was introduced in June 1998. Xeon processors are based on the same architecture as regular desktop-grade CPUs, but have advanced features such as support for ECC memory, higher core counts, more PCI Express lanes, support for larger amounts of RAM, larger cache memory and extra provision for enterprise-grade reliability, availability and serviceability (RAS) features responsible for handling hardware exceptions through the Machine Check Architecture. They are often capable of safely continuing execution where a normal processor cannot due to these extra RAS features, depending on the type and severity of the machine-check exception (MCE). Some also support multi-socket systems with two, four, or eight sockets through use of the Quick Path Interconnect (QPI) bus.

https://en.wikipedia.org/wiki/Xeon

- **Bluetooth development : from short-link radio technology to Bluetooth (1989-2001) - Ericsson Mobile, IBM**
The development of the "short-link" radio technology, later named Bluetooth, was initiated in 1989 by Nils Rydbeck, CTO at Ericsson Mobile in Lund, Sweden. The purpose was to develop wireless headsets, according to two inventions by Johan Ullman, SE 8902098-6, issued 1989-06-12 and SE 9202239, issued 1992-07-24. Nils Rydbeck tasked Tord Wingren with specifying and Dutchman Jaap Haartsen and Sven Mattisson with developing.[17] Both were working for Ericsson in Lund.[18] Principal design and development began in 1994 and by 1997 the team had a workable solution.[19] From 1997 Örjan Johansson became the project leader and propelled the technology and standardization.[20][21][22][23]

The development of the "short-link" radio technology, later named Bluetooth, was initiated in 1989 by Nils Rydbeck, CTO at Ericsson Mobile in Lund, Sweden. The purpose was to develop wireless headsets, according to two inventions by Johan Ullman, SE 8902098-6, issued 1989-06-12 and SE 9202239, issued 1992-07-24. Nils Rydbeck tasked Tord Wingren with specifying and Dutchman Jaap Haartsen and Sven Mattisson with developing.[17] Both were working for Ericsson in Lund.[18] Principal design and development began in 1994 and by 1997 the team had a workable solution.[19] From 1997 Örjan Johansson became the project leader and propelled the technology and standardization.[20][21][22][23]

In 1997, Adalio Sanchez, then head of IBM ThinkPad product R&D, approached Nils Rydbeck about collaborating on integrating a mobile phone into a ThinkPad notebook. The two assigned engineers from Ericsson and IBM to study the idea. The conclusion was that power consumption on cellphone technology at that time was too high to allow viable integration into a notebook and still achieve adequate battery life. Instead, the two companies agreed to integrate Ericsson's short-link technology on both a ThinkPad notebook and an Ericsson phone to accomplish the goal. Since neither IBM ThinkPad notebooks nor Ericsson phones were the market share leaders in their respective markets at that time, Adalio Sanchez and Nils Rydbeck agreed to make the short-link technology an open industry standard to permit each player maximum market access. Ericsson contributed the short-link radio technology, and IBM contributed patents around the logical layer. Adalio Sanchez of IBM then recruited Stephen Nachtsheim of Intel to join and then Intel also recruited Toshiba and Nokia. In May 1998, the Bluetooth SIG was launched with IBM and Ericsson as the founding signatories and a total of five members: Ericsson, Intel, Nokia, Toshiba and IBM.

The first Bluetooth device was revealed in 1999. It was a hands-free mobile headset that earned the "Best of show Technology Award" at COMDEX. The first Bluetooth mobile phone was the Ericsson T36 but it was the revised T39 model that actually made it to store shelves in 2001. In parallel, IBM introduced the IBM ThinkPad A30 in October 2001 which was the first notebook with integrated Bluetooth.

https://en.wikipedia.org/wiki/Bluetooth


- **Intel 810 - Intel Hub Architecture (1999) - Intel**

Intel Hub Architecture (IHA) was Intel's architecture for the 8xx family of chipsets, starting in 1999 with the Intel 810. It uses a memory controller hub (MCH) that is connected to an I/O controller hub (ICH) via a 266 MB/s bus. The MCH chip supports memory and AGP (replaced by PCI Express in 9xx series chipsets), while the ICH chip provides connectivity for PCI (revision 2.2 before ICH5 series and revision 2.3 since ICH5 series), USB (version 1.1 before ICH4 series and version 2.0 since ICH4 series), sound (originally AC'97, Azalia added in ICH6 series), IDE hard disks (supplemented by Serial ATA since ICH5 series, fully replaced IDE since ICH8 series for desktops and ICH9 series for notebooks) and LAN (uncommonly activated on desktop motherboards and notebooks, usually independent LAN controller were placed instead of PHY chip).

https://en.wikipedia.org/wiki/Intel_Hub_Architecture

- **3D Now (1999) - AMD**

3DNow! is a deprecated extension to the x86 instruction set developed by Advanced Micro Devices (AMD). It adds single instruction multiple data (SIMD) instructions to the base x86 instruction set, enabling it to perform vector processing of floating-point vector-operations using Vector registers, which improves the performance of many graphic-intensive applications. The first microprocessor to implement 3DNow was the AMD K6-2, which was introduced in 1998. When the application was appropriate, this raised the speed by about 2–4 times.[1]

However, the instruction set never gained much popularity, and AMD announced on August 2010 that support for 3DNow would be dropped in future AMD processors, except for two instructions (the PREFETCH and PREFETCHW instructions).[2] The two instructions are also available in Bay-Trail Intel processors.[3]

3DNow was developed at a time when 3D graphics were becoming mainstream in PC multimedia and games. Realtime display of 3D graphics depended heavily on the host CPU's floating-point unit (FPU) to perform floating-point calculations, a task in which AMD's K6 processor was easily outperformed by its competitor, the Intel Pentium II.

As an enhancement to the MMX instruction set, the 3DNow instruction-set augmented the MMX SIMD registers to support common arithmetic operations (add/subtract/multiply) on single-precision (32-bit) floating-point data. Software written to use AMD's 3DNow instead of the slower x87 FPU could execute up to 4x faster, depending on the instruction-mix.

https://en.wikipedia.org/wiki/3DNow!

- **SSE - Streaming SIMD Extensions (1999) - Intel**

In computing, Streaming SIMD Extensions (SSE) is a single instruction, multiple data (SIMD) instruction set extension to the x86 architecture, designed by Intel and introduced in 1999 in their Pentium III series of Central processing units (CPUs) shortly after the appearance of Advanced Micro Devices (AMD's) 3DNow!. SSE contains 70 new instructions (65 unique mnemonics[1] using 70 encodings), most of which work on single precision floating-point data. SIMD instructions can greatly increase performance when exactly the same operations are to be performed on multiple data objects. Typical applications are digital signal processing and graphics processing.

Intel's first IA-32 SIMD effort was the MMX instruction set. MMX had two main problems: it re-used existing x87 floating-point registers making the CPUs unable to work on both floating-point and SIMD data at the same time, and it only worked on integers. SSE floating-point instructions operate on a new independent register set, the XMM registers, and adds a few integer instructions that work on MMX registers.

SSE was subsequently expanded by Intel to SSE2, SSE3, SSSE3 and SSE4. Because it supports floating-point math, it had wider applications than MMX and became more popular. The addition of integer support in SSE2 made MMX largely redundant, though further performance increases can be attained in some situations[when?] by using MMX in parallel with SSE operations.

SSE was originally called Katmai New Instructions (KNI), Katmai being the code name for the first Pentium III core revision. During the Katmai project Intel sought to distinguish it from their earlier product line, particularly their flagship Pentium II. It was later renamed Internet Streaming SIMD Extensions (ISSE[2]), then SSE. AMD eventually added support for SSE instructions, starting with its Athlon XP and Duron (Morgan core) processors.

https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions

- **Abit BP6 - multi-processor board for the consumer market (1999) - Abit**

The legendary Abit BP6, released in 1999 was the first dual-processor socket PPGA 370 motherboard and the first board to finally bring multi-processor boards at an affordable price to the consumer market. There were motherboards prior to the BP6 which featured more than one CPU on the board. Slot 1, Pentium Pro and even so far back as the 386 motherboards were available that could support multiple physical CPUs on board in what was known as SMP processing. Generally these setups were found in very high end and very expensive workstations and were more or less out of reach to the average consumer.

https://ancientelectronics.wordpress.com/2022/03/13/the-abit-bp6-motherboard-dual-cpu-processing-for-the-masses/


### Evolution of uses
#### Servers

- **httpd (1990) : First web server goes live** 

https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol

CERN httpd (later also known as W3C httpd) is an early, now discontinued, web server (HTTP) daemon originally developed at CERN from 1990 onwards by Tim Berners-Lee, Ari Luotonen[2] and Henrik Frystyk Nielsen.[1] Implemented in C, it was the first web server software.[3]

https://en.wikipedia.org/wiki/CERN_httpd

- **NSCA HTTPd (1993)**

NCSA HTTPd is an early, now discontinued, web server originally developed at the NCSA at the University of Illinois at Urbana–Champaign by Robert McCool and others.[1] First released in 1993, it was among the earliest web servers developed, following Tim Berners-Lee's CERN httpd, Tony Sanders' Plexus server, and some others

https://en.wikipedia.org/wiki/NCSA_HTTPd

- **Apache HTTP Server (1995): Most successful internet server**

Originally based on the NCSA HTTPd server, development of Apache began in early 1995 after work on the NCSA code stalled. Apache played a key role in the initial growth of the World Wide Web,[11] quickly overtaking NCSA HTTPd as the dominant HTTP server. In 2009, it became the first web server software to serve more than 100 million websites.[12]

https://en.wikipedia.org/wiki/Apache_HTTP_Server

#### Clients

##### GUI

- **Microsoft Windows 3.0 (1990) : Multi-tasked DOS**

improved the design, mostly because of virtual memory and loadable virtual device drivers (VxDs) that allow Windows to share arbitrary devices between **multi-tasked DOS applications**.

https://en.wikipedia.org/wiki/Microsoft_Windows

- **Microsoft Windows 3.11 (1993) : Windows for Workgroups - peer-to-peer networking features**

Windows 3.1, made generally available on March 1, 1992, featured a facelift. In August 1993, ***Windows for Workgroups, a special version with integrated peer-to-peer networking features*** and a version number of 3.11, was released.

https://en.wikipedia.org/wiki/Microsoft_Windows

- **Microsoft Windows 95 (1995) : 32 bit applications, plug and play hardware**

Windows 95 introduced support for native 32-bit applications, plug and play hardware, preemptive multitasking, long file names of up to 255 characters, and provided increased stability over its predecessors. Windows 95 also introduced a redesigned, object oriented user interface, replacing the previous Program Manager with the Start menu, taskbar, and Windows Explorer shell. Windows 95 was a major commercial success for Microsoft
https://en.wikipedia.org/wiki/Microsoft_Windows

- **Microsoft Windows 98 (1998) : USB support**

The release of Windows 98 on June 25, 1998, which introduced the Windows Driver Model, support for USB composite devices, support for ACPI, hibernation, and support for multi-monitor configuration**

https://en.wikipedia.org/wiki/Microsoft_Windows

Windows 98 added built-in support for USB Human Interface Device class (USB HID),[105] with native vertical scrolling support.[106] Windows 2000 and Windows Me expanded this built-in support to 5-button mice.[107]

https://en.wikipedia.org/wiki/Computer_mouse

- **VNC (1999): Remote desktop application** : 

The remote desktop application VNC (Virtual Network Computing) is made available by the AT&T Laboratories Cambridge. ORL, the Olivetti Research Laboratory was founded 12 years earlier, accquired in 1999 by AT&T to create AT&T; Laboratories Cambridge.

https://en.wikipedia.org/wiki/Virtual_Network_Computing

##### Web browsers

- **NSCA Mosaïc (1993)**: web browser developped at the end of 1992 at the american research center NCSA

https://en.wikipedia.org/wiki/Mosaic_(web_browser)

- **Netscape Navigator (1994): web browser**

https://en.wikipedia.org/wiki/Netscape

- **Microsoft Internet Explorer (1995): Most successful web browser**

Starting in 1995, It was first released as part of the add-on package Plus! for Windows 95 that year. Later versions were available as free downloads, or in-service packs, and included in the original equipment manufacturer (OEM) service releases of Windows 95 and later versions of Windows. 
Internet Explorer was once the most widely used web browser, attaining a peak of 95% usage share by 2003.[12] This came after Microsoft used bundling to win the first browser war against Netscape, which was the dominant browser in the 1990s

https://en.wikipedia.org/wiki/Internet_Explorer

##### Mobile

- **Early form RFID Patent (1997) - Innovision Research and Technology - Hasbro**

1997: Early form patented and first used in Star Wars character toys for Hasbro. The patent was originally held by Andrew White and Marc Borrett at Innovision Research and Technology (Patent WO9723060). The device allowed data communication between two units in close proximity.[17]

https://en.wikipedia.org/wiki/Near-field_communication

## Consumer electronics

### TV

- **Europe: HDTV launch - Italy (1990)**

Between 1988 and 1991, several European organizations were working on discrete cosine transform (DCT) based digital video coding standards for both SDTV and HDTV. The EU 256 project by the CMTT and ETSI, along with research by Italian broadcaster RAI, developed a DCT video codec that broadcast near-studio-quality HDTV transmission at about 70–140 Mbit/s.[23][40] The first HDTV transmissions in Europe, albeit not direct-to-home, began in 1990, when RAI broadcast the 1990 FIFA World Cup using several experimental HDTV technologies, including the digital DCT-based EU 256 codec,[23] 

https://en.wikipedia.org/wiki/High-definition_television

- **Europe : DVB - Digital Video Broadcasting (1993-1997) : Satelitte, Cable, Terrestrial**

DVB-S and DVB-C were ratified in 1994. DVB-T was ratified in early 1997. The first commercial DVB-T broadcasts were performed by the United Kingdom's Digital TV Group in late 1998. In 2003 Berlin, Germany was the first area to completely stop broadcasting analog TV signals. Most European countries are fully covered by digital television and many have switched off PAL/SECAM services.
Digital Video Broadcasting (DVB) is a set of international open standards for digital television. DVB standards are maintained by the DVB Project, an international industry consortium,[1] and are published by a Joint Technical Committee (JTC) of the European Telecommunications Standardisé Institute (ETSI), European Committee for Electrotechnical Standardization (CENELEC) and European Broadcasting Union (EBU).
Satellite: DVB-S, DVB-S2, and DVB-SH
DVB-SMATV for distribution via SMATV
Cable: DVB-C, DVB-C2

https://en.wikipedia.org/wiki/Digital_Video_Broadcasting

DVB-T, short for Digital Video Broadcasting — Terrestrial, is the DVB European-based consortium standard for the broadcast transmission of digital terrestrial television that was first published in 1997[1] and first broadcast in Singapore in February, 1998.[2][3][4][5][6][7][8] This system transmits compressed digital audio, digital video and other data in an MPEG transport stream, using coded orthogonal frequency-division multiplexing (COFDM or OFDM) modulation. It is also the format widely used worldwide (including North America) for Electronic News Gathering for transmission of video and audio from a mobile newsgathering vehicle to a central receive point.

https://en.wikipedia.org/wiki/DVB-T

- **HDTV deployment start (1994)**

HDTV technology was introduced in the United States in the early 1990s and made official in 1993 by the Digital HDTV Grand Alliance, a group of television, electronic equipment, communications companies consisting of AT&T Bell Labs, General Instrument, Philips, Sarnoff, Thomson, Zenith and the Massachusetts Institute of Technology. Field testing of HDTV at 199 sites in the United States was completed August 14, 1994.[34] The first public HDTV broadcast in the United States occurred on July 23, 1996, when the Raleigh, North Carolina television station WRAL-HD began broadcasting from the existing tower of WRAL-TV southeast of Raleigh, winning a race to be first with the HD Model Station in Washington, D.C., which began broadcasting July 31, 1996 with the callsign WHD-TV, based out of the facilities of NBC owned and operated station WRC-TV.[35][36][37] The American Advanced Television Systems Committee (ATSC) HDTV system had its public launch on October 29, 1998, during the live coverage of astronaut John Glenn's return mission to space on board the Space Shuttle Discovery.[38] The signal was transmitted coast-to-coast, and was seen by the public in science centers, and other public theaters specially equipped to receive and display the broadcast.[38][39]

https://en.wikipedia.org/wiki/High-definition_television

- **USA : ATSC HDTV Standards (1996)**

Advanced Television Systems Committee (ATSC) standards are an American set of standards for digital television transmission over terrestrial, cable and satellite networks. It is largely a replacement for the analog NTSC standard and, like that standard, is used mostly in the United States, Mexico, Canada, and South Korea. Several former NTSC users, in particular Japan, have not used ATSC during their digital television transition, because they adopted their own system called ISDB.
The ATSC standards were developed in the early 1990s by the Grand Alliance, a consortium of electronics and telecommunications companies that assembled to develop a specification for what is now known as HDTV.
The high-definition television standards defined by the ATSC produce widescreen 16:9 images up to 1920×1080 pixels in size – more than six times the display resolution of the earlier standard. However, many different image sizes are also supported. The reduced bandwidth requirements of lower-resolution images allow up to six standard-definition "subchannels" to be broadcast on a single 6 MHz TV channel.
ATSC standards are marked A/x (x is the standard number) and can be downloaded for free from the ATSC's website at ATSC.org. ATSC Standard A/53, which implemented the system developed by the Grand Alliance, was published in 1995; the standard was adopted by the Federal Communications Commission in the United States in 1996. It was revised in 2009. ATSC Standard A/72 was approved in 2008 and introduces H.264/AVC video coding to the ATSC system.
ATSC supports 5.1-channel surround sound using Dolby Digital's AC-3 format. Numerous auxiliary datacasting services can also be provided.
Many aspects of ATSC are patented, including elements of the MPEG video coding, the AC-3 audio coding, and the 8VSB modulation.[2] The cost of patent licensing, estimated at up to $50 per digital TV receiver,[3] had prompted complaints by manufacturers.[4]
Companies with patents included : 
LG Electronics, Zenith Electronics, Panasonic, Samsung Electronics, Columbia University, Mitsubishi Electric, JVC Kenwood, Cisco Technology, Inc., Vientos Alisios Co., Ltd., Philips

https://en.wikipedia.org/wiki/ATSC_standards

- **Satelitte : Medium power satellites with smaller dishes - PrimeStar**

In the US in the early 1990s, four large cable companies launched PrimeStar, a direct broadcasting company using medium power satellites. The relatively strong transmissions allowed the use of smaller (90 cm) dishes. Its popularity declined with the 1994 launch of the Hughes DirecTV and Dish Network satellite television systems.
Digital satellite broadcasts began in 1994 in the United States through DirecTV using the DSS format. They were launched (with the DVB-S standard) in South Africa, Middle East, North Africa and Asia-Pacific in 1994 and 1995, and in 1996 and 1997 in European countries including France, Germany, Spain, Portugal, Italy and the Netherlands, as well as Japan, North America and Latin America. Digital DVB-S broadcasts in the United Kingdom and Ireland started in 1998. Japan started broadcasting with the ISDB-S standard in 2000.
On March 4, 1996, EchoStar introduced Digital Sky Highway (Dish Network) using the EchoStar 1 satellite.[80] EchoStar launched a second satellite in September 1996 to increase the number of channels available on Dish Network to 170.[80] These systems provided better pictures and stereo sound on 150–200 video and audio channels, and allowed small dishes to be used. This greatly reduced the popularity of TVRO systems. In the mid-1990s, channels began moving their broadcasts to digital television transmission using the DigiCipher conditional access system.[81]

https://en.wikipedia.org/wiki/Satellite_television

### Video games

- **Renaissance of arcade (1990-1996)**
 
Fighting games like Street Fighter II (1991) and Mortal Kombat (1992) helped to revive it in the early 1990s, leading to a renaissance for the arcade industry.[22] 3D graphics were popularized in arcades during the early 1990s with games such as Sega's Virtua Racing and Virtua Fighter,[59] with later arcade systems such as the Sega Model 3 remaining considerably more advanced than home systems through the late 1990s.[60][61] However, the improved capabilities of home consoles and computers to mimic arcade video games during this time drew crowds away from arcades.[22]

https://en.wikipedia.org/wiki/Arcade_game#History

- **SNES - Super Nitendo (1990) - Nintendo - ROM, Cartridge**

The Super Nintendo Entertainment System (SNES),[b] commonly shortened to Super NES or Super Nintendo,[c] is a 16-bit home video game console developed by Nintendo that was released in 1990 in Japan and South Korea,[19] 1991 in North America, 1992 in Europe and Oceania, and 1993 in South America. In Japan, it is called the Super Famicom (SFC).[d] In South Korea, it is called the Super Comboy[e] and was distributed by Hyundai Electronics.[20] The system was released in Brazil on August 30, 1993,[19][21] by Playtronic. Although each version is essentially the same, several forms of regional lockout prevent cartridges for one version from being used in other versions.
The Super NES is Nintendo's second programmable home console, following the Nintendo Entertainment System (NES). The console introduced advanced graphics and sound capabilities compared with other systems at the time. It was designed to accommodate the ongoing development of a variety of enhancement chips integrated into game cartridges to be competitive into the next generation.

https://en.wikipedia.org/wiki/Super_Nintendo_Entertainment_System

- **Consoles surpasses arcade (1997-1998)**

Up until about 1996, arcade video games had remained the largest sector of the global video game industry, before arcades declined in the late 1990s, with the console market surpassing arcade video games for the first time around 1997–1998.[62

https://en.wikipedia.org/wiki/Arcade_game#History

- **32/64 bits game consoles - Fifth generation video games consoles (1993-2006)**

The fifth-generation era (also known as the 32-bit era, the 64-bit era, or the 3D era) refers to computer and video games, video game consoles, and handheld gaming consoles dating from approximately October 4, 1993 to March 23, 2006.[note 1] For home consoles, the best-selling console was the **Sony PlayStation**, followed by the **Nintendo 64**, and then the **Sega Saturn**. The PlayStation also had a redesigned version, the PSone, which was launched on July 7, 2000.

Some features that distinguished fifth generation consoles from previous fourth generation consoles include:
- 3D polygon graphics with texture mapping
- 3D graphics capabilities – lighting, Gouraud shading, anti-aliasing and texture filtering
- Optical disc (CD-ROM) game storage, allowing much larger storage space (up to 650 MB) than ROM cartridges
- CD quality audio recordings (music and speech) – PCM audio with 16-bit depth and 44.1 kHz sampling rate
- Wide adoption of full motion video, displaying pre-rendered computer animation or live action footage
- Analog controllers
- Display resolutions from 480i/480p to 576i
- Color depth up to 16,777,216 colors (24-bit true color)
- 
https://en.wikipedia.org/wiki/Fifth_generation_of_video_game_consoles

- **Sony PlayStation - First console to ship over 100 million units (1994) - Sony**

PlayStation (Japanese: プレイステーション, Hepburn: Pureisutēshon, officially abbreviated as PS) is a video game brand that consists of five home video game consoles, two handhelds, a media center, and a smartphone, as well as an online service and multiple magazines. The brand is produced by Sony Interactive Entertainment, a division of Sony; the first PlayStation console was released in Japan in December 1994, and worldwide the following year.[1]
The original console in the series was the first console of any type to ship over 100 million units, doing so in under a decade.[2]

https://en.wikipedia.org/wiki/PlayStation

Sony began developing the standalone PlayStation after a failed venture with Nintendo to create a CD-ROM peripheral for the Super Nintendo Entertainment System in the early 1990s. The console was primarily designed by Ken Kutaragi and Sony Computer Entertainment in Japan, while additional development was outsourced in the United Kingdom. An emphasis on 3D polygon graphics was placed at the forefront of the console's design. PlayStation game production was designed to be streamlined and inclusive, enticing the support of many third-party developers.

The console proved popular for its extensive game library, popular franchises, low retail price, and aggressive youth marketing which advertised it as the preferable console for adolescents and adults. Premier PlayStation franchises included Gran Turismo, Crash Bandicoot, Tomb Raider, and Final Fantasy, all of which spawned numerous sequels. PlayStation games continued to sell until Sony ceased production of the PlayStation and its games on 23 March 2006—over eleven years after it had been released, and less than a year before the debut of the PlayStation 3.[8] A total of 3,061 PlayStation games were released, with cumulative sales of 967 million units.

SCE was an upstart in the video game industry in late 1994, as the video game market in the early 1990s was dominated by Nintendo and Sega. Nintendo had been the clear leader in the industry since the introduction of the Nintendo Entertainment System in 1985 and the Nintendo 64 was initially expected to maintain this position. The PlayStation's target audience included the generation which was the first to grow up with mainstream video games, along with 18- to 29-year-olds who were not the primary focus of Nintendo.[202] By the late 1990s, Sony became a highly regarded console brand due to the PlayStation, with a significant lead over second-place Nintendo, while Sega was relegated to a distant third.[203]

The PlayStation became the first "computer entertainment platform" to ship over 100 million units worldwide,[6][204] with many critics attributing the console's success to third-party developers.[76] It remains the fifth best-selling console of all time as of 2022, with a total of 102.49 million units sold.[204] Around 7,900 individual games were published for the console during its 11-year life span, the second-most amount of games ever produced for a console.[6] Its success resulted in a significant financial boon for Sony as profits from its video game division contributed to 23%.[205]

The success of the PlayStation contributed to the demise of cartridge-based home consoles. While not the first system to use an optical disc format, it was the first highly successful one, and ended up going head-to-head with the proprietary cartridge-relying Nintendo 64.[c][207] After the demise of the Sega Saturn, Nintendo was left as Sony's main competitor in Western markets. Nintendo chose not to use CDs for the Nintendo 64; it was likely concerned with the proprietary cartridge format's ability to help enforce copy protection, given its substantial reliance on licensing and exclusive games for its revenue.

https://en.wikipedia.org/wiki/PlayStation_(console)

### Automation
- **LonWork (1999)** 

LonWorks or Local Operating Network is an open standard (ISO/IEC 14908) for networking platforms specifically created to address the needs of control applications. The platform is built on a protocol created by Echelon Corporation for networking devices over media such as twisted pair, powerlines, fibre optics , and RF. It is used for the automation of various functions within buildings such as lighting and HVAC; see building automation.
The technology has its origins with chip designs, power line and twisted pair, signaling technology, routers, network management software, and other products from Echelon Corporation. In 1999 the communications protocol (then known as LonTalk) was submitted to ANSI and accepted as a standard for control networking (ANSI/CEA-709.1-B). Echelon's power line and twisted pair signaling technology was also submitted to ANSI for standardization and accepted. Since then, ANSI/CEA-709.1 has been accepted as the basis for IEEE 1473-L (in-train controls), AAR electro-pneumatic braking systems for freight trains, IFSF (European petrol station control), SEMI (semiconductor equipment manufacturing), and in 2005 as EN 14908 (European building automation standard). The protocol is also one of several data link/physical layers of the BACnet ASHRAE/ANSI standard for building automation.
China ratified the technology as a national controls standard, GB/Z 20177.1-2006 and as a building and intelligent community standard, GB/T 20299.4-2006; and in 2007 CECED, the European Committee of Domestic Equipment Manufacturers, adopted the protocol as part of its Household Appliances Control and Monitoring – Application Interworking Specification (AIS) standards.
During 2008 ISO and IEC have granted the communications protocol, twisted pair signaling technology, power line signaling technology, and Internet Protocol (IP) compatibility standard numbers ISO/IEC 14908-1, -2, -3, and -4.[1]

https://en.wikipedia.org/wiki/LonWorks

## Some standards, languages and protocols

### Network layer

- **IPv6 - Internet Protocol version 6 (1995)**

most recent version of the Internet Protocol (IP), the communications protocol that provides an identification and location system for computers on networks and routes traffic across the Internet. 
Introduced in 1995, IPv6 was developed by the Internet Engineering Task Force (IETF) to deal with the long-anticipated problem of IPv4 address exhaustion, and is intended to replace IPv4.
In December 1998, IPv6 became a Draft Standard for the IETF, which subsequently ratified it as an Internet Standard on 14 July 2017.
Deployment was slowed by the widespread use of NAT (1999).

https://en.wikipedia.org/wiki/IPv6

- **NAT - Network address translation (1999)** : a method of mapping an IP address space into another by modifying network address information in the IP header of packets while they are in transit across a traffic routing device.
Popular with the exhaustion of IPv4 addresses and home routers. Essential to networking. 
https://en.wikipedia.org/wiki/Network_address_translation

### Transport layer

- **SSL - Secure Socket Layer (1995-1998)**

Netscape developed the original SSL protocols, and Taher Elgamal, chief scientist at Netscape Communications from 1995 to 1998. Superseded by TLS.

https://en.wikipedia.org/wiki/Transport_Layer_Security

- **TLS - Transport Layer Security (1999)** 

A cryptographic protocol designed to provide communications security over a computer network. The protocol is widely used 
in applications such as email, instant messaging, and voice over IP, but its use in securing HTTPS remains the most publicly visible.
Essential to networking and the internet.
https://en.wikipedia.org/wiki/Transport_Layer_Security

### Application layer

- **HTTP/0.9 - Hypertext Transfer Protocol (1991)**

application layer protocol in the Internet protocol suite model for distributed, collaborative, hypermedia information systems. Essential to the internet.

https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol

- **DHCP - Dynamic Host Configuration Protocol (1991)** 

A network management protocol used on Internet Protocol (IP) networks for automatically assigning IP addresses and other communication parameters to devices connected to the network using a client–server architecture.
Essential to networking.

https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol

- **RADIUS (1991)**

Remote Authentication Dial-In User Service (RADIUS) is a networking protocol that provides centralized authentication, authorization, and accounting (AAA) management for users who connect and use a network service. RADIUS was developed by Livingston Enterprises in 1991 as an access server authentication and accounting protocol. It was later brought into IEEE 802 and IETF standards.

RADIUS is a client/server protocol that runs in the application layer, and can use either TCP or UDP. Network access servers, which control access to a network, usually contain a RADIUS client component that communicates with the RADIUS server.[1] RADIUS is often the back-end of choice for 802.1X authentication.[2] A RADIUS server is usually a background process running on UNIX or Microsoft Windows.[1]
As more dial-up customers used the NSFNET a request for proposal was sent out by Merit Network in 1991 to consolidate their various proprietary authentication, authorization and accounting systems. Among the early respondents was Livingston Enterprises and an early version of the RADIUS was written after a meeting. The early RADIUS server was installed on a UNIX operating system. Livingston Enterprises was acquired by Lucent and together with Merit steps were taken to gain industry acceptance for RADIUS as a protocol. Both companies offered a RADIUS server at no charge.[11] In 1997 RADIUS was published as RFC 2058 and RFC 2059, current versions are RFC 2865 and RFC 2866.[12]

https://en.wikipedia.org/wiki/RADIUS

- **LDAP (1993)**

The Lightweight Directory Access Protocol (LDAP /ˈɛldæp/) is an open, vendor-neutral, industry standard application protocol for accessing and maintaining distributed directory information services over an Internet Protocol (IP) network.[1] Directory services play an important role in developing intranet and Internet applications by allowing the sharing of information about users, systems, networks, services, and applications throughout the network.[2] As examples, directory services may provide any organized set of records, often with a hierarchical structure, such as a corporate email directory. Similarly, a telephone directory is a list of subscribers with an address and a phone number.
LDAP is specified in a series of Internet Engineering Task Force (IETF) Standard Track publications called Request for Comments (RFCs), using the description language ASN.1. The latest specification is Version 3, published as RFC 4511[3] (a road map to the technical specifications is provided by RFC4510).
A common use of LDAP is to provide a central place to store usernames and passwords. This allows many different applications and services to connect to the LDAP server to validate users.[4]
LDAP is based on a simpler subset of the standards contained within the X.500 standard. Because of this relationship, LDAP is sometimes called X.500-lite.[5]
The protocol was originally created[7] by Tim Howes of the University of Michigan, Steve Kille of Isode Limited, Colin Robbins of Nexor and Wengyik Yeong of Performance Systems International, circa 1993, as a successor[8] to DIXIE and DAS. Mark Wahl of Critical Angle Inc., Tim Howes, and Steve Kille started work in 1996 on a new version of LDAP, LDAPv3, under the aegis of the Internet Engineering Task Force (IETF). LDAPv3, first published in 1997, superseded LDAPv2 and added support for extensibility, integrated the Simple Authentication and Security Layer, and better aligned the protocol to the 1993 edition of X.500. Further development of the LDAPv3 specifications themselves and of numerous extensions adding features to LDAPv3 has come through the IETF.

https://en.wikipedia.org/wiki/Lightweight_Directory_Access_Protocol

### Data transmission

#### Text

- **HTML (1990-1991) - HyperText Markup Language**

In 1980, physicist Tim Berners-Lee, a contractor at CERN, proposed and prototyped ENQUIRE, a system for CERN researchers to use and share documents. In 1989, Berners-Lee wrote a memo proposing an Internet-based hypertext system.[3] Berners-Lee specified HTML and wrote the browser and server software in late 1990. 
The first publicly available description of HTML was a document called "HTML Tags", first mentioned on the Internet by Tim Berners-Lee in late 1991.

https://en.wikipedia.org/wiki/HTML

- **URL (1992-1994) - Uniform Resource Locators**

Uniform Resource Locators were defined in RFC 1738 in 1994 by Tim Berners-Lee, the inventor of the World Wide Web, and the URI working group of the Internet Engineering Task Force (IETF),[7] as an outcome of collaboration started at the IETF Living Documents birds of a feather session in 1992.[7][8]

https://en.wikipedia.org/wiki/URL

- **XML (1998) - Extensible Markup Language**

A markup language and file format for storing, transmitting, and reconstructing arbitrary data. It defines a set of rules for encoding documents in a format that is both human-readable and machine-readable. The World Wide Web Consortium's XML 1.0 Specification[2] of 1998[3] and several other related specifications[4]—all of them free open standards—define XML.[5]

https://en.wikipedia.org/wiki/XML

#### Images

- **WMF - Windows Metafile, SVG ancestor (1991) - Microsoft**

Long before the invention of Scalable Vector Graphics, Microsoft Corporation recognized the value of recording images in a format that its applications and operating systems could easily render irrespective of the output device.  With the release of Windows 3.0, Microsoft released its Windows Metafile (WMF) format, which can contain vector and raster graphics in one package.  

https://datatracker.ietf.org/doc/html/rfc7903

- **JPEG (1992)**

Picture compression format.

https://en.wikipedia.org/wiki/JPEG

- **PNG (1997)** 

Portable Network Graphics (PNG, officially pronounced /pɪŋ/[2][3] PING, colloquially pronounced /ˌpiːɛnˈdʒiː/[4] PEE-en-JEE) is a raster-graphics file format that supports lossless data compression.
PNG was published as informational RFC 2083 in March 1997 and as an ISO/IEC 15948 standard in 2004

https://en.wikipedia.org/wiki/Portable_Network_Graphics

#### Music and Video

- **RIFF - Resource Interchange File Format (1991) - Microsoft and IBM**

The Resource Interchange File Format (RIFF) is a generic file container format for storing data in tagged chunks.[2] It is primarily used to store multimedia such as sound and video, though it may also be used to store any arbitrary data.[3]
The Microsoft implementation is mostly known through container formats like AVI, ANI and WAV, which use RIFF as their basis.[4]
RIFF was introduced in 1991 by Microsoft and IBM, and was presented by Microsoft as the default format for Windows 3.1 multimedia files. It is based on Electronic Arts' Interchange File Format, introduced in 1985 on the Commodore Amiga, the only difference being that multi-byte integers are in little-endian format, native to the 80x86 processor series used in IBM PCs, rather than the big-endian format native to the 68k processor series used in Amiga and Apple Macintosh computers, where IFF files were heavily used. A RIFX format, which is big-endian, was also introduced.

https://en.wikipedia.org/wiki/Resource_Interchange_File_Format

- **WAV - Uncompressed sound format (1991) - Microsoft and IBM**

Waveform Audio File Format[3] (WAVE,[3] or WAV due to its filename extension;[3][6][7] pronounced "wave"[8]) is an audio file format standard, developed by IBM and Microsoft, for storing an audio bitstream on PCs. It is the main format used on Microsoft Windows systems for uncompressed audio. The usual bitstream encoding is the linear pulse-code modulation (LPCM) format.
WAV is an application of the Resource Interchange File Format (RIFF) bitstream format method for storing data in chunks, and thus is similar to the 8SVX and the AIFF format used on Amiga and Macintosh computers, respectively.
August 1991

https://en.wikipedia.org/wiki/WAV

- **AVI - Audio Video Interleave (1992) - Microsoft**

Audio Video Interleave (also Audio Video Interleaved and known by its initials and filename extension AVI, usually pronounced /ˌeɪ.viːˈaɪ/[3]), is a proprietary multimedia container format and Windows standard[4] introduced by Microsoft in November 1992 as part of its Video for Windows software. AVI files can contain both audio and video data in a file container that allows synchronous audio-with-video playback. Like the DVD video format, AVI files support multiple streaming audio and video, although these features are seldom used.

https://en.wikipedia.org/wiki/Audio_Video_Interleave

- **MP3 (1993)**

MP3 (formally MPEG-1 Audio Layer III or MPEG-2 Audio Layer III)[4] is a coding format for digital audio developed largely by the Fraunhofer Society in Germany, with support from other digital scientists in the United States and elsewhere. Originally defined as the third audio format of the MPEG-1 standard, it was retained and further extended — defining additional bit-rates and support for more audio channels — as the third audio format of the subsequent MPEG-2 standard. A third version, known as MPEG 2.5 — extended to better support lower bit rates — is commonly implemented, but is not a recognized standard.
On 7 July 1994, the Fraunhofer Society released the first software MP3 encoder, called l3enc.[59] The filename extension .mp3 was chosen by the Fraunhofer team on 14 July 1995 (previously, the files had been named .bit).[1] With the first real-time software MP3 player WinPlay3 (released 9 September 1995) many people were able to encode and play back MP3 files on their PCs. Because of the relatively small hard drives of the era (≈500–1000 MB) lossy compression was essential to store multiple albums' worth of music on a home computer as full recordings (as opposed to MIDI notation, or tracker files which combined notation with short recordings of instruments playing single notes).

https://en.wikipedia.org/wiki/MP3

#### 3D

- **OpenGL (1991) - Silicon Graphics**

OpenGL (Open Graphics Library[3]) is a cross-language, cross-platform application programming interface (API) for rendering 2D and 3D vector graphics. The API is typically used to interact with a graphics processing unit (GPU), to achieve hardware-accelerated rendering.

Silicon Graphics, Inc. (SGI) began developing OpenGL in 1991 and released it on June 30, 1992;[4][5] applications use it extensively in the fields of computer-aided design (CAD), virtual reality, scientific visualization, information visualization, flight simulation, and video games. Since 2006, OpenGL has been managed by the non-profit technology consortium Khronos Group.[6]

https://en.wikipedia.org/wiki/OpenGL

- **DirectX (1995)**

Microsoft DirectX is a collection of application programming interfaces (APIs) for handling tasks related to multimedia, especially game programming and video, on Microsoft platforms. Originally, the names of these APIs all began with "Direct", such as Direct3D, DirectDraw, DirectMusic, DirectPlay, DirectSound, and so forth. The name DirectX was coined as a shorthand term for all of these APIs (the X standing in for the particular API names) and soon became the name of the collection. When Microsoft later set out to develop a gaming console, the X was used as the basis of the name Xbox to indicate that the console was based on DirectX technology.[3] The X initial has been carried forward in the naming of APIs designed for the Xbox such as XInput and the Cross-platform Audio Creation Tool (XACT), while the DirectX pattern has been continued for Windows APIs such as Direct2D and DirectWrite.

Direct3D (the 3D graphics API within DirectX) is widely used in the development of video games for Microsoft Windows and the Xbox line of consoles. Direct3D is also used by other software applications for visualization and graphics tasks such as CAD/CAM engineering. As Direct3D is the most widely publicized component of DirectX, it is common to see the names "DirectX" and "Direct3D" used interchangeably.

The DirectX software development kit (SDK) consists of runtime libraries in redistributable binary form, along with accompanying documentation and headers for use in coding. Originally, the runtimes were only installed by games or explicitly by the user. Windows 95 did not launch with DirectX, but DirectX was included with Windows 95 OEM Service Release 2.[4] Windows 98 and Windows NT 4.0 both shipped with DirectX, as has every version of Windows released since. The SDK is available as a free download. While the runtimes are proprietary, closed-source software, source code is provided for most of the SDK samples. Starting with the release of Windows 8 Developer Preview, DirectX SDK has been integrated into Windows SDK.[5]

https://en.wikipedia.org/wiki/DirectX

- **Direct3D (1996) - Microsoft**

Direct3D is a graphics application programming interface (API) for Microsoft Windows. Part of DirectX, Direct3D is used to render three-dimensional graphics in applications where performance is important, such as games. Direct3D uses hardware acceleration if it is available on the graphics card, allowing for hardware acceleration of the entire 3D rendering pipeline or even only partial acceleration. Direct3D exposes the advanced graphics capabilities of 3D graphics hardware, including Z-buffering,[1] W-buffering,[2] stencil buffering, spatial anti-aliasing, alpha blending, color blending, mipmapping, texture blending,[3][4] clipping, culling, atmospheric effects, perspective-correct texture mapping, programmable HLSL shaders[5] and effects.[6] Integration with other DirectX technologies enables Direct3D to deliver such features as video mapping, hardware 3D rendering in 2D overlay planes, and even sprites, providing the use of 2D and 3D graphics in interactive media ties.

https://en.wikipedia.org/wiki/Direct3D

- **Unreal Engine (1998) - Unreal**

Unreal Engine (UE) is a 3D computer graphics game engine developed by Epic Games, first showcased in the 1998 first-person shooter game Unreal. Initially developed for PC first-person shooters, it has since been used in a variety of genres of games and has seen adoption by other industries, most notably the film and television industry. Written in C++, the Unreal Engine features a high degree of portability, supporting a wide range of desktop, mobile, console and virtual reality platforms.

https://en.wikipedia.org/wiki/Unreal_Engine

#### File compression

- **DEFLATE (1993-1996)**

In computing, Deflate (stylized as DEFLATE) is a lossless data compression file format that uses a combination of LZ77 and Huffman coding. It was designed by Phil Katz, for version 2 of his PKZIP archiving tool. Deflate was later specified in RFC 1951 (1996).[1]

Katz also designed the original algorithm used to construct Deflate streams. This algorithm was patented as U.S. Patent 5,051,745, and assigned to PKWARE, Inc.[2][3] As stated in the RFC document, an algorithm producing Deflate files was widely thought to be implementable in a manner not covered by patents.[1] This led to its widespread use – for example, in gzip compressed files and PNG image files, in addition to the ZIP file format for which Katz originally designed it. The patent has since expired.

https://en.wikipedia.org/wiki/Deflate

- **Winzip - Popular ZIP Gui on Windows (1991)** 

WinZip is a trialware file archiver and compressor for Windows, macOS, iOS and Android. It is developed by WinZip Computing (formerly Nico Mak Computing), which is owned by Corel Corporation. The program can create archives in Zip file format, unpack some other archive file formats and it also has various tools for system integration.

WinZip 1.0 was released in April 1991 as a Graphical User Interface (GUI) front-end for PKZI.

https://en.wikipedia.org/wiki/WinZip


- **ARJ (1993)**

ARJ (Archived by Robert Jung) is a software tool designed by Robert K. Jung for creating high-efficiency compressed file archives. ARJ is currently on version 2.86 for MS-DOS and 3.20 for Microsoft Windows and supports 16-bit, 32-bit and 64-bit Intel architectures.[1]

ARJ was one of many file compression utilities for MS-DOS and Microsoft Windows during the early and mid-1990s. Parts of ARJ were covered by U.S. Patent 5,140,321 (expired). ARJ is well-documented and includes over 150 command line switches.

https://en.wikipedia.org/wiki/ARJ

- **RAR (1993)**

RAR is a proprietary archive file format that supports data compression, error correction and file spanning.[3] It was developed in 1993 by Russian software engineer Eugene Roshal and the software is licensed by win.rar GmbH.[3] The name RAR stands for Roshal Archive.

https://en.wikipedia.org/wiki/RAR_(file_format)

- **7ZIP - 7z file format, ZIP improvement (1999) - Igor Pavlov**

7-Zip is a free and open-source file archiver, a utility used to place groups of files within compressed containers known as "archives". It is developed by Igor Pavlov and was first released in 1999.[2] 7-Zip has its own archive format called 7z, but can read and write several others.

By default, 7-Zip creates 7z-format archives with a .7z file extension.

In 2011, TopTenReviews found that the 7z compression was at least 17% better than ZIP,[17] and 7-Zip's own site has since 2002 reported that while compression ratio results are very dependent upon the data used for the tests, "Usually, 7-Zip compresses to 7z format 30–70% better than to zip format, and 7-Zip compresses to zip format 2–10% better than most other zip-compatible programs."[18

https://en.wikipedia.org/wiki/7-Zip

#### Voice over IP

- **SIP - Session Initiation Protocol (1996)** 

Signaling protocol used for initiating, maintaining, and terminating communication sessions that include voice, video and messaging applications. 
SIP is used in **Internet telephony**, in private IP telephone systems, as well as mobile phone calling over LTE (**VoLTE**). Essential to telecommunications.

https://en.wikipedia.org/wiki/Session_Initiation_Protocol

#### Video over IP

- **MPEG-1 (1993)**

Since 1972, International Telecommunication Union's radio telecommunications sector (ITU-R) had been working on creating a global recommendation for Analog HDTV. These recommendations, however, did not fit in the broadcasting bands which could reach home users. The standardization of MPEG-1 in 1993 led to the acceptance of recommendations ITU-R BT.709

https://en.wikipedia.org/wiki/High-definition_television

- **MPEG-2 (1995)**

MPEG-2 evolved out of the shortcomings of MPEG-1.
MPEG-1's known weaknesses:
An audio compression system limited to two channels (stereo).
No standardized support for interlaced video with poor compression when used for interlaced video
Only one standardized "profile" (Constrained Parameters Bitstream), which was unsuited for higher resolution video. MPEG-1 could support 4k video but there was no easy way to encode video for higher resolutions, and identify hardware capable of supporting it, as the limitations of such hardware were not defined.
Support for only one chroma subsampling, 4:2:0.
Sakae Okubo of NTT was the ITU-T coordinator for developing the H.262/MPEG-2 Part 2 video coding standard and the requirements chairman in MPEG for the MPEG-2 set of standards.[27] The majority of patents underlying MPEG-2 technology are owned by three companies: Sony (311 patents), Thomson (198 patents) and Mitsubishi Electric (119 patents).[28] Hyundai Electronics (now SK Hynix) developed the first MPEG-2 SAVI (System/Audio/Video) decoder in 1995.[29]

https://en.wikipedia.org/wiki/MPEG-2

- **H.323 (1996)** 

Recommendation from the ITU Telecommunication Standardization Sector (ITU-T) that defines the protocols to provide audio-visual communication sessions on any packet network.[1] The H.323 standard addresses call signaling and control, multimedia transport and control, and bandwidth control for point-to-point and multi-point conferences.[2]
It is widely implemented[3] by **voice and videoconferencing equipment manufacturers**, is used within various **Internet real-time applications** such as GnuGK and NetMeeting and is widely deployed worldwide by service providers and enterprises for both voice and video services over IP networks.

https://en.wikipedia.org/wiki/H.323

- **H.263 (1996)**

The H.263 standard was first designed to be utilized in H.324 based systems (PSTN and other **circuit-switched network videoconferencing and videotelephony)**, but it also found use in H.323 (RTP/IP-based videoconferencing), H.320 (ISDN-based videoconferencing, where it became the most widely used video compression standard),[4] RTSP (streaming media) and SIP (IP-based videoconferencing) solutions.

https://en.wikipedia.org/wiki/H.263

#### Instant messaging

- **XMPP (1999)**
 
Open communication protocol designed for **instant messaging (IM), presence information, and contact list maintenance**. enables the near-real-time exchange of structured data between two or more network entities. Widely in use.

https://en.wikipedia.org/wiki/XMPP

#### Machine-to-machine
- **MQTT (1999)**

Lightweight, **publish-subscribe, machine to machine network protocol**. It is designed for connections with remote locations that have devices with resource constraints or limited network bandwidth. Widely in use.

https://en.wikipedia.org/wiki/MQTT

#### Files
- **Rsync (1999)**

Utility for **efficiently transferring and synchronizing files** between a computer and a storage drive and across networked computers by comparing the modification times and sizes of files. Detailed in the creator's PhD Thesis.

https://en.wikipedia.org/wiki/Rsync

#### Remote access
- **SSH - Secure Shell (1995): remote login and command-line execution**

The Secure Shell Protocol (SSH) is a cryptographic network protocol for operating network services securely over an unsecured network.[1] Its most notable applications are remote login and command-line execution.
SSH applications are based on a client–server architecture, connecting an SSH client instance with an SSH server
SSH was first designed in 1995 by Finnish computer scientist Tatu Ylönen. Subsequent development of the protocol suite proceeded in several developer groups, producing several variants of implementation. The protocol specification distinguishes two major versions, referred to as SSH-1 and SSH-2. The most commonly implemented software stack is **OpenSSH**, released in **1999** as open-source software by the OpenBSD developers. 

https://en.wikipedia.org/wiki/Secure_Shell

- **VNC - Virtual Network Computing (1999)** 

**Remote desktop protocol**. Still widely used. Insecure (not encrypted by default).

https://en.wikipedia.org/wiki/Virtual_Network_Computing

## Navigation

**GPS is operational, military use, miniaturization and move to dual-use**

The Gulf War from 1990 to 1991 was the first conflict in which the military widely used GPS.[58]

In 1991, a project to create a miniature GPS receiver successfully ended, replacing the previous 16 kg (35 lb) military receivers with a 1.25 kg (2.8 lb) handheld receiver.[26]

In 1992, the 2nd Space Wing, which originally managed the system, was inactivated and replaced by the 50th Space Wing.

By December 1993, GPS achieved initial operational capability (IOC), with a full constellation (24 satellites) available and providing the Standard Positioning Service (SPS).[59]

Full Operational Capability (FOC) was declared by Air Force Space Command (AFSPC) in April 1995, signifying full availability of the military's secure Precise Positioning Service (PPS).[59]

In 1996, recognizing the importance of GPS to civilian users as well as military users, U.S. President Bill Clinton issued a policy directive[60] declaring GPS a dual-use system and establishing an Interagency GPS Executive Board to manage it as a national asset.

In 1998, United States Vice President Al Gore announced plans to upgrade GPS with two new civilian signals for enhanced user accuracy and reliability, particularly with respect to aviation safety, and in 2000 the United States Congress authorized the effort, referring to it as GPS III.

