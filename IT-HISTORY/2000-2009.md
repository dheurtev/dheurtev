**PCI Express**
Transfer rate is expressed in transfers per second instead of bits per second because the number of transfers includes the overhead bits, which do not provide additional throughput;[48] PCIe 1.x uses an 8b/10b encoding scheme, resulting in a 20% (= 2/10) overhead on the raw channel bandwidth.[49] So in the PCIe terminology, transfer rate refers to the encoded bit rate: 2.5 GT/s is 2.5 Gbps on the encoded serial link. This corresponds to 2.0 Gbps of pre-coded data or 250 MB/s, which is referred to as throughput in PCIe.
While in early development, PCIe was initially referred to as HSI (for High Speed Interconnect), and underwent a name change to 3GIO (for 3rd Generation I/O) before finally settling on its PCI-SIG name PCI Express. A technical working group named the Arapaho Work Group (AWG) drew up the standard. For initial drafts, the AWG consisted only of Intel engineers; subsequently, the AWG expanded to include industry partners.
In 2003, PCI-SIG introduced PCIe 1.0a, with a per-lane data rate of 250 MB/s and a transfer rate of 2.5 gigatransfers per second (GT/s).
Since, PCIe has undergone several large and smaller revisions, improving on performance and other features.
Transfer rate is expressed in transfers per second instead of bits per second because the number of transfers includes the overhead bits, which do not provide additional throughput;[48] PCIe 1.x uses an 8b/10b encoding scheme, resulting in a 20% (= 2/10) overhead on the raw channel bandwidth.[49] So in the PCIe terminology, transfer rate refers to the encoded bit rate: 2.5 GT/s is 2.5 Gbps on the encoded serial link. This corresponds to 2.0 Gbps of pre-coded data or 250 MB/s, which is referred to as throughput in PCIe.
https://en.wikipedia.org/wiki/PCI_Express

**SATA**
SATA was announced in 2000[5][6] in order to provide several advantages over the earlier PATA interface such as reduced cable size and cost (seven conductors instead of 40 or 80), native hot swapping, faster data transfer through higher signaling rates, and more efficient transfer through an (optional) I/O queuing protocol. Revision 1.0 of the specification was released in January 2003.[3]
Serial ATA (SATA, abbreviated from Serial AT Attachment)[3] is a computer bus interface that connects host bus adapters to mass storage devices such as hard disk drives, optical drives, and solid-state drives. Serial ATA succeeded the earlier Parallel ATA (PATA) standard to become the predominant interface for storage devices.
Serial ATA industry compatibility specifications originate from the Serial ATA International Organization (SATA-IO) which are then promulgated by the INCITS Technical Committee T13, AT Attachment (INCITS T13).
https://en.wikipedia.org/wiki/Serial_ATA

XGA 1024×768 had over 50% marketshare before 2007 and was overtaken by 1366×768 in March 2012 which reached a peak of 27%+ in 2015. Next popular were 1280×800 with a peak of 19.72% in Nov 2009 and 1280×1024 with peak in 2007
https://www.teoalida.com/webdesign/screen-resolution/

In 2002, 1024 × 768 eXtended Graphics Array was the most common display resolution. Many web sites and multimedia products were re-designed from the previous 800 × 600 format to the layouts optimized for 1024 × 768.
The availability of inexpensive LCD monitors made the 5∶4 aspect ratio resolution of 1280 × 1024 more popular for desktop usage during the first decade of the 21st century. 
https://en.wikipedia.org/wiki/Display_resolution

The popular widescreen format (16:10) appeared in 2004 originally in laptops (14″ / 15″ had 1280×800, 17″ had 1440×900), next year desktop monitors appeared in this format (19″ 1440×900, 22″ 1680×1050, 24″ 1920×1200) and high-end laptops received such resolutions. Widescreen monitors come with VGA and DVI ports, running 4:3 resolutions on VGA cable will make image stretching on full screen, while DVI cable maintain aspect ratio and display black bars.

Television industry adopted 16:9 standard in 2008, and because is cheaper to produce both monitors and TV with same aspect ratio, 14″ / 15″ laptops became 1366×768, 17″ laptops became 1600×900, high-end laptops ($1000+) used 1920×1080. New desktop monitors were 18.5″ 1366×768, 20″ 1600×900, 21.5″ / 23″ 1920×1080. QHD, 27″ 2560×1440 appeared in 2009. LG G3 was in 2013 first smartphone with QHD display. Mini-laptops with 10″ screens used 1024×600, called netbooks (smaller than 14-17″ notebooks).
https://www.teoalida.com/webdesign/screen-resolution/

Sony was offering 4K projectors as early as 2004
https://en.wikipedia.org/wiki/4K_resolution

As the first decade of the 21st century progressed, the legacy-free PC went mainstream, with legacy ports removed from commonly available computer systems in all form factors. However, the PS/2 keyboard connector still retains some use, as it can offer some uses (e.g. implementation of n-key rollover) not offered by USB.[6]
https://en.wikipedia.org/wiki/Legacy-free_PC

USB 2.0
The Hi-Speed USB logo
A USB 2.0 PCI expansion card
USB 2.0 was released in April 2000, adding a higher maximum signaling rate of 480 Mbit/s (maximum theoretical data throughput 53 MByte/s[19]) named High Speed or High Bandwidth, in addition to the USB 1.x Full Speed signaling rate of 12 Mbit/s (maximum theoretical data throughput 1.2 MByte/s[20]).
USB 3.x
Main article: USB 3.0
The SuperSpeed USB logo
The USB 3.0 specification was released on 12 November 2008, with its management transferring from USB 3.0 Promoter Group to the USB Implementers Forum (USB-IF), and announced on 17 November 2008 at the SuperSpeed USB Developers Conference.[23]
USB 3.0 adds a SuperSpeed transfer mode, with associated backward compatible plugs, receptacles, and cables. SuperSpeed plugs and receptacles are identified with a distinct logo and blue inserts in standard format receptacles.



# Europe HDTV
The first regular broadcasts started on January 1, 2004, when the Belgian company Euro1080 launched the HD1 channel with the traditional Vienna New Year's Concert. Test transmissions had been active since the IBC exhibition in September 2003, but the New Year's Day broadcast marked the official launch of the HD1 channel, and the official start of direct-to-home HDTV in Europe.[44]
https://en.wikipedia.org/wiki/High-definition_television

Advanced Video Coding (AVC), also referred to as H.264 or MPEG-4 Part 10, is a video compression standard based on block-oriented, motion-compensated coding.[2] It is by far the most commonly used format for the recording, compression, and distribution of video content, used by 91% of video industry developers as of September 2019.[3][4] It supports resolutions up to and including 8K UHD.[5][6]
The intent of the H.264/AVC project was to create a standard capable of providing good video quality at substantially lower bit rates than previous standards (i.e., half or less the bit rate of MPEG-2, H.263, or MPEG-4 Part 2), without increasing the complexity of design so much that it would be impractical or excessively expensive to implement. This was achieved with features such as a reduced-complexity integer discrete cosine transform (integer DCT),[6][7][8][9] variable block-size segmentation, and multi-picture inter-picture prediction. An additional goal was to provide enough flexibility to allow the standard to be applied to a wide variety of applications on a wide variety of networks and systems, including low and high bit rates, low and high resolution video, broadcast, DVD storage, RTP/IP packet networks, and ITU-T multimedia telephony system
H.264 was standardized by the ITU-T Video Coding Experts Group (VCEG) of Study Group 16 together with the ISO/IEC JTC1 Moving Picture Experts Group (MPEG). The project partnership effort is known as the Joint Video Team (JVT). The ITU-T H.264 standard and the ISO/IEC MPEG-4 AVC standard (formally, ISO/IEC 14496-10 – MPEG-4 Part 10, Advanced Video Coding) are jointly maintained so that they have identical technical content. The final drafting work on the first version of the standard was completed in May 2003, and various extensions of its capabilities have been added in subsequent editions. High Efficiency Video Coding (HEVC), a.k.a. H.265 and MPEG-H Part 2 is a successor to H.264/MPEG-4 AVC developed by the same organizations, while earlier standards are still in common use.
H.264 is perhaps best known as being the most commonly used video encoding format on Blu-ray Discs. It is also widely used by streaming Internet sources, such as videos from Netflix, Hulu, Amazon Prime Video, Vimeo, YouTube, and the iTunes Store, Web software such as the Adobe Flash Player and Microsoft Silverlight, and also various HDTV broadcasts over terrestrial (ATSC, ISDB-T, DVB-T or DVB-T2), cable (DVB-C), and satellite (DVB-S and DVB-S2) systems.
https://en.wikipedia.org/wiki/Advanced_Video_Coding


The Blu-ray Disc (BD), often known simply as Blu-ray, is a digital optical disc storage format. It is designed to supersede the DVD format, and capable of storing several hours of high-definition video (HDTV 720p and 1080p). The main application of Blu-ray is as a medium for video material such as feature films and for the physical distribution of video games for the PlayStation 3, PlayStation 4, PlayStation 5, Xbox One, and Xbox Series X. The name "Blu-ray" refers to the blue laser (which is actually a violet laser) used to read the disc, which allows information to be stored at a greater density than is possible with the longer-wavelength red laser used for DVDs.
The BD format was developed by the Blu-ray Disc Association, a group representing makers of consumer electronics, computer hardware, and motion pictures. Sony unveiled the first Blu-ray Disc prototypes in October 2000, and the first prototype player was released in Japan in April 2003. Afterward, it continued to be developed until its official worldwide release on June 20, 2006, beginning the high-definition optical disc format war, where Blu-ray Disc competed with the HD DVD format. Toshiba, the main company supporting HD DVD, conceded in February 2008,[8] and later released its own Blu-ray Disc player in late 2009.[9] According to Media Research, high-definition software sales in the United States were slower in the first two years than DVD software sales.[10] Blu-ray faces competition from video on demand (VOD) and the continued sale of DVDs.[11] In January 2016, 44% of U.S. broadband households had a Blu-ray player.[12]
https://en.wikipedia.org/wiki/Blu-ray


DVB-T2 is an abbreviation for "Digital Video Broadcasting — Second Generation Terrestrial"; it is the extension of the television standard DVB-T, issued by the consortium DVB, devised for the broadcast transmission of digital terrestrial television. DVB has been standardized by ETSI.
This system transmits compressed digital audio, video, and other data in "physical layer pipes" (PLPs), using OFDM modulation with concatenated channel coding and interleaving. The higher offered bit rate, with respect to its predecessor DVB-T, makes it a system suited for carrying HDTV signals on the terrestrial TV channel (though many broadcasters still use plain DVB-T for this purpose). As of 2019, it was implemented in broadcasts in the United Kingdom (Freeview HD, eight channels across two multiplexes, plus an extra multiplex in Northern Ireland carrying three SD channels), Italy (Europa 7 HD, twelve channels), Finland (21 channels, five in HD), Germany (six HD (1080p50) channels, with 40 in planning),[1] the Netherlands (Digitenne, 30 HD (1080p50) channels), Sweden (five channels),[2][3] Thailand (41 SD, 9 HD channels),[4] Flanders (18 SD channels), Serbia (eight channels),[5] Ukraine (32 SD and HD channels in four nationwide multiplexes), Croatia (all national, local and pay-TV channels), Denmark (two pay-TV multiplexes with 20 channels), Romania (8 SD channels, 1 HD channel), and some other countries.
In March 2006 DVB decided to study options for an upgraded DVB-T standard. In June 2006, a formal study group named TM-T2 (Technical Module on Next Generation DVB-T) was established by the DVB Group to develop an advanced modulation scheme that could be adopted by a second generation digital terrestrial television standard, to be named DVB-T2.[6]
According to the commercial requirements and call for technologies[7] issued in April 2007, the first phase of DVB-T2 would be devoted to provide optimum reception for stationary (fixed) and portable receivers (i.e., units which can be nomadic, but not fully mobile) using existing aerials, whereas a second and third phase would study methods to deliver higher payloads (with new aerials) and the mobile reception issue. The novel system should provide a minimum 30% increase in payload, under similar channel conditions already used for DVB-T.
The BBC, ITV, Channel 4 and Channel 5 agreed with the regulator Ofcom to convert one UK multiplex (B, or PSB3) to DVB-T2 to increase capacity for HDTV via DTT.[8] They expected the first TV region to use the new standard would be Granada in November 2009 (with existing switched over regions being changed at the same time). It was expected that over time there would be enough DVB-T2 receivers sold to switch all DTT transmissions to DVB-T2, and H.264.
Ofcom published its final decision on 3 April 2008, for HDTV using DVB-T2 and H.264:[9] BBC HD would have one HD slot after digital switchover (DSO) at Granada. ITV and C4 had, as expected, applied to Ofcom for the 2 additional HD slots available from 2009 to 2012.[10]
Ofcom indicated that it found an unused channel covering 3.7 million households in London, which could be used to broadcast the DVB-T2 HD multiplex from 2010, i.e., before DSO in London. Ofcom indicated that they would look for more unused UHF channels in other parts of the UK, that can be used for the DVB-T2 HD multiplex from 2010 until DSO.[11]
https://en.wikipedia.org/wiki/DVB-T2


