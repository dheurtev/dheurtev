# Some tech evolutions in the 2010s

**In short:**
- [Inventions](#inventions) : 
- [Electronics](#electronics) : 
  * first commercially available NVMe chipset
  * high volume quantum dot manufacturing
  * first commercial shipment of inkjet-printed OLED
  * northbridge merger into CPUs
  * Intel Ultra Path Interconnect
- [Energy](#energy) : 
- [Telecommunications](#telecommunications) : 
  * phasing out ISDN
- [Networking](#networking) : 
  * Energy efficient ethernet / Green Ethernet
  * 40 GbE /100 GbE (core)
  * 2.5 GbE/5 GbE (home/business)
  * NFC deployment
  * Terabyte ethernet research (200 GbE, 400 GbE standards)
- [Cryptography](#cryptography) : 
- [Computers](#computers) :
  * [Form factor](#form-factor) : 
    - Cloud computing and mainframes' demise
    - Tablet computers (Apple iPad) 
  * [OS](#os) : 
    - Google ChromeOS
    - Windows 8
  * [Peripherals](#peripherals) : 
    - Laser printers outsell inkjet printers
  * [Storage](#storage) :  
    - SSD/RAM convergence
    - First NVMe drive
    - M.2 - Next Generation Form Factor (NGFF) connector
    - M.2 mass production begins
    - SSD size and speed growth
  * [Uses](#uses) :
    - *Web browsers*: 
        * Google Chrome (2008+)
        * V8 JavaScript engine(2008+)
        * Google Web Store (2010+)
        * Chromium codebase is widely used
    
- [Consumer Electronics](#consumer-electronics) :
  * [Gadgets](#gadgets) : 
    - Universal remotes
    - Apple Watch : OLED in wearable device
    - Apple iPhone X : optimized OLED
    - first mobile phones with 8K video recording capabilities
  * [Multimedia](#multimedia) : 
    - first players and discs for Ultra HD Blu-ray
    - decline of physical media CD/DVDs 
  * [Screens](#screens) : 
    - 4K LCD monitors
    - high DPI display (Apple Retina)
    - 4K theater and home theater projector
    - microLED demonstration
    - shift to OLED in premium TV
    - UHD 4K
    - plasma decline and demise
    - QLED (Sony Triluminos)
    - high contrast LCD displays 
  * [Broadcast](#broadcast) : 
    - 12K video on Youtube and Vimeo
    - first 4K broadcast in North America
    - 4K on VOD
  * [Video games](#video-games) : 
    - Microsoft Xbox One
    - Sony PlayStation 4
- [Standards and protocols](#standards-and-protocols) : 
  * Broadband via electric power lines (IEEE Std 1901-2010)
  * Energy efficient ethernet
  * 40 GbE -> 400 GbE
  * HTTP/2
  * New wave of file compression (Snappy, Zstd)
- [Programming languages and frameworks](#programming-languages-and-frameworks): 
  * Microsoft .NET framework : extension of patents grants and Mono part of .NET Foundation
  * Rust : system programming language with memory safety
  * Kotlin : Android app developers language, alternative to Java, compiles to Javascript
  * Dart : a tentative Javascript alternative
  * Elixir : a language compatible with the Erlang ecosystem 
  * Elm : domain-specific programming language for declaratively creating web browser-based GUI
  * Julia : high-level, high-performance, dynamic programming language, numerical analysis and computational science
  * Typescript : Javascript transpiler with static types, replaced Coffeescript
  * Swift : Apple's replacement of Objective-C
  * Vulkan : OpenGL successor
  * Microsoft .NET core : cross-platform open-source successor to .NET Framework
  * Solidity : language for the block chain to write smart contracts
- [Navigation](#navigation) : 
  * U.S. GPS modernization
  * Europe's Galileo goes live

## Inventions
**[`^        back to top        ^`](#)**

## Electronics
**[`^        back to top        ^`](#)**

- **First commercially available NVMe chipset (2012) - Integrated Device Technology**

The first commercially available NVMe chipsets were released by Integrated Device Technology (89HF16P04AG3 and 89HF32P08AG3) in August 2012.[24][25] 

https://en.wikipedia.org/wiki/NVM_Express

- **High volume quantum dot manufacturing (2011-2013)**

Quantum dot manufacturing relies on a process called high temperature dual injection which has been scaled by multiple companies for commercial applications that require large quantities (hundreds of kilograms to tonnes) of quantum dots. This reproducible production method can be applied to a wide range of quantum dot sizes and compositions.

The bonding in certain cadmium-free quantum dots, such as III-V-based quantum dots, is more covalent than that in II-VI materials, therefore it is more difficult to separate nanoparticle nucleation and growth via a high temperature dual injection synthesis. An alternative method of quantum dot synthesis, the molecular seeding process, provides a reproducible route to the production of high-quality quantum dots in large volumes. The process utilises identical molecules of a molecular cluster compound as the nucleation sites for nanoparticle growth, thus avoiding the need for a high temperature injection step. Particle growth is maintained by the periodic addition of precursors at moderate temperatures until the desired particle size is reached.[38] The molecular seeding process is not limited to the production of cadmium-free quantum dots; for example, the process can be used to synthesise kilogram batches of high-quality II-VI quantum dots in just a few hours.

Another approach for the mass production of colloidal quantum dots can be seen in the transfer of the well-known hot-injection methodology for the synthesis to a technical continuous flow system. The batch-to-batch variations arising from the needs during the mentioned methodology can be overcome by utilizing technical components for mixing and growth as well as transport and temperature adjustments. For the production of CdSe based semiconductor nanoparticles this method has been investigated and tuned to production amounts of kg per month. Since the use of technical components allows for easy interchange in regards of maximum throughput and size, it can be further enhanced to tens or even hundreds of kilograms.[39]

In 2011 a consortium of U.S. and Dutch companies reported a milestone in high volume quantum dot manufacturing by applying the traditional high temperature dual injection method to a flow system.[40]

On 23 January 2013 Dow entered into an exclusive licensing agreement with UK-based Nanoco for the use of their low-temperature molecular seeding method for bulk manufacture of cadmium-free quantum dots for electronic displays, and on 24 September 2014 Dow commenced work on the production facility in South Korea capable of producing sufficient quantum dots for "millions of cadmium-free televisions and other devices, such as tablets". Mass production is due to commence in mid-2015.[41] On 24 March 2015 Dow announced a partnership deal with LG Electronics to develop the use of cadmium free quantum dots in displays.[42]

https://en.wikipedia.org/wiki/Quantum_dot

- **first commercial shipment of inkjet-printed OLED (2017) - JOLED (Sony, Panasonic)**

On 5 December 2017, JOLED, the successor of Sony and Panasonic's printable OLED business units, began the world's first commercial shipment of inkjet-printed OLED panels.[40][41]

https://en.wikipedia.org/wiki/OLED

- **Northbridge merger into CPUs (2010s)**

In computing, a northbridge (also host bridge, or memory controller hub) is one of two chips comprising the core logic chipset architecture on a PC motherboard. A northbridge is connected directly to a CPU via the front-side bus (FSB) to handle high-performance tasks, and is usually used in conjunction with a slower southbridge[1] to manage communication between the CPU and other parts of the motherboard.[2] Since the 2010s, die shrink and improved transistor density have allowed for increasing chipset integration, and the functions performed by northbridges are now often incorporated into other components (like southbridges or CPUs themselves).[3][4] As of 2019, Intel and AMD had both released chipsets in which all northbridge functions had been integrated into the CPU.[4] Modern Intel Core processors have the northbridge integrated on the CPU die, where it is known as the uncore or system agent.
Since the 2010s, die shrink and improved transistor density have allowed for increasing chipset integration, and the functions performed by northbridges are now often incorporated into other components (like southbridges or CPUs themselves).[3][4] As of 2019, Intel and AMD had both released chipsets in which all northbridge functions had been integrated into the CPU.[4] Modern Intel Core processors have the northbridge integrated on the CPU die, where it is known as the uncore or system agent.

https://en.wikipedia.org/wiki/Northbridge_(computing)

- **Intel Ultra Path Interconnect (2017) - Intel**

The Intel Ultra Path Interconnect (UPI)[1][2] is a point-to-point processor interconnect developed by Intel which replaced the Intel QuickPath Interconnect (QPI) in Xeon Skylake-SP platforms starting in 2017. UPI is a low-latency coherent interconnect for scalable multiprocessor systems with a shared address space. It uses a directory-based home snoop coherency protocol with a transfer speed of up to 10.4 GT/s. Supporting processors typically have two or three UPI links.
Comparing to QPI, it improves power efficiency with a new low-power state, improves transfer efficiency with a new packetization format, and improves scalability with protocol layer that does not require preallocation of resources. UPI only supports directory-based coherency, unlike previous QPI processors which supported multiple snoop modes (no snoop, early snoop, home snoop, and directory).
A combined caching and home agent (CHA) handles resolution of coherency across multiple processors, as well as snoop requests from processor cores and local and remote agents. Separate physical CHAs are placed within each processor core and last level cache (LLC) bank to improve scalability according to the number of cores, memory controllers, or the sub-NUMA clustering mode. The address space is interleaved across different CHAs, which act like a single logical agent.

https://en.wikipedia.org/wiki/Intel_Ultra_Path_Interconnect


## Energy
**[`^        back to top        ^`](#)**
## Telecommunications
**[`^        back to top        ^`](#)**

- **Phasing out ISDN (2010s-2020s)**

In 2013, Verizon announced it would no longer take orders for ISDN service in the Northeastern United States.[9]

In early 2015, BT announced their intention to retire the UK's ISDN infrastructure by 2025.[27]

On November 2, 2010, NTT announced plans to migrate their backend from PSTN to the IP network from around 2020 to around 2025. For this migration, ISDN services will be retired, and fiber optic services are recommended as an alternative.[26]

https://en.wikipedia.org/wiki/Integrated_Services_Digital_Network

As of 2015 ISDN is being phased out by most major telecommunication carriers throughout Europe in favour of all-IP networks, with some expecting complete migration by 2025.

https://en.wikipedia.org/wiki/Business_telephone_system#Private_branch_exchange

## Networking
**[`^        back to top        ^`](#)**

### Ethernet

- **Energy Efficient ethernet (2008-2010)**

The power reduction is accomplished in a few ways. In Fast Ethernet and faster links, constant and significant energy is used by the physical layer as transmitters are active regardless of whether data is being sent. If they could be put into sleep mode when no data is being sent, that energy could be saved.[8] When the controlling software or firmware decides that no data needs to be sent, it can issue a low-power idle (LPI) request to the Ethernet controller physical layer PHY. The PHY will then send LPI symbols for a specified time onto the link, and then disable its transmitter. Refresh signals are sent periodically to maintain link signaling integrity. When there is data to transmit, a normal IDLE signal is sent for a predetermined period of time. The data link is considered to be always operational, as the receive signal circuit remains active even when the transmit path is in sleep mode.[9]

Green Ethernet technology was a superset of the 802.3az standard. In addition to the link load power savings of Energy-Efficient Ethernet, Green Ethernet works in one of two ways. First, it detects link status, allowing each port on the switch to power down into a standby mode when a connected device, such as a computer, is not active. Second, it detects cable length and adjusts the power used for transmission accordingly. Standard switches provide enough power to send a signal up to 100 meters (330 ft).[10] However, this is often unnecessary in the SOHO environment, where 5 to 10 meters (16 to 33 ft) of cabling are typical between rooms. Moreover, small data centers can also benefit from this approach since the majority of cabling is confined to a single room with a few meters of cabling among servers and switches. In addition to the pure power saving benefits of Green Ethernet, backing off the transmit power on shorter cable runs reduces alien crosstalk, and improves the overall performance of the cabling system.

Green Ethernet also encompasses the use of more efficient circuitry in Ethernet chips, and the use of offload engines on Ethernet interface cards intended for network servers.[6] In April 2008, the term was used for switches, and, in July 2008, used with wireless routers which featured user-selectable off periods for Wi-Fi to further reduce energy consumption.[11]

Green Ethernet was first employed on home products. However, low port counts mean that significant energy savings are not going to be made using this technology only in the home. Turning off existing devices when they are idle is likely to provide a more immediate saving.[12] Projected power savings of up to 80 percent were estimated using Green Ethernet switches,[13] translating into a longer product life due to reduced heat dissipation.[14]

https://en.wikipedia.org/wiki/Energy-Efficient_Ethernet

- **40GbE / 100GbE - 40 Gigabit Ethernet / 100 Gigabit Ethernet**

40 Gigabit Ethernet (40GbE) and 100 Gigabit Ethernet (100GbE) are groups of computer networking technologies for transmitting Ethernet frames at rates of 40 and 100 gigabits per second (Gbit/s), respectively. These technologies offer significantly higher speeds than 10 Gigabit Ethernet. The technology was first defined by the IEEE 802.3ba-2010 standard[1] and later by the 802.3bg-2011, 802.3bj-2014,[2] 802.3bm-2015,[3] and 802.3cd-2018 standards.

https://en.wikipedia.org/wiki/100_Gigabit_Ethernet

- **to Terabit Ethernet**

Terabit Ethernet or TbE is Ethernet with speeds above 100 Gigabit Ethernet. 400 Gigabit Ethernet (400G, 400GbE) and 200 Gigabit Ethernet (200G, 200GbE)[1] standards developed by the IEEE P802.3bs Task Force using broadly similar technology to 100 Gigabit Ethernet[2][3] were approved on December 6, 2017.[4][5] In 2016, several networking equipment suppliers were already offering proprietary solutions for 200G and 400G.[5]

The Ethernet Alliance's 2020 technology roadmap expects speeds of 800 Gbit/s and 1.6 Tbit/s to become IEEE standard after 2020, possibly between 2023 and 2030.[6][7] Doubling to 800 GbE is expected to occur after 112 Gbit/s SerDes become available. The Optical Internetworking Forum (OIF) has already announced five new projects at 112 Gbit/s which would also make 4th generation (single-lane) 100 GbE links possible.[8] The IEEE P802.3df Task Force started work in January 2022 to standardize 800 Gbit/s and 1.6 Tbit/s Ethernet. [9]

Facebook and Google, among other companies, have expressed a need for TbE.[10] While a speed of 400 Gbit/s is achievable with existing technology, 1 Tbit/s (1000 Gbit/s) would require different technology.[2][11] Accordingly, at the IEEE Industry Connections Higher Speed Ethernet Consensus group meeting in September 2012, 400 GbE was chosen as the next generation goal.[2] Additional 200GbE objectives were added in January 2016.

The University of California, Santa Barbara (UCSB) attracted help from Agilent Technologies, Google, Intel, Rockwell Collins, and Verizon Communications to help with research into next generation Ethernet.[12]

As of early 2016, chassis/modular based core router platforms from Cisco, Juniper and other major manufacturers support 400 Gbit/s full duplex data rates per slot. One, two and four port 100GbE and one port 400GbE line cards are presently available. As of early 2019, 200GbE line cards became available after 802.3cd standard ratification.[13][14]

200G Ethernet uses PAM4 signaling which allows 2 bits to be transmitted per clock cycle, but at a higher implementation cost.[15]

https://en.wikipedia.org/wiki/Terabit_Ethernet

### NFC

- **NFC deployment (2010-2015)**

A patent licensing program for NFC is under deployment by France Brevets, a patent fund created in 2011

2010: Innovision released a suite of designs and patents for low cost, mass-market mobile phones and other devices.[33]

2010: Nokia C7: First NFC-capable smartphone released.[34] NFC feature was enabled by software update in early 2011.[35]

2010: Samsung Nexus S: First Android NFC phone shown[36][37]

May 21, 2010: Nice, France launches, with "Cityzi", the "Nice City of contactless mobile" project, the first in Europe to provide inhabitants with NFC bank cards and mobile phones (like Samsung Player One S5230), and a "bouquet of services" covering transportation (tramways and bus), tourism and student's services[38][39][40]

2011: Google I/O "How to NFC" demonstrates NFC to initiate a game and to share a contact, URL, app or video.[41]

2011: NFC support becomes part of the Symbian mobile operating system with the release of Symbian Anna version.[42]

2011: Research In Motion devices are the first ones certified by MasterCard Worldwide for their PayPass service[43]

2012: UK restaurant chain EAT. and Everything Everywhere (Orange Mobile Network Operator), partner on the UK's first nationwide NFC-enabled smartposter campaign. A 
dedicated mobile phone app is triggered when the NFC-enabled mobile phone comes into contact with the smartposter.[44]

2012: Sony introduced NFC "Smart Tags" to change modes and profiles on a Sony smartphone at close range, included with the Sony Xperia P Smartphone released the same year.[45]

2013: Samsung and VISA announce their partnership to develop mobile payments.

2013: IBM scientists, in an effort to curb fraud and security breaches, develop an NFC-based mobile authentication security technology. This technology works on similar principles to dual-factor authentication security.[46]

October 2014: Dinube becomes the first non-card payment network [47][48] to introduce NFC contactless payments natively on a mobile device, i.e. no need for an external case attached or NFC 'sticker' nor for a card. Based on Host card emulation with its own application identifier (AID),[49] contactless payment was available on Android KitKat upwards and commercial release commenced in June 2015.[50]

2014: AT&T, Verizon and T-Mobile released Softcard (formerly ISIS mobile wallet). It runs on NFC-enabled Android phones and iPhone 4 and iPhone 5 when an external NFC case is attached. The technology was purchased by Google and the service ended on March 31, 2015.

November 2015: Swatch and Visa Inc. announced a partnership to enable NFC financial transactions using the "Swatch Bellamy" wristwatch. The system is currently online in Asia, through a partnership with China UnionPay and Bank of Communications. The partnership will bring the technology to the US, Brazil, and Switzerland.[51]

November 2015: Google’s Android Pay function was launched, a direct rival to Apple Pay, and its roll-out across the US commenced.[52]

https://en.wikipedia.org/wiki/Near-field_communication


## Cryptography
**[`^        back to top        ^`](#)**
## Computers
**[`^        back to top        ^`](#)**
### Form factor
**[`^        back to top        ^`](#)**

- **Cloud computing and mainframes' demise (2010s)**

Starting in the 2010s, cloud computing is now a less expensive, more scalable alternative commonly called Big Data.

https://en.wikipedia.org/wiki/Mainframe_computer

In 2012, NASA powered down its last mainframe, an IBM System z.

https://en.wikipedia.org/wiki/Mainframe_computer

- **iPad - first mass-market tablet computer (2010) - Apple**

A tablet computer, commonly shortened to tablet, is a mobile device, typically with a mobile operating system and touchscreen display processing circuitry, and a rechargeable battery in a single, thin and flat package. Tablets, being computers, do what other personal computers do, but lack some input/output (I/O) abilities that others have. Modern tablets largely resemble modern smartphones, the only differences being that tablets are relatively larger than smartphones, with screens 7 inches (18 cm) or larger, measured diagonally,[1][2][3][4] and may not support access to a cellular network.

The touchscreen display is operated by gestures executed by finger or digital pen (stylus), instead of the mouse, touchpad, and keyboard of larger computers. Portable computers can be classified according to the presence and appearance of physical keyboards. Two species of tablet, the slate and booklet, do not have physical keyboards and usually accept text and other input by use of a virtual keyboard shown on their touchscreen displays. To compensate for their lack of a physical keyboard, most tablets can connect to independent physical keyboards by Bluetooth or USB; 2-in-1 PCs have keyboards, distinct from tablets.

The form of the tablet was conceptualized in the middle of the 20th century (Stanley Kubrick depicted fictional tablets in the 1968 science fiction film A Space Odyssey) and prototyped and developed in the last two decades of that century. **In 2010, Apple released the iPad, the first mass-market tablet** to achieve widespread popularity.[5] Thereafter, tablets rapidly rose in ubiquity and soon became a large product category used for personal, educational and workplace applications,[6] with sales stabilizing in the mid-2010s.[7][8][9] Popular uses for a tablet PC include viewing presentations, video-conferencing, reading e-books, watching movies, sharing photos and more.[10]

https://en.wikipedia.org/wiki/Tablet_computer

### OS
**[`^        back to top        ^`](#)**

- **Google ChromeOS (2011)**

ChromeOS,[8] formerly styled as Chrome OS, is a Linux-based operating system designed by Google. It is derived from the open-source Chromium OS and uses the Google Chrome web browser as its principal user interface.

Google announced the project in July 2009, initially describing it as an operating system where applications and user data would reside in the cloud. ChromeOS was used primarily to run web applications.[9]

All ChromiumOS and ChromeOS versions support progressive web applications (such as Google Docs or Microsoft Office 365), as well as web browser extensions (which can resemble native applications). ChromeOS (but not ChromiumOS) from 2016 onwards can also run Android applications from the Play Store.[10] Since 2018, ChromiumOS/ChromeOS version 69 onwards also support Linux applications, which are executed in a lightweight virtual machine[11] with a Debian Linux environment.[12][13]

The operating system is now rarely evaluated apart from the hardware that runs it.

https://en.wikipedia.org/wiki/ChromeOS

- **Windows 8 - Bluetooth Low Energy, Touchscreen, Windows Store, UEFI, ARM support (2012)**

Windows 8 introduced BLE (Bluetooth Low Energy) mouse/HID support.[109]

https://en.wikipedia.org/wiki/Computer_mouse

Windows 8 is a major release of the Windows NT operating system developed by Microsoft. It was released to manufacturing on August 1, 2012; it was subsequently made available for download via MSDN and TechNet on August 15, 2012,[6] and later to retail on October 26, 2012.[7]
Windows 8 introduced major changes to the operating system's platform and user interface intended to improve its user experience on tablets, where Windows was now competing with mobile operating systems, including Android and iOS.[8] In particular, these changes included a touch-optimized Windows shell based on Microsoft's Metro design language and the Start screen, a new platform for developing apps with an emphasis on touchscreen input, integration with online services, and Windows Store, an online distribution for downloading and purchasing new software, and a new keyboard shortcut for screenshots.[9] Many of these features were adopted from Windows Phone. Windows 8 added support for USB 3.0, Advanced Format hard drives, near field communications, and cloud computing. Additional security features were introduced, such as built-in antivirus software, integration with Microsoft SmartScreen phishing filtering service and support for UEFI Secure Boot on supported devices. Windows 8 is the first version of Windows to support the ARM architecture, under the Windows RT branding. Windows 8 removed support for non-SSE2 CPUs and devices without NX.

https://en.wikipedia.org/wiki/Windows_8

### Peripherals
**[`^        back to top        ^`](#)**

- **Laser printers outsell inkjet printers (2010s)**

In 2015: laser printers: 10.2% vs. inkjet printers 5.1%.

https://www.statista.com/statistics/541415/worldwide-printer-market-installed-base-by-type/

By 2019, laser printers outsold inkjet printers by nearly a 2:1 ratio, 9.6% vs 5.1%.

https://en.wikipedia.org/wiki/Inkjet_printing

### Storage
**[`^        back to top        ^`](#)**

- **SSD/RAM convergence (2010s)**

Since 2006, "solid-state drives" (based on flash memory) with capacities exceeding 256 gigabytes and performance far exceeding traditional disks have become available. This development has started to blur the definition between traditional random-access memory and "disks", dramatically reducing the difference in performance.

https://en.wikipedia.org/wiki/Random-access_memory

- **First NVMe drive (2013) - Samsung**

The first NVMe drive, Samsung's XS1715 enterprise drive, was announced in July 2013; 

https://en.wikipedia.org/wiki/NVM_Express

- **M.2 - Next Generation Form Factor (NGFF) connector (2012) - Intel**

M.2, pronounced m dot two[1] and formerly known as the Next Generation Form Factor (NGFF), is a specification for internally mounted computer expansion cards and associated connectors. M.2 replaces the mSATA standard, which uses the PCI Express Mini Card physical card layout and connectors. Employing a more flexible physical specification, M.2 allows different module widths and lengths, which, paired with the availability of more advanced interfacing features, makes M.2 more suitable than mSATA in general for solid-state storage applications, particularly in smaller devices such as ultrabooks and tablets.[2][3][4]

Computer bus interfaces provided through the M.2 connector are PCI Express 4.0 (up to four lanes), Serial ATA 3.0, and USB 3.0 (a single logical port for each of the latter two). It is up to the manufacturer of the M.2 host or module to select which interfaces are to be supported, depending on the desired level of host support and the module type. Different M.2 connector keying notches denote various purposes and capabilities of both the M.2 hosts and modules, and also prevent the M.2 modules from being inserted into incompatible host connectors.[2][3][5]

The M.2 specification supports NVM Express (NVMe) as the logical device interface for M.2 PCI Express SSDs, in addition to supporting legacy Advanced Host Controller Interface (AHCI) at the logical interface level. While the support for AHCI ensures software-level backward compatibility with legacy SATA devices and legacy operating systems, NVM Express is designed to fully utilize the capability of high-speed PCI Express storage devices to perform many I/O operations in parallel.[2]: 14 [6]

https://en.wikipedia.org/wiki/M.2

M.2 was conceived as an successor of the mSATA interface and for the first time introduced by Intel in 2012 under the label Next Generation Form Factor (NGFF).

https://www.delock.de/infothek/M.2/M.2_e.html

- **M.2 mass production begins (2015) - Samsung**

Samsung has started mass production of the industry’s first NVMe PCIe solid state drive (SSD), with an M.2 form factor for use in PCs and workstations

https://nvmexpress.org/mass-production-of-industrys-first-m-2-nvme-pcie-ssds-begins/

- **SSD size and speed growth (2010s)**

In 2016, Seagate demonstrated 10 GB/s sequential read and write speeds from a 16-lane PCIe 3.0 SSD, and a 60 TB SSD in a 3.5-inch form factor. Samsung also launched to market a 15.36 TB SSD with a price tag of US$10,000 using a SAS interface, using a 2.5-inch form factor but with the thickness of 3.5-inch drives. This was the first time a commercially available SSD had more capacity than the largest currently available HDD.[50][51][52][53][54]

In 2018, both Samsung and Toshiba launched 30.72 TB SSDs using the same 2.5-inch form factor but with 3.5-inch drive thickness using a SAS interface. Nimbus Data announced and reportedly shipped 100 TB drives using a SATA interface, a capacity HDDs are not expected to reach until 2025. Samsung introduced an M.2 NVMe SSD with read speeds of 3.5 GB/s and write speeds of 3.3 GB/s.[55][56][57][58][59][60][61] A new version of the 100 TB SSD was launched in 2020 at a price of US$40,000, with the 50 TB version costing US$12,500.[62][63]

In 2019, Gigabyte Technology demonstrated an 8 TB 16-lane PCIe 4.0 SSD with 15.0 GB/s sequential read and 15.2 GB/s sequential write speeds at Computex 2019. It included a fan, as new, high speed SSDs run at high temperatures.[64] Also in 2019, NVMe M.2 SSDs using the PCIe 4.0 interface were launched. These SSDs have read speeds of up to 5.0 GB/s and write speeds of up to 4.4 GB/s. Due to their high speed operation, these SSDs use large heatsinks and, without sufficient cooling airflow, will typically thermally throttle down after roughly 15 minutes of continuous operation at full speed.[65] Samsung also introduced SSDs capable of 8 GB/s sequential read and write speeds and 1.5 million IOPS, capable of moving data from damaged chips to undamaged chips, to allow the SSD to continue working normally, albeit at a lower capacity.[66][67][68].

https://en.wikipedia.org/wiki/Solid-state_drive

### Uses
**[`^        back to top        ^`](#)**

#### Web Browsers

- **Google Chrome (2008+), V8 JavaScript engine(2008+), Google Web Store (2010+)**

Google Chrome is a cross-platform web browser developed by Google. It was first released in 2008 for Microsoft Windows, built with free software components from Apple WebKit and Mozilla Firefox.[14] It was later ported to Linux, macOS, iOS, and Android, where it is the default browser.[15] The browser is also the main component of ChromeOS, where it serves as the platform for web applications.

Chrome overtook Firefox in November 2011, in worldwide usage. As of March 2021, according to StatCounter, Google Chrome had 67% worldwide desktop usage share, making it the most widely used web browser.[297]

The first release of Google Chrome passed both the Acid1 and Acid2 tests. Beginning with version 4.0, Chrome has passed all aspects of the Acid3 test.[60]

As of May 2011, Chrome has very good support for JavaScript/ECMAScript according to Ecma International's ECMAScript standards conformance Test 262[61] (version ES5.1 May 18, 2012). This test reports as the final score the number of tests a browser failed; hence lower scores are better. In this test, Chrome version 37 scored 10 failed/11,578 passed. For comparison, Firefox 19 scored 193 failed/11,752 passed and Internet Explorer 9 has a score of 600+ failed, while Internet Explorer 10 has a score of 7 failed.

In 2011, on the official CSS 2.1 test suite by standardization organization W3C, WebKit, the Chrome rendering engine, passes 89.75% (89.38% out of 99.59% covered) CSS 2.1 tests.[62]

Announced on December 7, 2010, the Chrome Web Store allows users to install web applications as extensions to the browser, although most of these extensions function simply as links to popular web pages and/or games, some of the apps like Springpad do provide extra features like offline access. The themes and extensions have also been tightly integrated into the new store, allowing users to search the entire catalog of Chrome extras.[84]

The Chrome Web Store was opened on February 11, 2011, with the release of Google Chrome 9.0.[85]

The JavaScript virtual machine used by Chrome, the **V8 JavaScript engine**, has features such as dynamic code generation, hidden class transitions, and precise garbage collection.[29]

In 2008, several websites performed benchmark tests using the SunSpider JavaScript Benchmark tool as well as Google's own set of computationally intense benchmarks, which include ray tracing and constraint solving.[98] They unanimously reported that Chrome performed much faster than all competitors against which it had been tested, including Safari (for Windows), Firefox 3.0, Internet Explorer 7, Opera, and Internet Explorer 8.[99][100][101][58]{[102][103] However, on October 11, 2010, independent tests of JavaScript performance, Chrome has been scoring just behind Opera's Presto engine since it was updated in version 10.5.[104]

On September 3, 2008, Mozilla responded by stating that their own TraceMonkey JavaScript engine (then in beta), was faster than Chrome's V8 engine in some tests.[105][106][107] John Resig, Mozilla's JavaScript evangelist, further commented on the performance of different browsers on Google's own suite, commenting on Chrome's "decimating" of the other browsers, but he questioned whether Google's suite was representative of real programs. He stated that Firefox 3.0 performed poorly on recursion-intensive benchmarks, such as those of Google, because the Mozilla team had not implemented recursion-tracing yet.[108]

Two weeks after Chrome's launch in 2008, the WebKit team announced a new JavaScript engine, SquirrelFish Extreme,[109] citing a 36% speed improvement over Chrome's V8 engine

https://en.wikipedia.org/wiki/Google_Chrome

- **Chromium codebase is widely used (2008+)** 

Chromium is a free and open-source web browser project, mainly developed and maintained by Google.[3] This codebase provides the vast majority of code for the Google Chrome browser, which is proprietary software and has some additional features.

Google Chrome debuted in September 2008, and along with its release, the Chromium source code was also made available, allowing builds to be constructed from it.[1][19][20]

Chromium has been a Google project since its inception,[1][3] and Google employees have done the bulk of the development work.[2]

The Chromium codebase is widely used. Microsoft Edge, Samsung Internet, Opera, and many other browsers are based on the Chromium code. Moreover, significant portions of the code are used by several app frameworks.

Google does not provide an official stable version of the Chromium browser, but does provide some official API keys for some included functionality, such as speech to text, text to speech, translation, etc.


## Consumer Electronics
**[`^        back to top        ^`](#)**
### Gadgets
**[`^        back to top        ^`](#)**

- **Universal remotes - infrared emitters in smartphones (2010s)**

By the early 2000s, the number of consumer electronic devices in most homes greatly increased, along with the number of remotes to control those devices. According to the Consumer Electronics Association, an average US home has four remotes.[citation needed] To operate a home theater as many as five or six remotes may be required, including one for cable or satellite receiver, VCR or digital video recorder (DVR/PVR), DVD player, TV and audio amplifier. Several of these remotes may need to be used sequentially for some programs or services to work properly. However, as there are no accepted interface guidelines, the process is increasingly cumbersome. One solution used to reduce the number of remotes that have to be used is the universal remote, a remote control that is programmed with the operation codes for most major brands of TVs, DVD players, etc. 

In the early 2010s, many smartphone manufacturers began incorporating infrared emitters into their devices, thereby enabling their use as **universal remotes** via an included or downloadable app.[30]

- **Apple Watch - OLED display on wearable device (2015) - Apple**

Apple Watch is a line of smartwatches produced by Apple Inc. It incorporates fitness tracking, health-oriented capabilities, and wireless telecommunication, and integrates with iOS and other Apple products and services.

The Apple Watch was released in April 2015[18][19] and quickly became the best-selling wearable device: 4.2 million were sold in the second quarter of fiscal 2015,[20][21] and more than 100 million people were estimated to use an Apple Watch as of December 2020.[22] Apple has introduced new generations of the Apple Watch with improved internal components each September[23][24][25][26]—each labeled by Apple as a 'Series', with certain exceptions.[a]

Each Series has been initially sold in multiple variants defined by the watch casing's material, color, and size (except for the budget watches Series 1 and SE, available only in aluminum[27][29]), and beginning with Series 3, by the option in the aluminum variants for LTE cellular connectivity, which comes standard with the other materials.[30] The band included with the watch can be selected from multiple options from Apple, and watch variants in aluminum co-branded with Nike and in stainless steel co-branded with Hermès are also offered, which include exclusive bands, colors, and digital watch faces carrying those companies' brandings.[30]

The Apple Watch operates primarily in conjunction with the user's iPhone for functions such as configuring the watch and syncing data with iPhone apps, but can separately connect to a Wi-Fi network for some data-reliant purposes, including basic communications and audio streaming.[31][32] LTE-equipped models can connect to a mobile network, including for calling, texting, and installed mobile app data use, substantially reducing the need for an iPhone after initial setup. Although the paired iPhone need not be near the watch, to make a call with the watch, the paired iPhone must still be powered on and connected to a cellular network.[33][b] The oldest iPhone model that is compatible with any given Apple Watch depends on the version of system software installed on each device.[35] As of September 2021, new Apple Watches come with watchOS 8 preinstalled and require an iPhone running iOS 15, which is available for the iPhone 6S and later.[36]

https://en.wikipedia.org/wiki/Apple_Watch

Apple began using OLED panels in its watches in 2015 and in its laptops in 2016 with the introduction of an OLED touchbar to the MacBook Pro

https://en.wikipedia.org/wiki/OLED

- **Apple iPhone X - optimized OLED display (2017) - Apple**

In 2017, Apple announced the introduction of their tenth anniversary iPhone X with their own optimized OLED display licensed from Universal Display Corporation.[223] Apple has continued the use of the technology in the iPhone X's successors, such as the iPhone XS and iPhone XS Max, and iPhone 11 Pro and iPhone 11 Pro Max.

https://en.wikipedia.org/wiki/OLED

### Multimedia
**[`^        back to top        ^`](#)**

- **first players and discs for Ultra HD Blu-ray — a physical optical disc format supporting 4K resolution and high-dynamic-range video (HDR) (2016)**

In March 2016 the first players and discs for Ultra HD Blu-ray—a physical optical disc format supporting 4K resolution and high-dynamic-range video (HDR) at 60 frames per second—were released.[86]
On August 2, 2016, Microsoft released the Xbox One S, which supports 4K streaming and has an Ultra HD Blu-ray disc drive, but does not support 4K gaming.[87] On November 10, 2016, Sony released the PlayStation 4 Pro, which supports 4K streaming and gaming,[88] though many games use checkerboard rendering or are upscaled 4K.[89] On November 7, 2017, Microsoft released the Xbox One X, which supports of 4K streaming and gaming,[90] though not all games are rendered at native 4K.[91]

https://en.wikipedia.org/wiki/4K_resolution

- **Decline of physical media CD/DVD (2010s)**

In the 2010s, revenues from digital music services, such as iTunes, Spotify, and YouTube, matched those from physical format sales for the first time.[3] According to the RIAA's midyear report in 2020, phonograph record revenues surpassed those of CDs for the first time since the 1980s.

As of 2012, CDs and DVDs made up only 34% of music sales in the United States.[50] By 2015, only 24% of music in the United States was purchased on physical media, 2/3 of this consisting of CDs;[51] however, in the same year in Japan, over 80% of music was bought on CDs and other physical formats.[52] In 2018, U.S. CD sales were 52 million units—less than 6% of the peak sales volume in 2000.[46] In the UK 32 million units were sold, almost 100 million fewer than in 2008.[53]

During the 2010s, the increasing popularity of solid-state media and music streaming services caused automakers to remove automotive CD players in favor of minijack auxiliary inputs, wired connections to USB devices and wireless Bluetooth connections.[54] Automakers viewed CD players as using up valuable space and taking up weight which could be reallocated to more popular features, like large touchscreens.[55] By 2021, only Lexus and General Motors were still including CD players as standard equipment with certain vehicles.[55]

Despite rapidly declining sales year-over-year, the pervasiveness of the technology lingered for a time, with companies placing CDs in pharmacies, supermarkets, and filling station convenience stores to target buyers less likely to be able to use Internet-based distribution.[13] In 2018 Best Buy announced plans to decrease their focus on CD sales, however, while continuing to sell records, sales of which are growing during the vinyl revival.[56][57][58]

https://en.wikipedia.org/wiki/Compact_Disc_Digital_Audio



### Screens
**[`^        back to top        ^`](#)**

- **4K LCD monitors (2010) - Multiple manufacturers and 5K MacBookPro (2012) - Apple**

In 2010, 27-inch LCD monitors with the 2560 × 1440 resolution were released by multiple manufacturers, and in 2012, Apple introduced a 2880 × 1800 display on the MacBook Pro. Panels for professional environments, such as medical use and air traffic control, support resolutions up to 4096 × 2160[3] (or, more relevant for control rooms, 1∶1 2048 × 2048 pixels).[4][5]

https://en.wikipedia.org/wiki/Display_resolution

- **Retina display - High DPI display (2011) - Apple**

A trend of high DPI display was started in 2011 when Apple launched Retina display, which doubles number of pixels while keeping the size of screen elements. PC monitors and laptops did not adopt this trend quickly because of poor Windows scaling, certain elements retain their original size when you set screen scaling.

https://www.teoalida.com/webdesign/screen-resolution/

- **4K theater and home theater projector (2011-2012) - Sony**

The first 4K home theater projector was released by Sony in 2012.
Theaters began projecting movies at 4K resolution in 2011

https://en.wikipedia.org/wiki/4K_resolution

- **MicroLED demonstration - Sony, 2012**

The first microLED products were demonstrated by Sony in 2012. These displays, however, were very expensive.[43]
microLEDs have innate potential performance advantages over LCD displays, including higher brightness, lower latency, higher contrast ratio, greater color saturation, intrinsic self-illumination, and better efficiency. As of 2016, technological and production barriers have prevented commercialization.[72]

As of 2016, a number of different technologies were under active research for the assembling of individual LED pixels on a substrate. These include chip bonding of microLED chips onto a substrate (considered to have potential for large displays), wafer production methods using etching to produce an LED array followed by bonding to an IC, and wafer production methods using an intermediate temporary thin film to transfer the LED array to a substrate.

Glo and Jasper Display Corporation demonstrated the world's first RGB microLED microdisplay, measuring 0.55 inches (1.4 cm) diagonally, at SID Display Week 2017. Glo transferred their microLEDs to the Jasper Display backplane.[73]

Sony launched a 55-inch (140 cm) "Crystal LED Display" in 2012 with 1920 × 1080 resolution, as a demonstration product.[72] Sony announced its CLEDIS (Crystal LED Integrated Structure) brand which used surface mounted LEDs for large display production.[74] As of August 2019, Sony offers CLEDIS in 146-inch (3.7 m), 182-inch (4.6 m) and 219-inch (5.6 m) displays.[75] On 12 September 2019, Sony announced Crystal LED availability to consumers ranging from 1080p 110-inch (2.8 m) to 16K 790-inch (20 m) displays.[76]

Samsung demonstrated a 146-inch (3.7 m) microLED display called The Wall at CES 2018.[77] In July 2018, Samsung announced plans on bringing a 4K microLED TV to consumer market in 2019.[78] At CES 2019, Samsung demonstrated a 75-inch (1.9 m) 4K microLED display and 219-inch (5.6 m) 6K microLED display.[79] On June 12 at InfoComm 2019, Samsung announced the global launch of The Wall Luxury microLED display configurable from 73-inch (1.9 m) in 2K to 292-inch (7.4 m) in 8K.[80] On October 4, 2019, Samsung announced that The Wall Luxury microLED display shipments had begun.[9][81]

In March 2018, Bloomberg reported Apple to have about 300 engineers devoted to in-house development of microLED screens.[82][83] At IFA 2018 in August, LG Display demonstrated a 173-inch (4.4 m) microLED display.[12]

At SID's Display Week 2019 in May, Tianma and PlayNitride demonstrated their co-developed 7.56-inch (19.2 cm) microLED display with over 60% transparency.[13][14] China Star Optoelectronics Technology (CSoT) demonstrated a 3.3-inch (8.4 cm) transparent microLED display with around 45% transparency, also co-developed with PlayNitride.[15] Plessey Semiconductors Ltd demonstrated a monolithic monochrome blue GaN-on-silicon wafer bonded to a Jasper Display CMOS backplane 0.7-inch (18 mm) active-matrix microLED display with an 8 μm pixel pitch.[84][85][86][87]

At SID's Display Week 2019 in May, Jade Bird Display demonstrated their 720p and 1080p microLED microdisplays with 5 μm and 2.5 μm pitch respectively, achieving luminance in the millions of candelas per square metre. In 2021, Jade Bird Display and Vuzix have entered a Joint manufacturing agreement for making microLED based projectors for smart glasses and augmented reality glasses [88]

At Touch Taiwan 2019 on September 4, 2019, AU Optronics demonstrated a 12.1-inch (31 cm) microLED display and indicated that microLED was 1–2 years from mass commercialization.[89] At IFA 2019 on September 13, 2019, TCL Corporation demonstrated their Cinema Wall featuring a 4K 132-inch (3.4 m) microLED display with maximum brightness of 1,500 cd/m2 and 2,500,000∶1 contrast ratio produced by their subsidiary China Star Optoelectronics Technology (CSoT).[16]

https://en.wikipedia.org/wiki/MicroLED

- **Shift to OLED in premium TV (2013+)**

Cumulative global shipments of organic light-emitting diode (OLED) TVs manufactured by South Korea’s LG Electronics Inc. have exceeded 5 million units this month amid fast penetration into the premium TV mainstream.

According to industry sources on Monday, global shipments of LG Electronics OLED TVs have surpassed 5 million units this month, with the average addition of 100,000 units per month after passing the threshold of 4.86 million units by the end of September.

LG Electronics was the first to shift to OLED in early 2010s at a time when the liquid crystal display (LCD) was the mainstream for TV screens. With the first mass production in 2013, the company grew to the industry leader and its OLED TV shipments surged from 76,000 units in 2014 to 666,000 units in 2016 and 1.564 million units in 2018.

Currently, OLED TVs are being manufactured by a total of 15 brands including China’s Skyworth, Konka, Changhong and Hisense, Japan’s Sony, Toshiba and Panasonic, and Europe’s Phillips, Grundig, Vestel and Bang & Olufsen.

TV shipments in North America, Europe and China were down around 20% year-over-year in the first quarter and Omdia also expects TV sales for the full year to decrease due to rising inflation and covid-19 restrictions being lifted.

LCD TV sales are expected to fall while OLED TV sales are expected to rise as the TV market is shifting towards the newer and more advanced display technology.

https://pulsenews.co.kr/view.php?year=2019&no=985982

- **UHD - Ultra High Definition (3840×2160) - 4K (2014)**

UHD, ultra high resolution, 3840×2160, popularily known as 4K, appeared in 2014 for high-end monitors but also laptops, using scaling is a MUST. They require HDMI or DisplayPort. Sony Xperia Z5 Premium (2015) was first smartphone with 4K screen, having 806 pixels per inch. 

https://www.teoalida.com/webdesign/screen-resolution/

- **Plasma decline and demise (2010-2015)**

At the 2010 Consumer Electronics Show in Las Vegas, Panasonic introduced their 152" 2160p 3D plasma. In 2010, Panasonic shipped 19.1 million plasma TV panels.[73]

In 2010, the shipments of plasma TVs reached 18.2 million units globally.[74] Since that time, shipments of plasma TVs have declined substantially. This decline has been attributed to the competition from liquid crystal (LCD) televisions, whose prices have fallen more rapidly than those of the plasma TVs.[75] In late 2013, Panasonic announced that they would stop producing plasma TVs from March 2014 onwards.[76] In 2014, LG and Samsung discontinued plasma TV production as well,[77][78] effectively killing the technology, probably because of lowering demand.

- **Sony Triluminos - QLED - Quantum dot enhancement layer - QD-enhanced TVs (2013) - Sony**

A widespread practical application is using quantum dot enhancement film (QDEF) layer to improve the LED backlighting in LCD TVs. Light from a blue LED backlight is converted by QDs to relatively pure red and green, so that this combination of blue, green and red light incurs less blue-green crosstalk and light absorption in the color filters after the LCD screen, thereby increasing useful light throughput and providing a better color gamut.

The first manufacturer shipping TVs of this kind was Sony in 2013 as Triluminos, Sony's trademark for the technology.[15] At the Consumer Electronics Show 2015, Samsung Electronics, LG Electronics, TCL Corporation and Sony showed QD-enhanced LED-backlighting of LCD TVs.[16][17][18] At the CES 2017, Samsung rebranded their 'SUHD' TVs as 'QLED'; later in April 2017, Samsung formed the QLED Alliance with Hisense and TCL to produce and market QD-enhanced TVs.[19][20]

Quantum dot on glass (QDOG) replaces QD film with a thin QD layer coated on top of the light-guide plate (LGP), reducing costs and improving efficiency.[21][22]

Traditional white LED backlights that use blue LEDs with on-chip or on-rail red-green QD structures are being researched, though high operating temperatures negatively affect their lifespan.[23][24]

https://en.wikipedia.org/wiki/Quantum_dot_display

QLED or quantum dot LED is a flat panel display technology introduced by Samsung under this trademark. Other television set manufacturers such as Sony have used the same technology to enhance the backlighting of LCD TVs already in 2013.[27][28] Quantum dots create their own unique light when illuminated by a light source of shorter wavelength such as blue LEDs. This type of LED TV enhances the colour gamut of LCD panels, where the image is still generated by the LCD. In the view of Samsung, quantum dot displays for large-screen TVs are expected to become more popular than the OLED displays in the coming years; Firms like Nanoco and Nanosys compete to provide the QD materials. In the meantime, Samsung Galaxy devices such as smartphones are still equipped with OLED displays manufactured by Samsung as well. Samsung explains on their website that the QLED TV they produce can determine what part of the display needs more or less contrast. Samsung also announced a partnership with Microsoft that will promote the new Samsung QLED TV.

https://en.wikipedia.org/wiki/Flat-panel_display

- **High contrast LCD displays (2016) - Panasonic**

In 2016, Panasonic developed IPS LCDs with a contrast ratio of 1,000,000:1, rivaling OLEDs. This technology was later put into mass production as dual layer, dual panel or LMCL (Light Modulating Cell Layer) LCDs. The technology uses 2 liquid crystal layers instead of one, and may be used along with a mini-LED backlight and quantum dot sheets.[98][99][100][101][102][103]

https://en.wikipedia.org/wiki/Liquid-crystal_display

- **first mobile phones with 8K video recording capabilities (2019)**

In May 2019, mobile phone vendors started releasing the first mobile phones with 8K video recording capabilities, such as the ZTE Nubia Red Magic 3 series.

https://en.wikipedia.org/wiki/8K_resolution#Resolutions

### Broadcast
**[`^        back to top        ^`](#)**

- **12K video on Youtube and Vimeo (2010)**

YouTube, since 2010,[62] and Vimeo allow a maximum upload resolution of 4096 × 3072 pixels (12.6 megapixels, aspect ratio 4:3).

https://en.wikipedia.org/wiki/4K_resolution

- **First 4K broadcast in North America (2013) - BulbTV**

On April 11, 2013, Bulb TV created by Canadian serial entrepreneur Evan Kosiner became the first broadcaster to provide a 4K linear channel and VOD content to cable and satellite companies in North America.

https://en.wikipedia.org/wiki/4K_resolution

- **4K on VOD - Netflix, Amazon (2014)**

In 2014, Netflix began streaming House of Cards, Breaking Bad,[82] and "some nature documentaries" at 4K to compatible televisions with an HEVC decoder. Most 4K televisions sold in 2013 did not natively support HEVC, with most major manufacturers announcing support in 2014.[83] Amazon Studios began shooting their full-length original series and new pilots with 4K resolution in 2014.[84] They are now currently available though Amazon Video.[85]

https://en.wikipedia.org/wiki/4K_resolution



### Video games
**[`^        back to top        ^`](#)**

- **XBox One - Shift to x86 and AMD architecture (2013) - Microsoft**

The third console, the Xbox One, was released in November 2013 and has sold 51 million units.

https://en.wikipedia.org/wiki/Xbox

Moving away from its predecessor's PowerPC-based architecture, the Xbox One marks a shift back to the x86 architecture used in the original Xbox; it features an AMD Accelerated Processing Unit (APU) built around the x86-64 instruction set. Xbox One's controller was redesigned over the Xbox 360's, with a redesigned body, D-pad, and triggers capable of delivering directional haptic feedback. The console places an increased emphasis on cloud computing, as well as social networking features and the ability to record and share video clips or screenshots from gameplay or livestream directly to streaming services such as Mixer and Twitch. Games can also be played off-console via a local area network on supported Windows 10 devices. The console can play Blu-ray Disc, and overlay live television programming from an existing set-top box or a digital tuner for digital terrestrial television with an enhanced program guide. The console optionally included a redesigned Kinect sensor, marketed as the "Kinect 2.0", providing improved motion tracking and voice recognition.
Blu-ray, DVD, CD, Digital distribution

https://en.wikipedia.org/wiki/Xbox_One

- **Sony PlayStation 4 (2013) - Sony**

Sony's next console, the PlayStation 4, was released in 2013, selling a million units within a day, becoming the fastest selling console in history

https://en.wikipedia.org/wiki/PlayStation

The PlayStation 4 (PS4) is a home video game console developed by Sony Computer Entertainment. Announced as the successor to the PlayStation 3 in February 2013, it was launched on November 15, 2013, in North America, November 29, 2013 in Europe, South America and Australia, and on February 22, 2014 in Japan. A console of the eighth generation, it competes with Microsoft's Xbox One and Nintendo's Wii U and Switch.

Moving away from the more complex Cell microarchitecture of its predecessor, the console features an AMD Accelerated Processing Unit (APU) built upon the x86-64 architecture, which can theoretically peak at 1.84 teraflops; AMD stated that it was the "most powerful" APU it had developed to date. The PlayStation 4 places an increased emphasis on social interaction and integration with other devices and services, including the ability to play games off-console on PlayStation Vita and other supported devices ("Remote Play"), the ability to stream gameplay online or to friends, with them controlling gameplay remotely ("Share Play"). The console's controller was also redesigned and improved over the PlayStation 3, with improved buttons and analog sticks, and an integrated touchpad among other changes. The console also supports HDR10 High-dynamic-range video and playback of 4K resolution multimedia.

The PlayStation 4 was released to critical acclaim, with critics praising Sony for acknowledging its consumers' needs, embracing independent game development, and for not imposing the restrictive digital rights management schemes like those originally announced by Microsoft for the Xbox One. Critics and third-party studios, before its launch, also praised the capabilities of the PlayStation 4 in comparison to its competitors; developers described the performance difference between the console and Xbox One as "significant" and "obvious". Heightened demand also helped Sony top global console sales. By October 2019, PS4 became the second-best-selling home game console of all time, behind the PlayStation 2.

Its read-only optical drive is capable of reading Blu-ray Discs at speeds of up to three times that of its predecessor.[56][62] The console features a hardware on-the-fly zlib decompression module.[60] The original PS4 model supports up to 1080p and 1080i video standards,[63] while the Pro model supports 4K resolution.[64] The console includes a 500 gigabyte hard drive for additional storage,[65] which can be upgraded by the user.[66] System Software 4.50, which was released on March 9, 2017,[67] enabled the use of external USB hard drives up to 8 TB for additional storage.[68]

The PlayStation 4 features Wi-Fi and Ethernet connectivity, Bluetooth, and two USB 3.0 ports.[23][56] An auxiliary port is also included for connection to the PlayStation Camera, a motion detection digital camera device first introduced on the PS3.[23] A mono headset, which can be plugged into the DualShock 4, is bundled with the system.[69] Audio/video output options include HDMI TV and optical S/PDIF audio.[23] The console does not have an analog audio/video output.[70]

The PS4 features a "Rest mode" feature. This places the console in a low-power state, while allowing users to immediately resume their game or app once the console is awoken. The console also is able to download content such as game and OS updates while it is in this state.[71][72]

**PlayStation VR is a virtual reality** system for PlayStation 4; it consists of a headset, which features a 1080p display panel, LED lights on the headset that are used by PlayStation Camera to track its motion, and a control box that processes 3D audio effects, as well as video output to the external display (either simulcasting the player's VR perspective, or providing an asymmetrical secondary perspective). PlayStation VR can also be used with PlayStation Move motion controllers.[90][91]

https://en.wikipedia.org/wiki/PlayStation_4

## Standards and protocols
**[`^        back to top        ^`](#)**

### Hardware layer

- **IEEE Std 1901-2010 - Broadband via electric power lines (2010)**

a standard for high speed (up to 500 Mbit/s at the physical layer) communication devices via electric power lines, often called broadband over power lines (BPL).[1] The standard uses transmission frequencies below 100 MHz. This standard is usable by all classes of BPL devices, including BPL devices used for the connection (<1500m to the premises) to Internet access services as well as BPL devices used within buildings for local area networks, smart energy applications, transportation platforms (vehicle), and other data distribution applications (<100m between devices).[2]

https://en.wikipedia.org/wiki/IEEE_1901

- **802.3az	-	Energy-Efficient Ethernet (2010)**

In computer networking, Energy-Efficient Ethernet (EEE) is a set of enhancements to twisted-pair, twinaxial, backplane, and optical fiber Ethernet physical-layer variants that reduce power consumption during periods of low data activity.[1] The intention is to reduce power consumption by 50% or more, while retaining full compatibility with existing equipment.[2]

The Institute of Electrical and Electronics Engineers (IEEE), through the IEEE 802.3az task force, developed the standard. The first study group had its call for interest in November 2006, and the official standards task force was authorized in May 2007.[3] The IEEE ratified the final standard in September 2010.[4] Some companies introduced technology to reduce the power required for Ethernet before the standard was ratified, using the name Green Ethernet.

Some energy-efficient switch integrated circuits were developed before the IEEE 802.3az Energy-Efficient Ethernet standard was finalized.[5][6]

https://en.wikipedia.org/wiki/Energy-Efficient_Ethernet

- **40GbE / 100 GbE (2010-2018)**

  * 802.3ba	(2010-06):	40 Gbit/s and 100 Gbit/s Ethernet. 40 Gbit/s over 1 m backplane, 10 m Cu cable assembly (4×25 Gbit or 10×10 Gbit lanes) and 100 m of MMF and 100 Gbit/s up to 10 m of Cu cable assembly, 100 m of MMF or 40 km of SMF respectively
  * 802.3bg	(2011-03):	Provide a 40 Gbit/s PMD which is optically compatible with existing carrier SMF 40 Gbit/s client interfaces (OTU3/STM-256/OC-768/40G POS).
  * 802.3bj	(2014-06):	Define a 4-lane 100 Gbit/s backplane PHY for operation over links consistent with copper traces on "improved FR-4" (as defined by IEEE P802.3ap or better materials to be defined by the Task Force) with lengths up to at least 1 m and a 4-lane 100 Gbit/s PHY for operation over links consistent with copper twinaxial cables with lengths up to at least 5 m.
  * 802.3bm	(2015-02):	100G/40G Ethernet for optical fiber
  * 802.3cd	(2018-12):	Media Access Control Parameters for 50 Gbit/s and Physical Layers and Management Parameters for 50, 100, and 200 Gbit/s Operation

https://en.wikipedia.org/wiki/IEEE_802.3

- **802.3bz	- 2.5GBASE-T and 5GBASE-T (2013-2016)**

2.5 Gigabit and 5 Gigabit Ethernet over Cat-5e/Cat-6 twisted pair.

IEEE 802.3bz, NBASE-T and MGBASE-T are standards for Ethernet over twisted pair at speeds of 2.5 and 5 Gbit/s. These use the same cabling as the ubiquitous Gigabit Ethernet, yet offer higher speeds. The resulting standards are named 2.5GBASE-T and 5GBASE-T.[1][2][3]

NBASE-T refers to Ethernet equipment that can automatically negotiate to operate at speeds of 100 Mbit/s, 1, 2.5, 5, or 10 Gbit/s, depending on the quality of the cable and the capabilities of the equipment at the other end of the cable.

As faster Wi-Fi protocols such as IEEE 802.11ac were developed, there was significant demand for cheap uplinks faster than 1000BASE-T. These speeds became relevant around 2014 as it became clear that it would not be possible to run 10GBASE-T over already widely deployed Cat5e cable. IEEE 802.3bz also supports power over Ethernet, which had previously not been available at 10GBASE-T.

As early as 2013, the Intel Avoton server processors integrated 2.5 Gbit/s Ethernet ports.

Whilst Broadcom had announced a series of 2.5 Gbit/s transceiver ICs,[10] 2.5 Gbit/s switch hardware was not widely commercially available at that point. Many early 10GBASE-T switches, particularly those with SFP+ interfaces, do not support the intermediate speeds.

In October 2014, the NBASE-T Alliance was founded,[11][12] initially comprising Cisco, Aquantia, Freescale, and Xilinx. By December 2015, it contained more than 45 companies, and aimed to have its specification compatible with 802.3bz.[13] The competing MGBASE-T Alliance, stating the same faster Gigabit Ethernet objectives, was founded in December 2014.[14] In contrast to NBASE-T, the MGBASE-T said that their specifications would be open source.[15] IEEE 802.3's "2.5G/5GBASE-T Task Force" started working on the 2.5GBASE-T and 5GBASE-T standards in March 2015.[16] The two NBASE-T and MGBASE-T Alliances ended up collaborating.[17] with the forming of the IEEE 802.3bz Task Force under the patronage of the Ethernet Alliance in June 2015.

On September 23, 2016, the IEEE-SA Standards Board approved IEEE Std 802.3bz-2016.[18]

https://en.wikipedia.org/wiki/2.5GBASE-T_and_5GBASE-T

- **Terabit Ethernet road**

200 Gbit/s over single-mode fiber and 400 Gbit/s over optical physical media.

https://en.wikipedia.org/wiki/IEEE_802.3

The IEEE formed the "IEEE 802.3 Industry Connections Ethernet Bandwidth Assessment Ad Hoc", to investigate the business needs for short and long term bandwidth requirements.[16][17][18]

IEEE 802.3's "400 Gb/s Ethernet Study Group" started working on the 400 Gbit/s generation standard in March 2013.[19] Results from the study group were published and approved on March 27, 2014. Subsequently, the IEEE 802.3bs Task Force[20] started working to provide physical layer specifications for several link distances.[21]

The IEEE 802.3bs standard was approved on December 6, 2017[4] and is available online.[22]
802.3bs	2017-12	200GbE (200 Gbit/s) over single-mode fiber and 400GbE (400 Gbit/s) over optical physical media

The IEEE 802.3cd standard was approved on December 5, 2018.
802.3cd	2018-12	Media Access Control Parameters for 50 Gbit/s and Physical Layers and Management Parameters for 50, 100, and 200 Gbit/s Operation

The IEEE 802.3cn standard was approved on December 20, 2019.
802.3cn	2019-11	50 Gbit/s (40 km), 100 Gbit/s (80 km), 200 Gbit/s (four λ, 40 km), and 400 Gbit/s (eight λ, 40 km and single λ, 80 km over DWDM) over Single-Mode Fiber and DWDM

https://en.wikipedia.org/wiki/Terabit_Ethernet

### Application layer

- **HTTP/2 (2015)**

HTTP/2 (originally named HTTP/2.0) is a major revision of the HTTP network protocol used by the World Wide Web. It was derived from the earlier experimental SPDY protocol, originally developed by Google.[1][2] HTTP/2 was developed by the HTTP Working Group (also called httpbis, where "bis" means "twice") of the Internet Engineering Task Force (IETF).[3][4][5] HTTP/2 is the first new version of HTTP since HTTP/1.1, which was standardized in RFC 2068 in 1997. The Working Group presented HTTP/2 to the Internet Engineering Steering Group (IESG) for consideration as a Proposed Standard in December 2014,[6][7] and IESG approved it to publish as Proposed Standard on February 17, 2015 (and was updated in February 2020 in regard to TLS 1.3).[8][9] The HTTP/2 specification was published as RFC 7540 on May 14, 2015.[10]

The standardization effort was supported by Chrome, Opera, Firefox,[11] Internet Explorer 11, Safari, Amazon Silk, and Edge browsers.[12] Most major browsers had added HTTP/2 support by the end of 2015.[13] About 97% of web browsers used have the capability.[14] As of October 2021, 47% (after topping out at just over 50%) of the top 10 million websites supported HTTP/2.[15]

https://en.wikipedia.org/wiki/HTTP/2

### File compression

- **Snappy - High speed compression/decompression but lower compression (2011) - Google**

Snappy (previously known as Zippy) is a fast data compression and decompression library written in C++ by Google based on ideas from LZ77 and open-sourced in 2011.[2][3] It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. Compression speed is 250 MB/s and decompression speed is 500 MB/s using a single core of a circa 2011 "Westmere" 2.26 GHz Core i7 processor running in 64-bit mode. The compression ratio is 20–100% lower than gzip.[4]

Snappy is widely used in Google projects like Bigtable, MapReduce and in compressing data for Google's internal RPC systems. It can be used in open-source projects like MariaDB ColumnStore,[5] Cassandra, Couchbase, Hadoop, LevelDB, MongoDB, RocksDB, Lucene, Spark, and InfluxDB.[6] Decompression is tested to detect any errors in the compressed stream. Snappy does not use inline assembler (except some optimizations[7]) and is portable.

https://en.wikipedia.org/wiki/Snappy_(compression)

- **Zstandard, zstd - (2015) - Facebook**
Zstandard, commonly known by the name of its reference implementation zstd, is a lossless data compression algorithm developed by Yann Collet at Facebook. Zstd is the reference implementation in C. Version 1 of this implementation was released as open-source software on 31 August 2016.[3][4]

Zstandard was designed to give a compression ratio comparable to that of the DEFLATE algorithm (developed in 1991 and used in the original ZIP and gzip programs), but faster, especially for decompression. It is tunable with compression levels ranging from negative 7 (fastest)[5] to 22 (slowest in compression speed, but best compression ratio).

The Linux kernel has included Zstandard since November 2017 (version 4.14) as a compression method for the btrfs and squashfs filesystems.[16][17][18]

In 2017, Allan Jude integrated Zstandard into the FreeBSD kernel,[19] and it was subsequently integrated as a compressor option for core dumps (both user programs and kernel panics). It was also used to create a proof-of-concept OpenZFS compression method[7] which was integrated in 2020.[20]

The AWS Redshift and RocksDB databases include support for field compression using Zstandard.[21]

https://en.wikipedia.org/wiki/Zstd

## Programming languages and frameworks
**[`^        back to top        ^`](#)**

- **Microsoft .NET framework  - extension of patents grants and Mono part of .NET Foundation (2000-2019) - Microsoft**

In November 2014, Microsoft also produced an update to its patent grants, which further extends the scope beyond its prior pledges. Prior projects like Mono existed in a legal grey area because Microsoft's earlier grants applied only to the technology in "covered specifications", including strictly the 4th editions each of ECMA-334 and ECMA-335. The new patent promise, however, places no ceiling on the specification version, and even extends to any .NET runtime technologies documented on MSDN that have not been formally specified by the ECMA group, if a project chooses to implement them. This allows Mono and other projects to maintain feature parity with modern .NET features that have been introduced since the 4th edition was published without being at risk of patent litigation over the implementation of those features. The new grant does maintain the restriction that any implementation must maintain minimum compliance with the mandatory parts of the CLI specification.[11]

On March 31, 2016, Microsoft announced at Microsoft Build that they will completely relicense Mono under an MIT License even in scenarios where formerly a commercial license was needed.[12] Microsoft also supplemented its prior patent promise for Mono, stating that they will not assert any "applicable patents" against parties that are "using, selling, offering for sale, importing, or distributing Mono."[13][14] It was announced that the Mono Project was contributed to the .NET Foundation. These developments followed the acquisition of Xamarin, which began in February 2016 and was finished on March 18, 2016.[15]

In April 2019, Microsoft released .NET Framework 4.8, the last version of the framework as a proprietary offering. Only monthly security and reliability bug fixes to that version have been released since then. No further changes to that version are planned.[3]

https://en.wikipedia.org/wiki/.NET_Framework

- **Rust - System programming language with memory safety (2010)**

Rust is a multi-paradigm, general-purpose programming language. Rust emphasizes performance, type safety, and concurrency.[9][10][11] Rust enforces memory safety—that is, that all references point to valid memory—without requiring the use of a garbage collector or reference counting present in other memory-safe languages.[11][12] To simultaneously enforce memory safety and prevent concurrent data races, Rust's borrow checker tracks the object lifetime and variable scope of all references in a program during compilation.[13] Rust is popular for systems programming[11] but also offers high-level features including functional programming constructs.[14]

Software developer Graydon Hoare designed Rust while working at Mozilla Research in 2006.[15] Mozilla officially sponsored the project in 2009, and the designers refined the language while writing the Servo experimental browser engine[16] and the Rust compiler. Rust's major influences include SML, OCaml, C++, Cyclone, Haskell, and Erlang.[4] Since the first stable release in January 2014, Rust has been adopted by companies including Amazon, Discord, Dropbox, Facebook (Meta), Google (Alphabet), and Microsoft.

Rust has been noted for its growth as a newer language[10][17] and has been the subject of academic programming languages research.[18][19][20][11]

Rust aims to support concurrent systems programming, which has inspired a feature set with an emphasis on safety, control of memory layout, and concurrency.[57]

Rust grew out of a personal project begun in 2006 by Mozilla employee Graydon Hoare. Mozilla began sponsoring the project in 2009 and officially announced the project in 2010.[15][21] During the same year, work had shifted from the initial compiler written in OCaml to a self-hosting compiler based on LLVM written in Rust. The new Rust compiler, named rustc, successfully compiled itself in 2011. The first numbered pre-alpha version of the compiler, Rust 0.1, was released in January 2012.[22]

Rust's type system changed considerably between versions 0.2, 0.3, and 0.4. Version 0.2 introduced classes for the first time,[23] and version 0.3 added destructors and polymorphism through the use of interfaces.[24] In Rust 0.4, traits were added as a means to provide inheritance; interfaces were unified with traits and removed as a separate feature. Classes were also removed and replaced by a combination of implementations and structured types.[25] Along with conventional static typing, before version 0.4, Rust also supported typestate analysis through contracts. It was removed in release 0.4, though the same functionality can be achieved by leveraging Rust's type system.[26]

In January 2014, the editor-in-chief of Dr. Dobb's Journal, Andrew Binstock, commented on Rust's chances of becoming a competitor to C++ in addition to the languages D, Go, and Nim (then Nimrod). According to Binstock, while Rust was "widely viewed as a remarkably elegant language", adoption slowed because it repeatedly changed between versions.[27] The first stable release, Rust 1.0, was announced on May 15, 2015.[28][29]

In August 2020, Mozilla laid off 250 of its 1,000 employees worldwide as part of a corporate restructuring caused by the long-term impact of the COVID-19 pandemic.[30][31] The team behind Servo, a browser engine written in Rust, was completely disbanded. The event raised concerns about the future of Rust, as some members of the team were active contributors to Rust.[32] In the following week, the Rust Core Team acknowledged the severe impact of the layoffs and announced that plans for a Rust foundation were underway. The first goal of the foundation would be to take ownership of all trademarks and domain names, and take financial responsibility for their costs.[33]

On February 8, 2021, the formation of the Rust Foundation was announced by its five founding companies (AWS, Huawei, Google, Microsoft, and Mozilla).[34][35] In a blog post published on April 6, 2021, Google announced support for Rust within Android Open Source Project as an alternative to C/C++.[36][37]

On November 22, 2021, the Moderation team, responsible for enforcing community standards and the Code of Conduct, announced their resignation "in protest of the Core Team placing themselves unaccountable to anyone but themselves." Although no further comments regarding the reasons for the resignation has been provided in the announcements, one of the former members of the Moderation team commented on Reddit, stating that "[c]ommunication with Core has failed, there's no team above Core, so... it's up to the members of the Rust Project to organize themselves and decide what to do in term of follow-up."[38]

Rust aims "to be as efficient and portable as idiomatic C++, without sacrificing safety".[87] Rust does not perform garbage collection, which allows it to be more efficient and performant than other memory-safe languages.[88]

https://en.wikipedia.org/wiki/Rust_(programming_language)

- **Kotlin - Android app developers language, alternative to Java, compiles to Javascript (2011)**

Kotlin (/ˈkɒtlɪn/)[2] is a cross-platform, statically typed, general-purpose programming language with type inference. Kotlin is designed to interoperate fully with Java, and the JVM version of Kotlin's standard library depends on the Java Class Library,[3][failed verification] but type inference allows its syntax to be more concise. Kotlin mainly targets the JVM, but also compiles to JavaScript (e.g., for frontend web applications using React[4]) or native code via LLVM (e.g., for native iOS apps sharing business logic with Android apps).[5] Language development costs are borne by JetBrains, while the Kotlin Foundation protects the Kotlin trademark.[6]

On 7 May 2019, Google announced that the Kotlin programming language is now its preferred language for Android app developers.[7] Since the release of Android Studio 3.0 in October 2017, Kotlin has been included as an alternative to the standard Java compiler. The Android Kotlin compiler produces Java 8 bytecode by default (which runs in any later JVM), but lets the programmer choose to target Java 9 up to 18, for optimization,[8] or allows for more features; has bidirectional record class interoperability support for JVM, introduced in Java 16, considered stable as of Kotlin 1.5.

Kotlin has support for the web; by compiling to JavaScript (i.e., Kotlin/JS with the classic back-end, is declared stable since version 1.3), while the newer Kotlin/JS (IR-based) is in beta as of version 1.5.30. Kotlin/Native (for e.g. Apple silicon support) is considered beta since version 1.3.[9][10]

In July 2011, JetBrains unveiled Project Kotlin, a new language for the JVM, which had been under development for a year.[11] JetBrains lead Dmitry Jemerov said that most languages did not have the features they were looking for, with the exception of Scala. However, he cited the slow compilation time of Scala as a deficiency.[11] One of the stated goals of Kotlin is to compile as quickly as Java. In February 2012, JetBrains open sourced the project under the Apache 2 license.[12]

Development lead Andrey Breslav has said that Kotlin is designed to be an industrial-strength object-oriented language, and a "better language" than Java, but still be fully interoperable with Java code, allowing companies to make a gradual migration from Java to Kotlin.[24]

https://en.wikipedia.org/wiki/Kotlin_(programming_language)

- **Dart - a tentative Javascript alternative (2011)**

Dart is a programming language designed for client development,[8][9] such as for the web and mobile apps. It is developed by Google and can also be used to build server and desktop applications.

It is an object-oriented, class-based, garbage-collected language with C-style syntax.[10] It can compile to either native code or JavaScript, and supports interfaces, mixins, abstract classes, reified generics and type inference.[11]

Dart was unveiled at the GOTO conference in Aarhus, Denmark, October 10–12, 2011.[12] The project was founded by Lars Bak and Kasper Lund.[13] Dart 1.0 was released on November 14, 2013.[14]

Dart initially had a mixed reception and the Dart initiative has been criticized by some for fragmenting the web, due to the original plans to include a Dart VM in Chrome. Those plans were dropped in 2015 with the 1.9 release of Dart to focus instead on compiling Dart to JavaScript.[15]

https://en.wikipedia.org/wiki/Dart_(programming_language)

- **Elixir - a language compatible with the Erlang ecosystem (2012)**

Elixir is a functional, concurrent, general-purpose programming language that runs on the BEAM virtual machine which is also used to implement the Erlang programming language.[2] Elixir builds on top of Erlang and shares the same abstractions for building distributed, fault-tolerant applications. Elixir also provides productive tooling and an extensible design. The latter is supported by compile-time metaprogramming with macros and polymorphism via protocols.[3]

Elixir is used by companies such as Ramp,[4] PagerDuty,[5] Discord,[6] Brex,[7] E-MetroTel,[8] Pinterest,[9] Moz,[10] Bleacher Report,[11] The Outline,[12] Inverse,[13] Divvy,[14] FarmBot[15] and for building embedded systems.[16][17] The community organizes yearly events in the United States,[18] Europe,[19] and Japan,[20] as well as minor local events and conferences.[21][22]

José Valim is the creator of the Elixir programming language, a research and development project created at Plataformatec. His goals were to enable higher extensibility and productivity in the Erlang VM while keeping compatibility with Erlang's ecosystem.[23][24]

Elixir was aimed for large-scale sites and apps. Elixir uses features of Ruby, Erlang, and Clojure to develop a "high-concurrency" and "low-latency" language. Elixir was designed to handle large data volumes. Elixir is used in telecommunication, eCommerce, and finance industries.[25]

https://en.wikipedia.org/wiki/Elixir_(programming_language)

- **Elm - domain-specific programming language for declaratively creating web browser-based GUI (2012)**

Elm is a domain-specific programming language for declaratively creating web browser-based graphical user interfaces. Elm is purely functional, and is developed with emphasis on usability, performance, and robustness. It advertises "no runtime exceptions in practice",[6] made possible by the Elm compiler's static type checking.

Elm was initially designed by Evan Czaplicki as his thesis in 2012.[7] The first release of Elm came with many examples and an online editor that made it easy to try out in a web browser.[8] Evan joined Prezi in 2013 to work on Elm,[9] and in 2016 moved to NoRedInk as an Open Source Engineer, also starting the Elm Software Foundation.[10]

The initial implementation of the Elm compiler targets HTML, CSS, and JavaScript.[11] The set of core tools has continued to expand, now including a REPL,[12] package manager,[13] time-travelling debugger,[14] and installers for macOS and Windows.[15] Elm also has an ecosystem of community created libraries and Ellie, an advanced online editor that allows saved work and inclusion of community libraries.

Elm has a small set of language constructs, including traditional if-expressions, let-expressions for local state, and case-expressions for pattern matching.[16] As a functional language, it supports anonymous functions, functions as arguments, and functions can return functions, the latter often by partial application of curried functions. Functions are called by value. Its semantics include immutable values, stateless functions, and static typing with type inference. Elm programs render HTML through a virtual DOM, and may interoperate with other code by using "JavaScript as a service".

Elm does not support higher-kinded polymorphism,[26] which related languages Haskell and PureScript offer, nor does Elm support the creation of type classes.

This means that, for example, Elm does not have a generic map function which works across multiple data structures such as List and Set. In Elm, such functions are typically invoked qualified by their module name, for example calling List.map and Set.map. In Haskell or PureScript, there would be only one function map. This is a known feature request that is on Czaplicki's rough roadmap since at least 2015.[27]

Another outcome is a large amount of boilerplate code in medium to large size projects as illustrated by the author of "Elm in Action" in their single page application example[28] with almost identical fragments being repeated in update, view, subscriptions, route parsing and building functions.

https://en.wikipedia.org/wiki/Elm_(programming_language)

- **Julia - high-level, high-performance, dynamic programming language, numerical analysis and computational science (2012)**

Julia is a high-level, high-performance, dynamic programming language. While it is a general-purpose language and can be used to write any application, many of its features are well suited for numerical analysis and computational science.[24][25][26][27]

Distinctive aspects of Julia's design include a type system with parametric polymorphism in a dynamic programming language; with multiple dispatch as its core programming paradigm. Julia supports concurrent, (composable) parallel and distributed computing (with or without using MPI[28] or the built-in corresponding[clarification needed][29] to "OpenMP-style" threads[30]), and direct calling of C and Fortran libraries without glue code. Julia uses a just-in-time (JIT) compiler that is referred to as "just-ahead-of-time" (JAOT) in the Julia community, as Julia compiles all code (by default) to machine code before running it.[31][32]

Julia is garbage-collected,[33] uses eager evaluation, and includes efficient libraries for floating-point calculations, linear algebra, random number generation, and regular expression matching. Many libraries are available, including some (e.g., for fast Fourier transforms) that were previously bundled with Julia and are now separate.[34]

Several development tools support coding in Julia, such as integrated development environments (e.g. for Microsoft's Visual Studio Code, an extension is available[35] providing debugging and linting support); with integrated tools, e.g. a profiler (and flame graph support available[36][37] for the built-in one), debugger,[38] and the Rebugger.jl package "supports repeated-execution debugging"[a] and more.[40]

Julia has attracted some high-profile users, from investment manager BlackRock, which uses it for time-series analytics, to the British insurer Aviva, which uses it for risk calculations. Since 2015, the Federal Reserve Bank of New York has used Julia to make models of the United States economy (including estimating COVID-19 shocks in 2021[65]), noting that the language made model estimation "about 10 times faster" than its previous MATLAB implementation. At the 2017 JuliaCon[66] conference, Jeffrey Regier, Keno Fischer and others announced[67] that the Celeste project[68] used Julia to achieve "peak performance of 1.54 petaFLOPS using 1.3 million threads"[69] on 9300 Knights Landing (KNL) nodes of the Cori II (Cray XC40) supercomputer (then 6th fastest computer in the world).[70] Julia thus joins C, C++, and Fortran as high-level languages in which petaFLOPS computations have been achieved.

Three of the Julia co-creators are the recipients of the 2019 James H. Wilkinson Prize for Numerical Software (awarded every four years) "for the creation of Julia, an innovative environment for the creation of high-performance tools that enable the analysis and solution of computational science problems."[71] Also, Alan Edelman, professor of applied mathematics at MIT, has been selected to receive the 2019 IEEE Computer Society Sidney Fernbach Award "for outstanding breakthroughs in high-performance computing, linear algebra, and computational science and for contributions to the Julia programming language."[72]

In 2019, Julia Computing announced "the availability of the Julia programming language as a pre-packaged container on the NVIDIA GPU Cloud (NGC) container registry"[73] and a blog post at Nvidia's site states "Easily Deploy Julia on x86 and Arm [..] Julia offers a package for a comprehensive HPC ecosystem covering machine learning, data science, various scientific domains and visualization."[74]

Additionally, "Julia was selected by the Climate Modeling Alliance as the sole implementation language for their next generation global climate model. This multi-million dollar project aims to build an earth-scale climate model providing insight into the effects and challenges of climate change."[73]

Julia is used by NASA, e.g. for modeling spacecraft separation dynamics (15,000 times faster than before with Simulink/MATLAB[75])[76][77][78] and the Brazilian INPE for space mission planning and satellite simulation.[79] Another effort is working on an embedded project to control a satellite in space using Julia for attitude control.[citation needed]

Julia is used at CERN[80][81][82] for the Large Hadron Collider (LHCb experiment).[83][84][85]

https://en.wikipedia.org/wiki/Julia_(programming_language)


- **Typescript - Javascript transpiller with static types, replaced Coffeescript (2012)**

TypeScript is a free and open source programming language developed and maintained by Microsoft. It is a strict syntactical superset of JavaScript and adds optional static typing to the language. It is designed for the development of large applications and transpiles to JavaScript.[5] As it is a superset of JavaScript, existing JavaScript programs are also valid TypeScript programs.

TypeScript may be used to develop JavaScript applications for both client-side and server-side execution (as with Node.js or Deno). Multiple options are available for transpilation. The default TypeScript Compiler can be used,[6] or the Babel compiler can be invoked to convert TypeScript to JavaScript.

TypeScript supports definition files that can contain type information of existing JavaScript libraries, much like C++ header files can describe the structure of existing object files. This enables other programs to use the values defined in the files as if they were statically typed TypeScript entities. There are third-party header files for popular libraries such as jQuery, MongoDB, and D3.js. TypeScript headers for the Node.js basic modules are also available, allowing development of Node.js programs within TypeScript.[7]

The TypeScript compiler is itself written in TypeScript and compiled to JavaScript. It is licensed under the Apache License 2.0. TypeScript is included as a first-class programming language in Microsoft Visual Studio 2013 Update 2 and later, alongside C# and other Microsoft languages.[8] An official extension also allows Visual Studio 2012 to support TypeScript.[9] Anders Hejlsberg, lead architect of C# and creator of Delphi and Turbo Pascal, has worked on the development of TypeScript.[10][11][12][13]

TypeScript was first made public in October 2012 (at version 0.8), after two years of internal development at Microsoft.[14][15] Soon after the announcement, Miguel de Icaza praised the language itself, but criticized the lack of mature IDE support apart from Microsoft Visual Studio, which was not available on Linux and OS X at that time.[16][17] As of April 2021 there is support in other IDEs and text editors, including Emacs, Vim, Webstorm, Atom[18] and Microsoft's own Visual Studio Code.[19]

TypeScript 0.9, released in 2013, added support for generics.[20] TypeScript 1.0 was released at Microsoft's Build developer conference in 2014.[21] Visual Studio 2013 Update 2 provides built-in support for TypeScript.[22]

In July 2014, the development team announced a new TypeScript compiler, claiming 5× performance gains. Simultaneously, the source code, which was initially hosted on CodePlex, was moved to GitHub.[23]

On 22 September 2016, TypeScript 2.0 was released; it introduced several features, including the ability for programmers to optionally prevent variables from being assigned null values,[24] sometimes referred to as the billion-dollar mistake.

TypeScript 3.0 was released on 30 July 2018,[25] bringing many language additions like tuples in rest parameters and spread expressions, rest parameters with tuple types, generic rest parameters and so on.[26]

TypeScript 4.0 was released on 20 August 2020.[27] While 4.0 did not introduce any breaking changes, it added language features such as Custom JSX Factories and Variadic Tuple Types.[27]

https://en.wikipedia.org/wiki/TypeScript

- **Swift - Apple's replacement of Objective-C (2014)**

Development of Swift started in July 2010 by Chris Lattner, with the eventual collaboration of many other programmers at Apple. Swift took language ideas "from Objective-C, Rust, Haskell, Ruby, Python, C#, CLU, and far too many others to list".[7] On June 2, 2014, the Apple Worldwide Developers Conference (WWDC) application became the first publicly released app written with Swift.[23] A beta version of the programming language was released to registered Apple developers at the conference, but the company did not promise that the final version of Swift would be source code compatible with the test version. Apple planned to make source code converters available if needed for the full release.[23]

The Swift Programming Language, a free 500-page manual, was also released at WWDC, and is available on the Apple Books Store and the official website.[24]

Swift reached the 1.0 milestone on September 9, 2014, with the Gold Master of Xcode 6.0 for iOS.[25] Swift 1.1 was released on October 22, 2014, alongside the launch of Xcode 6.1.[26] Swift 1.2 was released on April 8, 2015, along with Xcode 6.3.[27] Swift 2.0 was announced at WWDC 2015, and was made available for publishing apps in the App Store in September 21, 2015.[28] Swift 3.0 was released on September 13, 2016.[29] Swift 4.0 was released on September 19, 2017.[30] Swift 4.1 was released on March 29, 2018.[31]

Swift won first place for Most Loved Programming Language in the Stack Overflow Developer Survey 2015[32] and second place in 2016.[33]

Swift is a general-purpose, multi-paradigm, compiled programming language developed by Apple Inc. and the open-source community. First released in 2014, Swift was developed as a replacement for Apple's earlier programming language Objective-C, as Objective-C had been largely unchanged since the early 1980s and lacked modern language features. Swift works with Apple's Cocoa and Cocoa Touch frameworks, and a key aspect of Swift's design was the ability to interoperate with the huge body of existing Objective-C code developed for Apple products over the previous decades. It was built with the open source LLVM compiler framework and has been included in Xcode since version 6, released in 2014. On Apple platforms,[10] it uses the Objective-C runtime library, which allows C, Objective-C, C++ and Swift code to run within one program.[11]

Apple intended Swift to support many core concepts associated with Objective-C, notably dynamic dispatch, widespread late binding, extensible programming and similar features, but in a "safer" way, making it easier to catch software bugs; Swift has features addressing some common programming errors like null pointer dereferencing and provides syntactic sugar to help avoid the pyramid of doom. Swift supports the concept of protocol extensibility, an extensibility system that can be applied to types, structs and classes, which Apple promotes as a real change in programming paradigms they term "protocol-oriented programming"[12] (similar to traits).[13]

Swift was introduced at Apple's 2014 Worldwide Developers Conference (WWDC).[14] It underwent an upgrade to version 1.2 during 2014 and a major upgrade to Swift 2 at WWDC 2015. Initially a proprietary language, version 2.2 was made open-source software under the Apache License 2.0 on December 3, 2015, for Apple's platforms and Linux.[15][16]

Through version 3.0 the syntax of Swift went through significant evolution, with the core team making source stability a focus in later versions.[17][18] In the first quarter of 2018 Swift surpassed Objective-C in measured popularity.[19]

Swift 4.0, released in 2017, introduced several changes to some built-in classes and structures. Code written with previous versions of Swift can be updated using the migration functionality built into Xcode. Swift 5, released in March 2019, introduced a stable binary interface on Apple platforms, allowing the Swift runtime to be incorporated into Apple operating systems. It is source compatible with Swift 4.[20]

Swift 5.1 was officially released in September 2019. Swift 5.1 builds on the previous version of Swift 5 by extending the stable features of the language to compile-time with the introduction of module stability. The introduction of module stability makes it possible to create and share binary frameworks that will work with future releases of Swift.[21]

Swift 5.5, officially announced by Apple at the 2021 WWDC, significantly expands language support for concurrency and asynchronous code, notably introducing a unique version of the actor model.[22]

https://en.wikipedia.org/wiki/Swift_(programming_language)

- **Vulkan - OpenGL successor (2015) - Khronos, AMD, DICE**

Vulkan is a low-overhead, cross-platform API, open standard for 3D graphics and computing.[16][17][18] Vulkan targets high-performance real-time 3D graphics applications, such as video games and interactive media. Vulkan is intended to offer higher performance and more efficient CPU and GPU usage compared to older OpenGL and Direct3D 11 APIs. It provides a considerably lower-level API for the application than the older APIs, making Vulkan comparable to Apple's Metal API and Microsoft's Direct3D 12. In addition to its lower CPU usage, Vulkan is designed to allow developers to better distribute work among multiple CPU cores.[19]

Vulkan was first announced by the non-profit Khronos Group at GDC 2015.[14][20][21] The Vulkan API was initially referred to as the "next generation OpenGL initiative", or "OpenGL next"[22] by Khronos, but use of those names was discontinued when Vulkan was announced.[23]

Vulkan is derived from and built upon components of AMD's Mantle API, which was donated by AMD to Khronos with the intent of giving Khronos a foundation on which to begin developing a low-level API that they could standardize across the industry.[14]

Vulkan is not backwards compatible with OpenGL,[24][18][note 1] although there exists within Mesa an implementation of OpenGL/GLES that runs on top of Vulkan, called Zink.[25]

https://en.wikipedia.org/wiki/Vulkan

- **Microsoft .NET core - cross-platform open-source successor to .NET Framework (2016-2019)**

.NET (pronounced as "dot net"; previously named .NET Core) is a free and open-source, managed computer software framework for Windows, Linux, and macOS operating systems.[3] It is a cross-platform[4] successor to .NET Framework.[5] The project is primarily developed by Microsoft employees by way of the .NET Foundation, and released under the MIT License.[2]

On November 12, 2014, Microsoft announced .NET Core, in an effort to include cross-platform support for .NET, including Linux and macOS, source for the .NET Core CoreCLR implementation, source for the "entire [...] library stack" for .NET Core, and the adoption of a conventional ("bazaar"-like) open-source development model under the stewardship of the .NET Foundation. Miguel de Icaza describes .NET Core as a "redesigned version of .NET that is based on the simplified version of the class libraries",[6] and Microsoft's Immo Landwerth explained that .NET Core would be "the foundation of all future .NET platforms". At the time of the announcement, the initial release of the .NET Core project had been seeded with a subset of the libraries' source code and coincided with the relicensing of Microsoft's existing .NET reference source away from the restrictions of the Ms-RSL. Landwerth acknowledged the disadvantages of the formerly selected shared license, explaining that it made codename Rotor "a non-starter" as a community-developed open source project because it did not meet the criteria of an Open Source Initiative (OSI) approved license.[7][8][9]

.NET Core 1.0 was released on June 27, 2016,[10] along with Microsoft Visual Studio 2015 Update 3, which enables .NET Core development.[11] .NET Core 1.0.4 and .NET Core 1.1.1 were released along with .NET Core Tools 1.0 and Visual Studio 2017 on March 7, 2017.[12]

.NET Core 2.0 was released on August 14, 2017, along with Visual Studio 2017 15.3, ASP.NET Core 2.0, and Entity Framework Core 2.0.[13] .NET Core 2.1 was released on May 30, 2018.[14] NET Core 2.2 was released on December 4, 2018.[15]

.NET Core 3 was released on September 23, 2019.[16] .NET Core 3 adds support for Windows desktop application development[17] and significant performance improvements throughout the base library.

https://en.wikipedia.org/wiki/.NET

- **Solidity - language for the block chain to write smart contracts (2018)**
 
Solidity is an object-oriented programming language for implementing smart contracts[2][3] on various blockchain platforms, most notably, Ethereum.[4] It was developed by Christian Reitwiessner, Alex Beregszaszi, and several former Ethereum core contributors.[5] Programs in Solidity run on Ethereum Virtual Machine.

Solidity was proposed in August 2014 by Gavin Wood;[6][non-primary source needed] the language was later developed by the Ethereum project's Solidity team, led by Christian Reitwiessner.

Solidity is the primary language on Ethereum as well as on other private blockchains, such as the enterprise-oriented Hyperledger Fabric blockchain. SWIFT deployed a proof of concept using Solidity running on Hyperledger Fabric.[7][8]

Many security properties of smart contracts are inherently difficult to reason about directly, and the Turing-completeness of Solidity means that verification of arbitrary properties cannot be decidably automated. Current automated solutions for smart contract security analysis can miss critical violations, produce false positives, and fail to achieve sufficient code coverage on realistic contracts.[20] Solidity has been blamed for the error-prone implementation of Ethereum smart contracts due to its counterintuitive nature, its lack of constructs to deal with blockchain domain-specific aspects, and its lack of centralized documentation of known vulnerabilities.[21]

In 2016, a Cornell University researcher stated that Solidity was partially to blame for The DAO hack that took place that year. He stated: "this was actually not a flaw or exploit in the DAO contract itself: technically the Ethereum Virtual Machine (EVM) was operating as intended, but Solidity was introducing security flaws into contracts that were not only missed by the community, but missed by the designers of the language t

Unlike programs in traditional programming languages, which can be debugged, in Solidity contracts mistakes cannot be edited or fixed; transactions cannot be reversed. Solidity follows the "Code is Law" mantra, which means any smart contract must be flawlessly coded when it comes into effect.

There have been some hacking cases such as the aforementioned 2016 DAO hack in which US$60 million was stolen, and a 2021 hack that caused a fork in the Ethereum system.

To prevent technical errors and mistakes, Coinbase, the largest cryptocurrency exchange in the US, introduced a new tool named Solidify. This tool is an AI auditing system that detects and classifies smart contract risks.

https://en.wikipedia.org/wiki/Solidity

## Navigation
**[`^        back to top        ^`](#)**

- **U.S. GPS modernization**
On January 11, 2010, an update of ground control systems caused a software incompatibility with 8,000 to 10,000 military receivers manufactured by a division of Trimble Navigation Limited of Sunnyvale, Calif.[citation needed][67]

On February 25, 2010,[68] the U.S. Air Force awarded the contract[citation needed] to develop the GPS Next Generation Operational Control System (OCX) to improve accuracy and availability of GPS navigation signals, and serve as a critical part of GPS modernization.

https://en.wikipedia.org/wiki/Global_Positioning_System

- **Europe's Galileo goes live (2016)**

Galileo is a global navigation satellite system (GNSS) that went live in 2016,[5] created by the European Union through the European Space Agency (ESA), operated by the European Union Agency for the Space Programme (EUSPA),[6] headquartered in Prague, Czech Republic,[7] with two ground operations centres in Fucino, Italy, and Oberpfaffenhofen, Germany. The €10 billion project[4][8] is named after the Italian astronomer Galileo Galilei. One of the aims of Galileo is to provide an independent high-precision positioning system so European political and military authorities do not have to rely on the US GPS, or the Russian GLONASS systems, which could be disabled or degraded by their operators at any time.[9] The use of basic (lower-precision) Galileo services is free and open to everyone. A fully encrypted higher-precision service is available for free to government-authorized users.[10][11] Galileo is intended to provide horizontal and vertical position measurements within 1-metre (3 ft 3 in) precision, and better positioning services at higher latitudes than other positioning systems. Galileo is also to provide a new global search and rescue (SAR) function as part of the MEOSAR system.

https://github.com/dheurtev/dheurtev/edit/main/IT-HISTORY/2010-2019.md











