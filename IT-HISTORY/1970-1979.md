# 1970s

**In short:**
- Digital phone
- Proprietary networking
- TCP
- First long distance fiber optic network
- birth of modern cryptography
- First CPUs
- Motherboard : Backplane system
- dot matrix and inkjet printers, development of laser printers
- First PCs
- storage from cassettes to 5"1/4
- expansion of satellite TV
- arcade video games
- first and second generation video games consoles
- Standards : X.25, FTP, TAR compression
- Navigation : GPS design and launch

## Telecommunications

- **Conversion of the Bell backbone from analog to digital, Digital leased lines**

Leased line services (or private line services) became digital in the 1970s with the conversion of the Bell backbone network from analog to digital circuits. This allowed AT&T to offer Dataphone Digital Services (later re-branded digital data services) that started the deployment of ISDN and T1 lines to customer premises to connect

https://en.wikipedia.org/wiki/Leased_line

## Networking
- **Proprietary protocol suites**

Computer manufacturers developed **proprietary protocol suites** such as IBM's Systems Network Architecture (SNA), Digital Equipment Corporation's DECnet, and Xerox's Xerox Network Systems (XNS). During the late 1970s and most of the 1980s, there remained a lack of open networking options. Therefore, proprietary standards, particularly SNA and DECnet, as well as some variants of XNS (e.g., Novell NetWare and Banyan VINES), were commonly used on private networks, becoming somewhat "de facto" industry standards.

https://en.wikipedia.org/wiki/Protocol_Wars

In 1976, John Murphy of Datapoint Corporation created ARCNET, a token-passing network first used to share storage devices.
In 1977, Xerox Network Systems (XNS) was developed by Robert Metcalfe and Yogen Dalal at Xerox.[24]

https://en.wikipedia.org/wiki/Computer_network

- **First long distance fiber network (1977)**

In 1977, the first long-distance fiber network was deployed by GTE in Long Beach, California.

https://en.wikipedia.org/wiki/Computer_network

- **X.25 standard**

The X.25 standard gained political support in European countries and from the European Economic Community (EEC). For example, the European Informatics Network, which was based on datagrams was replaced with Euronet based on X.25.[38] Peter Kirstein wrote that European networks tended to be short-term projects with smaller numbers of computers and users. As a result, the European networking activities did not lead to any strong standards except X.25 which became the main European data protocol for fifteen to twenty years. 

https://en.wikipedia.org/wiki/Protocol_Wars

## Cryptography

- **DES - symmetric key (1975-1977)**

The Data Encryption Standard (DES /ˌdiːˌiːˈɛs, dɛz/) is a symmetric-key algorithm for the encryption of digital data. Although its short key length of 56 bits makes it too insecure for modern applications, it has been highly influential in the advancement of cryptography.
Developed in the early 1970s at IBM and based on an earlier design by Horst Feistel, the algorithm was submitted to the National Bureau of Standards (NBS) following the agency's invitation to propose a candidate for the protection of sensitive, unclassified electronic government data. In 1976, after consultation with the National Security Agency (NSA), the NBS selected a slightly modified version (strengthened against differential cryptanalysis, but weakened against brute-force attacks), which was published as an official Federal Information Processing Standard (FIPS) for the United States in 1977.[2]

https://en.wikipedia.org/wiki/Data_Encryption_Standard

- **asymmetric public-private key (1976) - Whitfield Diffie and Martin Hellman**

The idea of an asymmetric public-private key cryptosystem is attributed to Whitfield Diffie and Martin Hellman, who published this concept in 1976.

https://en.wikipedia.org/wiki/RSA_(cryptosystem)

- **RSA (1977) - Rivest–Shamir–Adleman**

a public-key cryptosystem that is widely used for secure data transmission. It is also one of the oldest. The acronym "RSA" comes from the surnames of Ron Rivest, Adi Shamir and Leonard Adleman, who publicly described the algorithm in 1977. Patented in 1983.

https://en.wikipedia.org/wiki/RSA_(cryptosystem)

## Computers

### Evolution of mainframes

By the early 1970s, many mainframes acquired **interactive user terminals**[NB 1] operating as **timesharing computers**, supporting **hundreds of users simultaneously along with batch processing**. Users gained access through **keyboard/typewriter terminals** and **specialized text terminal CRT displays with integral keyboards**, or **later from personal computers equipped with terminal emulation software**

https://en.wikipedia.org/wiki/Mainframe_computer

### Evolution of hardware

- **IBM/7 - semiconductor memory (1970-1971) - IBM**

The IBM System/7 was a computer system designed for industrial control, announced on October 28, 1970 and first shipped in 1971. It was a 16-bit machine and one of the first made by IBM to use novel semiconductor memory, instead of magnetic core memory conventional at that date

https://en.wikipedia.org/wiki/IBM_System/7

- **Datapoint 2200 (1970) - Datapoint**

Released in June 1970, the programmable terminal called the Datapoint 2200 is among the **earliest known devices that bears significant resemblance to the modern personal computer, with a CRT screen, keyboard, programmability, and program storage**

https://en.wikipedia.org/wiki/History_of_personal_computers

- **Dot matrix printer (1970) - Centronics**

An Wang, Robert Howard and Prentice Robinson began development of a low-cost printer at Centronics, a subsidiary of Wang Laboratories that produced specialty computer terminals. The printer used the dot matrix printing principle, with a print head consisting of a vertical row of seven metal pins connected to solenoids. When power was applied to the solenoids, the pin was pushed forward to strike the paper and leave a dot. To make a complete character glyph, the print head would receive power to specified pins to create a single vertical pattern, then the print head would move to the right by a small amount, and the process repeated. On their original design, a typical glyph was printed as a matrix seven high and five wide, while the "A" models used a print head with 9 pins and formed glyphs that were 9 by 7.[2]
This left the problem of sending the ASCII data to the printer. While a serial port does so with the minimum of pins and wires, it requires the device to buffer up the data as it arrives bit by bit and turn it back into multi-bit values. A parallel port makes this simpler; the entire ASCII value is presented on the pins in complete form. In addition to the eight data pins, the system also needed various control pins as well as electrical grounds. Wang happened to have a surplus stock of 20,000 Amphenol 36-pin micro ribbon connectors that were originally used for one of their early calculators. The interface only required 21 of these pins, the rest were grounded or not connected. The connector has become so closely associated with Centronics that it is now popularly known as the "Centronics connector".[3]
The Centronics Model 101 printer, featuring this connector, was released in 1970.[3]
The printer side of the interface quickly became an industry de facto standard, but manufacturers used various connectors on the system side, so a variety of cables were required. For example, NCR used the 36-pin micro ribbon connector on both ends of the connection, early VAX systems used a DC-37 connector, Texas Instruments used a 25-pin card edge connector and Data General used a 50-pin micro ribbon connector. When IBM implemented the parallel interface on the IBM PC, they used the DB-25F connector at the PC-end of the interface, creating the now familiar parallel cable with a DB25M at one end and a 36-pin micro ribbon connector at the other.

https://en.wikipedia.org/wiki/Parallel_port

- **Intel Intel 1702A 2K (256 x 8) UV Erasable PROM - First EPROM (1971) - Intel**

An EPROM (rarely EROM), or erasable programmable read-only memory, is a type of programmable read-only memory (PROM) chip that retains its data when its power supply is switched off. Computer memory that can retrieve stored data after a power supply has been turned off and back on is called non-volatile. It is an array of floating-gate transistors individually programmed by an electronic device that supplies higher voltages than those normally used in digital circuits. Once programmed, an EPROM can be erased by exposing it to strong ultraviolet light source (such as from a mercury-vapor lamp). EPROMs are easily recognizable by the transparent fused quartz (or on later models resin) window on the top of the package, through which the silicon chip is visible, and which permits exposure to ultraviolet light during erasing

https://en.wikipedia.org/wiki/EPROM

- **Intel 4004 - Fist CPU (1971) - Intel** 

The Intel 4004 was the world's first microprocessor—a complete general-purpose CPU on a single chip

https://en.wikipedia.org/wiki/Intel_4004

When the first general purpose microprocessor was introduced in 1971 (Intel 4004) it immediately began chipping away at the low end of the computer market, replacing embedded minicomputers in many industrial devices.

https://en.wikipedia.org/wiki/Commodity_computing

- **EEPROM - electrically re-programmable non-volatile memory (1972) Toshiba**
In 1972, a type of electrically re-programmable non-volatile memory was invented by Fujio Masuoka at Toshiba, who is also known as the inventor of flash memory.[12] Most of the major semiconductor manufactures, such as Toshiba,[12][6] Sanyo (later, ON Semiconductor),[13] IBM,[14] Intel,[15][16] NEC (later, Renesas Electronics),[17] Philips (later, NXP Semiconductors),[18] Siemens (later, Infineon Technologies),[19] Honeywell (later, Atmel),[20] Texas Instruments,[21] studied, invented, and manufactured some electrically re-programmable non-volatile devices until 1977.
https://en.wikipedia.org/wiki/EEPROM

- **Winchester - disk platter hard drive (1973) - IBM**

In 1973, IBM introduced a new type of HDD code-named "Winchester". Its primary distinguishing feature was that the disk heads were not withdrawn completely from the stack of disk platters when the drive was powered down. Instead, the heads were allowed to "land" on a special area of the disk surface upon spin-down, "taking off" again when the disk was later powered on. This greatly reduced the cost of the head actuator mechanism, but precluded removing just the disks from the drive as was done with the disk packs of the day. Instead, the first models of "Winchester technology" drives featured a removable disk module, which included both the disk pack and the head assembly, leaving the actuator motor in the drive upon removal. Later "Winchester" drives abandoned the removable media concept and returned to non-removable platters.

https://en.wikipedia.org/wiki/Hard_disk_drive

- **Xerox Alto - First computers designed for individual use (1973) - Xerox**

The Xerox Alto was one of the first computers designed for individual use in 1973 and is regarded as the **first modern computer to use a mouse**.[48] Inspired by PARC's Alto, the Lilith, a computer which had been developed by a team around Niklaus Wirth at ETH Zürich between 1978 and 1980, provided a mouse as well. 

https://en.wikipedia.org/wiki/Computer_mouse

In the 1970s, Engelbart's ideas were further refined and extended to graphics by researchers at Xerox PARC and specifically Alan Kay, who went beyond text-based hyperlinks and used a GUI as the main interface for the Smalltalk programming language, which ran on the Xerox Alto computer, released in 1973. Most modern general-purpose GUIs are derived from this system.

https://en.wikipedia.org/wiki/Graphical_user_interface

- **Interdata 7/32 and 8/32 - first 32-bit minicomputers under $10,000 (1973) - Interdata** 

Interdata computers are primarily remembered for being the first 32-bit minicomputers under $10,000 
The 7/32 and 8/32 became the computers of choice in large scale embedded systems, such as FFT machines used in real-time seismic analysis, CAT scanners, and flight simulator systems. They were also often used as non-IBM peripherals in IBM networks, serving the role of HASP workstations and spooling systems, so called RJE (Remote Job Entry) stations. For example, the computers behind the first Space Shuttle simulator consisted of thirty-six 32-bit minis inputting and/or outputting data to networked mainframe computers (both IBM and Univac), all in real-time.

https://en.wikipedia.org/wiki/Interdata_7/32_and_8/32

- **Swinging arm actuator (1974) - IBM**

In 1974 IBM introduced the swinging arm actuator, made feasible because the Winchester recording heads function well when skewed to the recorded tracks. The simple design of the IBM GV (Gulliver) drive,[44] invented at IBM's UK Hursley Labs, became IBM's most licensed electro-mechanical invention[45] of all time, the actuator and filtration system being adopted in the 1980s eventually for all HDDs, and still universal nearly 40 years and 10 Billion arms later.

https://en.wikipedia.org/wiki/Hard_disk_drive

- **DIN connector (1974)**

The DIN connector is an electrical connector that was originally standardized in the early 1970s[1] by the Deutsches Institut für Normung (DIN), the German national standards organization. There are DIN standards for various different connectors.

https://en.wikipedia.org/wiki/DIN_connector

- **Bios (1975) - CP/M operating system - Digital Research, Inc.**

In computing, BIOS (/ˈbaɪɒs, -oʊs/, BY-oss, -⁠ohss; Basic Input/Output System (also known as the System BIOS, ROM BIOS, BIOS ROM or PC BIOS) is firmware used to provide runtime services for operating systems and programs and to perform hardware initialization during the booting process (power-on startup).[1] The BIOS firmware comes pre-installed on an IBM PC or IBM PC compatible's system board and exists in UEFI-based systems too.[2][3] The name originates from the Basic Input/Output System used in the CP/M operating system in 1975.[4][5] The BIOS originally proprietary to the IBM PC has been reverse engineered by some companies (such as Phoenix Technologies) looking to create compatible systems. The interface of that original system serves as a de facto standard.

https://en.wikipedia.org/wiki/BIOS
https://en.wikipedia.org/wiki/CP/M

- **Apple II - First commodity-like microcomputer, uses a 5 1/4 floppy disk(1977) - Apple**

By 1976, there were several firms racing to introduce the first truly successful commercial personal computers**. Three machines, the Apple II, PET 2001 and TRS-80 were all released in 1977,[34] becoming the most popular by late 1978.[35] Byte magazine later referred to Commodore, Apple, and Tandy as the "1977 Trinity".[36] Also in 1977, Sord Computer Corporation released the Sord M200 Smart Home Computer in Japan.[37]

  * **Apple** is founded in 1976.

https://en.wikipedia.org/wiki/Apple_Inc

While early Apple II models use ordinary cassette tapes as storage devices, they were superseded in 1978 by the introduction of a **5+1⁄4-inch floppy disk drive** and interface called the Disk II.

https://en.wikipedia.org/wiki/Commodity_computing

https://en.wikipedia.org/wiki/Apple_II

- **TV screens used as computer screens**

Many personal computers introduced in the late 1970s and the 1980s were designed to use television receivers as their display devices, making the resolutions dependent on the television standards in use, including PAL and NTSC. Picture sizes were usually limited to ensure the visibility of all the pixels in the major television standards and the broad range of television sets with varying amounts of over scan. The actual drawable picture area was, therefore, somewhat smaller than the whole screen, and was usually surrounded by a static-colored border (see image to right). Also, the interlace scanning was usually omitted in order to provide more stability to the picture, effectively halving the vertical resolution in progress. 160 × 200, 320 × 200 and 640 × 200 on NTSC were relatively common resolutions in the era (224, 240 or 256 scanlines were also common). In the IBM PC world, these resolutions came to be used by 16-color EGA video cards.

One of the drawbacks of using a classic television is that the computer display resolution is higher than the television could decode. Chroma resolution for NTSC/PAL televisions are bandwidth-limited to a maximum 1.5 MHz, or approximately 160 pixels wide, which led to blurring of the color for 320- or 640-wide signals, and made text difficult to read (see example image below). Many users upgraded to higher-quality televisions with S-Video or RGBI inputs that helped eliminate chroma blur and produce more legible displays. The earliest, lowest cost solution to the chroma problem was offered in the Atari 2600 Video Computer System and the Apple II+, both of which offered the option to disable the color and view a legacy black-and-white signal. On the Commodore 64, the GEOS mirrored the Mac OS method of using black-and-white to improve readability.

https://en.wikipedia.org/wiki/Display_resolution

- **PERQ - first commercially available computer with a GUI (1979) - Three Rivers Computer Corporation (3RCC)**

The first commercially available computer with a GUI was 1979 PERQ workstation, manufactured by Three Rivers Computer Corporation. Its design was heavily influenced by the work at Xerox PARC.

https://en.wikipedia.org/wiki/Graphical_user_interface

The design was heavily influenced by the original workstation computer, the Xerox Alto, which was never commercially produced.

https://en.wikipedia.org/wiki/PERQ

- **Storage on cassettes (late 70s, early 80s)**

Many late 1970s and early 1980s home computers used Compact Cassettes, encoded with the Kansas City standard, or alternate encodings. Modern cartridge formats include LTO, DLT, and DAT/DDC.

https://en.wikipedia.org/wiki/Magnetic_tape

- **Motherboards : Backplane system (1970s)**

Prior to the invention of the microprocessor, the digital computer consisted of multiple printed circuit boards in a card-cage case with components connected by a backplane, a set of interconnected sockets. In very old designs, copper wires were the discrete connections between card connector pins, but printed circuit boards soon became the standard practice. The central processing unit (CPU), memory, and peripherals were housed on individually printed circuit boards, which were plugged into the backplane. The ubiquitous S-100 bus of the 1970s is an example of this type of backplane system.

https://en.wikipedia.org/wiki/Motherboard

- **Inkjet printers**

In the late 1970s, inkjet printers that could reproduce digital images generated by computers were developed, mainly by Epson, Hewlett-Packard (HP) and Canon. In the worldwide consumer market, four manufacturers account for the majority of inkjet printer sales: Canon, HP, Epson and Brother.[5]

https://en.wikipedia.org/wiki/Inkjet_printing

- **Development of laser printers (1970s) - Xerox, IBM, Canon**
In the 1960s, the Xerox Corporation held a dominant position in the photocopier market.[2] In 1969, Gary Starkweather, who worked in Xerox's product development department, had the idea of using a laser beam to "draw" an image of what was to be copied directly onto the copier drum. After transferring to the recently formed Palo Alto Research Center (Xerox PARC) in 1971, Starkweather adapted a Xerox 7000 copier to make SLOT (Scanned Laser Output Terminal). In 1972, Starkweather worked with Butler Lampson and Ronald Rider to add a control system and character generator, resulting in a printer called EARS (Ethernet, Alto Research character generator, Scanned laser output terminal)—which later became the Xerox 9700 laser printer.[3][4][5]

1976: The **first commercial implementation of a laser printer**, the **IBM 3800**, was released. It was designed for data centers, where it replaced line printers attached to mainframe computers. The IBM 3800 was used for high-volume printing on continuous stationery, and achieved speeds of 215 pages per minute (ppm), at a resolution of 240 dots per inch (dpi). Over 8,000 of these printers were sold.[6]

1977: The Xerox 9700 was brought to market. Unlike the IBM 3800, the Xerox 9700 was not targeted to replace any particular existing printers; however, it did have limited support for the loading of fonts. The Xerox 9700 excelled at printing high-value documents on cut-sheet paper with varying content (e.g. insurance policies).[6]

**1979**: Inspired by the Xerox 9700's commercial success, Japanese camera and optics company Canon developed the **Canon LBP-10, a low-cost desktop laser printer**. Canon then began work on a much-improved print engine, the Canon CX, resulting in the LBP-CX printer. Having no experience in selling to computer users, Canon sought partnerships with three Silicon Valley companies: Diablo Data Systems (who rejected the offer), Hewlett-Packard (HP), and Apple Computer.[7][8]

https://en.wikipedia.org/wiki/Laser_printing

### Evolution of uses

#### Servers ####
- **File Transfer Protocol (1971)**

https://en.wikipedia.org/wiki/File_Transfer_Protocol

#### Clients ####

- **Bravo - First WYSIWYG document preparation program (1974) - Xerox Parc**

It provided multi-font capability using the bitmap displays on the Xerox Alto personal computer. It was produced at Xerox PARC by Butler Lampson, Charles Simonyi and colleagues in 1974.

https://en.wikipedia.org/wiki/Bravo_(editor)

The first version of Microsoft Word was developed by Charles Simonyi and Richard Brodie, former Xerox programmers hired by Bill Gates and Paul Allen in 1981. Both programmers worked on Xerox Bravo, the first WYSIWYG (What You See Is What You Get) word processor.

https://en.wikipedia.org/wiki/History_of_Microsoft_Word

- **Visicalc - First spreadsheet (1979) - Software Arts**

With the development of the VisiCalc application in 1979, microcomputers broke out of the factory and began entering office suites in large quantities, but still through the back door.
Appeared on Apple II.

https://en.wikipedia.org/wiki/Commodity_computing

https://en.wikipedia.org/wiki/VisiCalc

https://en.wikipedia.org/wiki/Software_Arts

## Consummer electronics

### TV
- **Discrete Cosine Transform (DCT) video compression (1972) - Nassir Ahmed**

Digital HDTV was made possible by the development of discrete cosine transform (DCT) video compression.[25][23] DCT coding is a lossy image compression technique that was first proposed by Nasir Ahmed in 1972,

- **Anik1 - First North American commercial satelitte to carry tv tranmissions (1972)**

The first commercial North American satellite to carry television transmissions was Canada's geostationary Anik 1, which was launched on 9 November 1972.[48] ATS-6, the world's first experimental educational and direct broadcast satellite (DBS), was launched on 30 May 1974.[49] It transmitted at 860 MHz using wideband FM modulation and had two sound channels. The transmissions were focused on the Indian subcontinent but experimenters were able to receive the signal in Western Europe using home constructed equipment that drew on UHF television design techniques already in use.[50]

The first in a series of Soviet geostationary satellites to carry direct-to-home television, Ekran 1, was launched on 26 October 1976.[51] It used a 714 MHz UHF downlink frequency so that the transmissions could be received with existing UHF television technology rather than microwave technology.[52]

https://en.wikipedia.org/wiki/Satellite_television

- **Beginning of the satellite TV industry (1976–1980)**

The satellite television industry developed first in the US from the cable television industry as communication satellites were being used to distribute television programming to remote cable television headends. Home Box Office (HBO), Turner Broadcasting System (TBS), and Christian Broadcasting Network (CBN, later The Family Channel) were among the first to use satellite television to deliver programming. Taylor Howard of San Andreas, California, became the first person to receive C-band satellite signals with his home-built system in 1976.[53]

In the US, PBS, a non-profit public broadcasting service, began to distribute its television programming by satellite in 1978.[54]

In 1979, Soviet engineers developed the Moskva (or Moscow) system of broadcasting and delivering of TV signals via satellites. They launched the Gorizont communication satellites later that same year. These satellites used geostationary orbits.[55] They were equipped with powerful on-board transponders, so the size of receiving parabolic antennas of downlink stations was reduced to 4 and 2.5 metres.[55] On October 18, 1979, the Federal Communications Commission (FCC) began allowing people to have home satellite earth stations without a federal government license.[56] The front cover of the 1979 Neiman-Marcus Christmas catalogue featured the first home satellite TV stations on sale for $36,500.[57] The dishes were nearly 20 feet (6.1 m) in diameter[58] and were remote controlled.[59] The price went down by half soon after that, but there were only eight more channels.[60] The Society for Private and Commercial Earth Stations (SPACE), an organisation which represented consumers and satellite TV system owners, was established in 1980.[61]

Early satellite television systems were not very popular due to their expense and large dish size.[62] The satellite television dishes of the systems in the late 1970s and early 1980s were 10 to 16 feet (3.0 to 4.9 m) in diameter,[63] made of fibreglass or solid aluminum or steel,[64] and in the United States cost more than $5,000, sometimes as much as $10,000.[65] Programming sent from ground stations was relayed from eighteen satellites in geostationary orbit located 22,300 miles (35,900 km) above the Earth.[66][67]

By 1980, satellite television was well established in the USA and Europe.

https://en.wikipedia.org/wiki/Satellite_television

### Video games

- **Magnavox Odyssey - Arcade video games (1972) - Sanders Associates / Magnavox**

In 1966, while working at Sanders Associates, Ralph Baer came up with an idea for an entertainment device that could be hooked up to a television monitor. Presenting this to his superiors at Sanders and getting their approval, he along with William Harrison and William Rusch refined Baer's concept into the "Brown Box" prototype of a home video game console that could play a simple table tennis game. The three patented the technology, and Sanders, not in the business of commercialization, sold licenses to the patents to Magnavox to commercialize. With Baer's help, Magnavox developed the Magnavox Odyssey, the first commercial home console, in 1972.

https://en.wikipedia.org/wiki/History_of_video_games

- **Pong - influencial arcade video game (1972) - Atari**

Concurrently, Nolan Bushnell and Ted Dabney had the idea of making a coin-operated cabinet housing a small, low-cost microcomputer to run Spacewar! By 1971, the two had developed Computer Space with Nutting Associates, the first recognized arcade video game.[7] Bushnell and Dabney struck out on their own and formed Atari. Bushnell, inspired by the table tennis game on the Odyssey, hired Allan Alcorn to develop an arcade version of the game, this time using discrete transistor–transistor logic (TTL) electronic circuitry. Atari's Pong was released in late 1972 and is considered the first successful arcade video game. It ignited the growth of the arcade game industry in the United States from both established coin-operated game manufacturers like Williams, Chicago Coin, and the Midway subsidiary of Bally Manufacturing, and new startups such as Ramtek and Allied Leisure. Many of these were Pong clones using ball-and-paddle controls, and led to saturation of the market in 1974, forcing arcade game makers to try to innovate new games in 1975. Many of the newer companies created in the wake of Pong failed to innovate on their own and shut down, and by the end of 1975, the arcade market had fallen by about 50% based on new game sale revenues.[8] Further, Magnavox took Atari and several other of these arcade game makers to court over violations of Baer's patents. Bushnell settled the suit for Atari, gaining perpetual rights for the patents for Atari as part of the settlement.[9] Others failed to settle, and Magnavox won around $100 million in damages from these patent infringement suits before the patents expired in 1990.[10]

Arcade video games caught on quickly in Japan due to partnerships between American and Japanese corporations that kept the Japan companies abreast of technology developments within the United States. The Nakamura Amusement Machine Manufacturing Company (Namco) partnered with Atari to import Pong into Japan in late 1973. Within the year, Taito and Sega released Pong clones in Japan by mid-1973. Japanese companies began developing novel games and exporting or licensing them through partners in 1974.[11] Among these included Taito's Gun Fight (originally Western Gun in its Japanese release), which was licensed to Midway. Midway's version, released in 1975, was the first arcade video game to use a microprocessor rather than discrete TLL components.[12] This innovation drastically reduced the complexity and time to design of arcade games and the number of physical components required to achieve more advanced gameplay.[13]

https://en.wikipedia.org/wiki/History_of_video_games

- **First generation of video games consoles (1972-1983)**

In the history of video games, the first-generation era refers to the video games, video game consoles, and handheld video game consoles available from 1972 to 1983. Notable consoles of the first generation include the **Odyssey series** (excluding the Magnavox Odyssey 2), the **Atari Home Pong**,[1] the **Coleco Telstar series** and the Color TV-Game series. The generation ended with the Computer TV-Game in 1980, but many manufacturers had left the market prior due to the market decline in 1977 and the start of the second generation of video game consoles.

https://en.wikipedia.org/wiki/First_generation_of_video_game_consoles

- **Dedicated console market (1975-1978)**

The Magnavox Odyssey never caught on with the public, due largely to the limited functionality of its primitive discrete electronic component technology.[8] By mid-1975 large-scale integration (LSI) microchips had become inexpensive enough to be incorporated into a consumer product.[8] In 1975, Magnavox reduced the part count of the Odyssey using a three-chip set created by Texas Instruments and released two new systems that only played ball-and-paddle games, the Magnavox Odyssey 100 and Magnavox Odyssey 200. Atari, meanwhile, entered the consumer market that same year with the single-chip Home Pong system. The next year, General Instrument released a "Pong-on-a-chip" LSI and made it available at a low price to any interested company. Toy company Coleco Industries used this chip to create the million-selling Telstar console model series (1976–77).

These initial home video game consoles were popular, leading to a large influx of companies releasing Pong and other video game clones to satisfy consumer demand. While there were only seven companies that were releasing home consoles in 1975, there were at least 82 by 1977, with more than 160 different models that year alone that were easily documented. A large number of these consoles were created in East Asia, and it is estimated that over 500 Pong-type home console models were made during this period.[8] As with the prior paddle-and-ball saturation in the arcade game field by 1975 due to consumer weariness, dedicated console sales dropped sharply in 1978, disrupted by the introduction of programmable systems and Handheld electronic games.[8]

https://en.wikipedia.org/wiki/History_of_video_games

- **Fairchild Channel F - Introduction of cartridge-based home consoles (1976) - Fairchild Camera and Instrument**

Development costs of dedicated game hardware for arcade and home consoles based on discrete component circuitry and application-specific integrated circuits (ASICs) with only limited consumer lifespans drove engineers to find alternatives. Microprocessors had dropped far enough in price by 1975 to make these a viable option for developing programmable consoles that could load in game software from a form of swappable media.[20]

The Fairchild Channel F by Fairchild Camera and Instrument was released in 1976. It is the first home console to use programmable ROM cartridges - allowing players to swap games - as well as being the first home console to use a microprocessor which read instructions from the ROM cartridge. Atari and Magnavox followed suit in 1977 with the release of the Atari Video Computer System (VCS, later known as the Atari 2600) and the Magnavox Odyssey 2, both systems also introducing the use of cartridges. As to complete the Atari VCS quickly, Bushnell sold Atari to Warner Communications $28 million, providing the necessary cash infusion to complete the system's design by the end of 1977.[13] The initial market for these new consoles were initially modest as consumers were still wary after the saturation of dedicated home consoles.[21] 

https://en.wikipedia.org/wiki/History_of_video_games

- **Second generation video games consoles (1976-1992)**
In the history of video games, the second-generation era refers to computer and video games, video game consoles, and handheld video game consoles available from 1976 to 1992. Notable platforms of the second generation include the **Fairchild Channel F**, **Atari 2600**, **Intellivision**, **Odyssey 2**, and **ColecoVision**. The generation began in November 1976 with the release of the Fairchild Channel F.[1] This was followed by the Atari 2600 in 1977,[2] Magnavox Odyssey² in 1978,[3] Intellivision in 1980[4] and then the Emerson Arcadia 2001, ColecoVision, Atari 5200, and Vectrex,[5] all in 1982. By the end of the era, there were over 15 different consoles. It coincided with, and was partly fuelled by, the golden age of arcade video games. This peak era of popularity and innovation for the medium resulted in many games for second generation home consoles being ports of arcade games. Space Invaders, the first "killer app" arcade game to be ported, was released in 1980 for the Atari 2600, though earlier Atari-published arcade games were ported to the 2600 previously.[6] Coleco packaged Nintendo's Donkey Kong with the ColecoVision when it was released in August 1982.
The primary driver of the second generation of consoles was the introduction of the low-cost microprocessor. 
https://en.wikipedia.org/wiki/Second_generation_of_video_game_consoles


### Gadgets
- **First single chip calculator (1970) - Pico Electronics**

In 1970, a group of engineers started a company in Glenrothes, Scotland called Pico Electronics.[2] The company developed the first single chip calculator

https://en.wikipedia.org/wiki/X10_(industry_standard)

### Automation
- **Remote control for light and applicances (1974) -  Pico Electronics / Birmingham Sound Reproducers** 

In 1974, the Pico engineers jointly developed a LP record turntable, the ADC Accutrac 4000, with Birmingham Sound Reproducers, at the time the largest manufacturer of record changers in the world. It could be programmed to play selected tracks, and could be operated by a remote control using ultrasound signals, which sparked the idea of remote control for lights and appliances. By 1975, the X10 project was conceived, so named because it was the tenth project. In 1978, X10 products started to appear in RadioShack and Sears stores. Together with BSR a partnership was formed, with the name X10 Ltd. At that time the system consisted of a 16 channel command console, a lamp module, and an appliance module. Soon after came the wall switch module and the first X10 timer.

https://en.wikipedia.org/wiki/X10_(industry_standard)

- **Modbus (1979)**

Modbus is a data communications protocol originally published by Modicon (now Schneider Electric) in 1979 for use with its programmable logic controllers (PLCs). Modbus has become a de facto standard communication protocol and is now a commonly available means of connecting industrial electronic devices.[1]

Modbus is popular in industrial environments because it is openly published and royalty-free. It was developed for industrial applications, is relatively easy to deploy and maintain compared to other standards, and places few restrictions on the format of the data to be transmitted.

The Modbus protocol uses character serial communication lines, Ethernet, or the Internet protocol suite as a transport layer. Modbus supports communication to and from multiple devices connected to the same cable or Ethernet network. For example, there can be a device that measures temperature and another device to measure humidity connected to the same cable, both communicating measurements to the same computer, via Modbus.

Modbus is often used to connect a plant/system supervisory computer with a remote terminal unit (RTU) in supervisory control and data acquisition (SCADA) systems. Many of the data types are named from industrial control of factory devices, such as ladder logic because of its use in driving relays: a single-bit physical output is called a coil, and a single-bit physical input is called a discrete input or a contact.

The development and update of Modbus protocols have been managed by the Modbus Organization[2] since April 2004, when Schneider Electric transferred rights to that organization.[3] The Modbus Organization is an association of users and suppliers of Modbus-compliant devices that advocates for the continued use of the technology.[4] Modbus Organization, Inc. is a trade association for the promotion and development of the Modbus protocol.[2]

https://en.wikipedia.org/wiki/Modbus

## Standards, languages and protocols : X.10 home automation, X.25 packet switching, FTP, TCP ##

### Network layer ###
- **X.10 (1975)**

X10 is a protocol for **communication among electronic devices used for home automation (domotics)**. It primarily uses power line wiring for signaling and control, where the signals involve brief radio frequency bursts representing digital information. A wireless radio-based protocol transport is also defined. X10 was developed in 1975 by Pico Electronics of Glenrothes, Scotland, in order to allow remote control of home devices and appliances. It was the first general purpose domotic network technology and remains the most widely available[citation needed].[1]
Although a number of higher-bandwidth alternatives exist, X10 remains popular in the home environment with millions of units in use worldwide, and inexpensive availability of new components.

- **X.25 (1976)**

ITU-T standard protocol suite for **packet-switched data communication in wide area networks (WAN)**. It was originally defined by the International Telegraph and Telephone Consultative Committee (CCITT, now ITU-T) in a series of drafts and finalized in a publication known as The Orange Book in 1976.
Networks using X.25 were popular during the late 1970s and 1980s with **telecommunications companies** and in **financial transaction systems** such as automated teller machines. An X.25 WAN consists of packet-switching exchange (PSE) nodes as the networking hardware, and leased lines, plain old telephone service connections, or ISDN connections as physical links. However, most users have moved to Internet Protocol (IP) systems instead. X.25 was used up to 2015 (e.g. by the credit card payment industry)[7] and is still used by aviation, purchasable from telecoms companies

https://en.wikipedia.org/wiki/X.25

### Transport layer ###
- **NCP - Network Control Protocol (1970)**

First implementation for the ARPANET. Transport layer protocol used during the early ARPANET. Remained in use until 1982.

https://en.wikipedia.org/wiki/Network_Control_Protocol_(ARPANET)

- **TCP - Transmission Control Protocol (1974)**

transmission of packets (reception is certain). Essential to the internet. 

https://en.wikipedia.org/wiki/Transmission_Control_Protocol

### Application layer ###
- **FTP - File Transfer Protocol (1971)**
standard communication protocol used for the transfer of computer files from a server to a client on a computer network. FTP is built on a client–server model architecture using separate control and data connections between the client and the server.
Until 1980, FTP ran on NCP, the predecessor of TCP/IP. Superseded by FTPS and SFTP.

https://en.wikipedia.org/wiki/File_Transfer_Protocol

- **G.711 (1972) : a narrowband audio codec**

originally designed for use in telephony that provides toll-quality audio at 64 kbit/s. G.711 passes audio signals in the range of 300–3400 Hz and samples them at the rate of 8,000 samples per second, with the tolerance on that rate of 50 parts per million (ppm). Non-uniform (logarithmic) quantization with 8 bits is used to represent each sample, resulting in a 64 kbit/s bit rate. There are two slightly different versions: μ-law, which is used primarily in North America and Japan, and A-law, which is in use in most other countries outside North America.
G.711 is an ITU-T standard (Recommendation) for audio companding, titled **Pulse code modulation (PCM)** of voice frequencies released for use in 1972. It is a required standard in many technologies, such as in the H.320 and H.323 standards.[1] It can also be used for fax communication over IP networks (as defined in T.38 specification).

https://en.wikipedia.org/wiki/G.711

-  **NAME/FINGER - Finger Protocol (1977)**
simple network protocols for the exchange of human-oriented status and user information (insecure, obsolete) https://en.wikipedia.org/wiki/Finger_(protocol)

### Electronic data exchange

Electronic data interchange (EDI) is the concept of businesses electronically communicating information that was traditionally communicated on paper, such as purchase orders and invoices. Technical standards for EDI exist to facilitate parties transacting such instruments without having to make special arrangements.

EDI has existed at least since the early 70s, and there are many EDI standards (including X12, EDIFACT, ODETTE, etc.), some of which address the needs of specific industries or regions. It also refers specifically to a family of standards. In 1996, the National Institute of Standards and Technology defined electronic data interchange as "the computer-to-computer interchange of a standardised format for data exchange. EDI implies a sequence of messages between two parties, either of whom may serve as originator or recipient. The formatted data representing the documents may be transmitted from originator to recipient via telecommunications or physically transported on electronic storage media." It distinguished mere electronic communication or data exchange, specifying that "in EDI, the usual processing of received messages is by computer only. Human intervention in the processing of a received message is typically intended only for error conditions, for quality review, and for special situations. For example, the transmission of binary or textual data is not EDI as defined here unless the data are treated as one or more data elements of an EDI message and are not normally intended for human interpretation as part of online data processing."[1] In short, EDI can be defined as the transfer of structured data, by agreed message standards, from one computer system to another without human intervention.

https://en.wikipedia.org/wiki/Electronic_data_interchange

- **ASC X12 (1979)**

The Accredited Standards Committee X12 (also known as ASC X12) is a standards organization. Chartered by the American National Standards Institute (ANSI) in 1979,[2] it develops and maintains the X12 Electronic data interchange (EDI) and Context Inspired Component Architecture (CICA) standards along with XML schemas which drive business processes globally. The membership of ASC X12 includes technologists and business process experts, encompassing health care, insurance, transportation, finance, government, supply chain and other industries.

https://en.wikipedia.org/wiki/ASC_X12

### File compression

- **TAR - Tape archive (1979) - Version 7 UNIX - AT&T Bell Laboratories**

In computing, tar is a computer software utility for collecting many files into one archive file, often referred to as a tarball, for distribution or backup purposes. The name is derived from "tape archive", as it was originally developed to write data to sequential I/O devices with no file system of their own. The archive data sets created by tar contain various file system parameters, such as name, timestamps, ownership, file-access permissions, and directory organization. POSIX abandoned tar in favor of pax, yet tar sees continued widespread use.

The command-line utility was first introduced in the Version 7 Unix in January 1979, replacing the tp program (which in turn replaced "tap").[7] The file structure to store this information was standardized in POSIX.1-1988[8] and later POSIX.1-2001,[9] and became a format supported by most modern file archiving systems. The tar command was abandoned in POSIX.1-2001 in favor of pax command, which was to support ustar file format; the tar command was indicated for withdrawal in favor of pax command at least since 1994.

Today, Unix-like operating systems usually include tools to support tar files, as well as utilities commonly used to compress them, such as gzip and bzip2.

https://en.wikipedia.org/wiki/Tar_(computing)

## Navigation

**GPS design and launch (1970-1978)**

A team led by Harold L Jury of Pan Am Aerospace Division in Florida from 1970 to 1973, used real-time data assimilation and recursive estimation to do so, reducing systematic and residual errors to a manageable level to permit accurate navigation.[35]

During Labor Day weekend in 1973, a meeting of about twelve military officers at the Pentagon discussed the creation of a Defense Navigation Satellite System (DNSS). It was at this meeting that the real synthesis that became GPS was created.

In 1972, the USAF Central Inertial Guidance Test Facility (Holloman AFB) conducted developmental flight tests of four prototype GPS receivers in a Y configuration over White Sands Missile Range, using ground-based pseudo-satellites.[53]

In 1978, the first experimental Block-I GPS satellite was launched.[39]

https://en.wikipedia.org/wiki/Global_Positioning_System

