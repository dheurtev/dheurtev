# 1990s

**In short:**
- [Inventions](#inventions) : LCD displays improvements (TN and IPS), high-efficiency green light-emitting polymer-based device, high brightness blue led, idea of using quantum dots as light source, ADSL, electronic ink
- [Electronics](#electronics) : Integrated ICs to motherboards, first while LED sold
- [Energy](#energy) : 
- [Telecommunications](#telecommunications) :  Connection of customers to ISPs, Dial-up internet access, Broadband internet (ADSL), Telecommunications Act of 1996, Local loop unbundling, Dot com bubble, Frame Relay, ATM, MPLS development, PBX to IP-PBX, VOIP
- [Networking](#networking) : Transition to IP traffic, birth of IPv6, ethernet to Gigabit Ethernet, Early RFID toy
- [Cryptography](#cryptography) : Cryptography goes mainstream : PGP, DSA, RSA, SSL, TLS
- [Computers](#computers) :
  * [Form factor](#form-factor) : Commodity servers instead of mainframes, 
  * [OS](#os) :  Microsoft Windows 3.0, 3.1, 95, 98
  * [Peripherals](#peripherals) : 
  * [Storage](#storage) : first commercial flash-based SSD, Storage area network (SAN), Fiber Channel (FC), iSCSI
  * [Uses](#uses) :
    
    *Servers*: Headless servers, Web/internet (HTTP servers, HTML, URL, XML), Apache (web server), Microsoft Active Directory (Directory service), Asterisk (IP-PBX)
    
    *Clients* : Remote desktop (VNC), Web browsers (Netscape Navigator, Microsoft Internet explorer)
    
    *CGI* : Pixar from hardware to 3D software to Toystory - first full-length 3D film
- [Consumer Electronics](#consumer-electronics) :
  * [Gadgets](#gadgets) : Sophisticated scientific calculators
  * [Multimedia](#multimedia) : DVD, First DVD Player
  * [Screens](#screens) : XGA computer monitors, HD resolution CRT, large plasma flat-panel TVs
  * [Broadcast](#broadcast) : HDTV with ATSC/DVB standards, Medium power satellites with smaller dishes
  * [Video games](#video-games) : Renaissance of arcade games, text-based multi-player virtual reality (Moo), fourth (Nintendo Super NES) to fifth generation video game consoles (Nintento 64, Sony Playstation)
- [Standards and protocols](#standards-and-protocols) : SSL, TLS, JPEG, MPEG-1, MPEG-2, WAV, MP3, PNG, SIP, SSH, VNC, Rsync, CORBA, ZIP compression tools and competitors (Winzip, RAR, ARJ, DEFLATE, 7-zip), CIFS, SOCKS, PPTP, Modems speeds (14.4k to 56k), HDTV (ATSC/DVB)
- [Programming languages and frameworks](#programming-languages-and-frameworks): OpenGL, Direct X, Direct3D, Unreal engine
- [Navigation](#navigation) : GPS is operational, military use, miniaturization and shift to dual-use

## Inventions
**[`^        back to top        ^`](#)**

- **LCD displays improvements (TN and IPS) (1990s)**

In 1990, under different titles, inventors conceived electro optical effects as alternatives to twisted nematic field effect LCDs (TN- and STN- LCDs). One approach was to use interdigital electrodes on one glass substrate only to produce an electric field essentially parallel to the glass substrates.[79][80] To take full advantage of the properties of this In Plane Switching (IPS) technology further work was needed. After thorough analysis, details of advantageous embodiments are filed in Germany by Guenter Baur et al. and patented in various countries.[81][82] The Fraunhofer Institute ISE in Freiburg, where the inventors worked, assigns these patents to Merck KGaA, Darmstadt, a supplier of LC substances. In 1992, shortly thereafter, engineers at Hitachi work out various practical details of the IPS technology to interconnect the thin-film transistor array as a matrix and to avoid undesirable stray fields in between pixels.[83][84]

Hitachi also improved the viewing angle dependence further by optimizing the shape of the electrodes (Super IPS). NEC and Hitachi become early manufacturers of active-matrix addressed LCDs based on the IPS technology. This is a milestone for implementing large-screen LCDs having acceptable visual performance for flat-panel computer monitors and television screens. In 1996, Samsung developed the optical patterning technique that enables multi-domain LCD. Multi-domain and In Plane Switching subsequently remain the dominant LCD designs through 2006.[85] 

In the late 1990s, the LCD industry began shifting away from Japan, towards South Korea and Taiwan,[77] which later shifted to China.

https://en.wikipedia.org/wiki/Liquid-crystal_display

- **High-efficiency green light-emitting polymer-based device (1990)**

Research into polymer electroluminescence culminated in 1990, with J. H. Burroughes et al. at the Cavendish Laboratory at Cambridge University, UK, reporting a high-efficiency green light-emitting polymer-based device using 100 nm thick films of poly(p-phenylene vinylene).[29] Moving from molecular to macromolecular materials solved the problems previously encountered with the long-term stability of the organic films and enabled high-quality films to be easily made.[30] Subsequent research developed multilayer polymers and the new field of plastic electronics and OLED research and device production grew rapidly.[31] White OLEDs, pioneered by J. Kido et al. at Yamagata University, Japan in 1995, achieved the commercialization of OLED-backlit displays and lighting.[32][33]

https://en.wikipedia.org/wiki/OLED

- **High brightness blue led (1993)**

Two years later, in 1993, high-brightness blue LEDs were demonstrated by Shuji Nakamura of Nichia Corporation using a gallium nitride growth process.[48][49][50] In parallel, Isamu Akasaki and Hiroshi Amano of Nagoya University were working on developing the important GaN deposition on sapphire substrates and the demonstration of p-type doping of GaN. This new development revolutionized LED lighting, making high-power blue light sources practical, leading to the development of technologies like Blu-ray.[citation needed]

In 1995, Alberto Barbieri at the Cardiff University Laboratory (GB) investigated the efficiency and reliability of high-brightness LEDs and demonstrated a "transparent contact" LED using indium tin oxide (ITO) on (AlGaInP/GaAs).

https://en.wikipedia.org/wiki/Light-emitting_diode

- **Idea of using quantum dots as a light source (1990s)**

The idea of using quantum dots as a light source emerged in the 1990s. Early applications included imaging using QD infrared photodetectors, light emitting diodes and single-color light emitting devices.[12] 

https://en.wikipedia.org/wiki/Quantum_dot_display

Quantum dots (QDs) are semiconductor particles a few nanometres in size, having optical and electronic properties that differ from larger particles due to quantum mechanics. They are a central topic in nanotechnology. When the quantum dots are illuminated by UV light, an electron in the quantum dot can be excited to a state of higher energy. In the case of a semiconducting quantum dot, this process corresponds to the transition of an electron from the valence band to the conductance band. The excited electron can drop back into the valence band releasing its energy as light. This light emission (photoluminescence) is illustrated in the figure on the right. The color of that light depends on the energy difference between the conductance band and the valence band, or the transition between discrete energy states when band structure is no longer a good definition in QDs.

https://en.wikipedia.org/wiki/Quantum_dot

- **ADSL (1995)**

Lechleider also believed this higher-speed standard would be much more attractive to customers than ISDN had proven. Unfortunately, at these speeds, the systems suffered from a type of crosstalk known as "NEXT", for "near-end crosstalk". This made longer connections on customer lines difficult. Lechleider noted that NEXT only occurred when similar frequencies were being used, and could be diminished if one of the directions used a different carrier rate, but doing so would reduce the potential bandwidth of that channel. Lechleider suggested that most consumer use would be asymmetric anyway, and that providing a high-speed channel towards the user and a lower speed return would be suitable for many uses.[6]
This work in the early 1990s eventually led to the ADSL concept, which emerged in 1995. An early supporter of the concept was Alcatel, who jumped on ADSL while many other companies were still devoted to ISDN. Krish Prabu stated that "Alcatel will have to invest one billion dollars in ADSL before it makes a profit, but it is worth it." They introduced the first DSL Access Multiplexers (DSLAM), the large multi-modem systems used at the telephony offices, and later introduced customer ADSL modems under the Thomson brand. Alcatel remained the primary vendor of ADSL systems for well over a decade.[7]
ADSL quickly replaced ISDN as the customer-facing solution for last-mile connectivity. ISDN has largely disappeared on the customer side, remaining in use only in niche roles like dedicated teleconferencing systems and similar legacy systems.

https://en.wikipedia.org/wiki/Integrated_Services_Digital_Network

- **Electronic Ink (1997) - E Ink**

The notion of a low-power paper-like display had existed since the 1970s, originally conceived by researchers at Xerox PARC, but had never been realized.[4] While a post-doctoral student at Stanford University, physicist Joseph Jacobson envisioned a multi-page book with content that could be changed at the push of a button and required little power to use.[5]

Neil Gershenfeld recruited Jacobson for the MIT Media Lab in 1995, after hearing Jacobson's ideas for an electronic book.[4] Jacobson, in turn, recruited MIT undergrads Barrett Comiskey, a math major, and J.D. Albert, a mechanical engineering major, to create the display technology required to realize his vision.[1]

The initial approach was to create tiny spheres which were half white and half black, and which, depending on the electric charge, would rotate such that the white side or the black side would be visible on the display. Albert and Comiskey were told this approach was impossible by most experienced chemists and materials scientists and they had trouble creating these perfectly half-white, half-black spheres; during his experiments, Albert accidentally created some all-white spheres.[1]

Comiskey experimented with charging and encapsulating those all-white particles in microcapsules mixed in with a dark dye. The result was a system of microcapsules that could be applied to a surface and could then be charged independently to create black and white images.[1] A first patent was filed by MIT for the microencapsulated electrophoretic display in October 1996.[6]

The scientific paper was featured on the cover of Nature, something extremely unusual for work done by undergraduates. The advantage of the microencapsulated electrophoretic display and its potential for satisfying the practical requirements of electronic paper were summarized in the abstract of the Nature paper:

It has for many years been an ambition of researchers in display media to create a flexible low-cost system that is the electronic analogue of paper ... viewing characteristic[s] result in an "ink on paper" look. But such displays have to date suffered from short lifetimes and difficulty in manufacture. Here we report the synthesis of an electrophoretic ink based on the microencapsulation of an electrophoretic dispersion. The use of a microencapsulated electrophoretic medium solves the lifetime issues and permits the fabrication of a bistable electronic display solely by means of printing. This system may satisfy the practical requirements of electronic paper.[7]

A second patent was filed by MIT for the microencapsulated electrophoretic display in March 1997.[8]

Subsequently, Albert, Comiskey and Jacobson along with Russ Wilcox and Jerome Rubin founded the E Ink Corporation in 1997, two months prior to Albert and Comiskey's graduation from MIT.[1]

https://en.wikipedia.org/wiki/E_Ink

## Electronics
**[`^        back to top        ^`](#)**

- **Integrated ICs to Motherboards (1990s)**

By the late 1990s, many personal computer motherboards included consumer-grade embedded audio, video, storage, and networking functions without the need for any expansion cards at all; higher-end systems for 3D gaming and computer graphics typically retained only the graphics card as a separate component. Business PCs, workstations, and servers were more likely to need expansion cards, either for more robust functions, or for higher speeds; those systems often had fewer embedded components.

Laptop and notebook computers that were developed in the 1990s integrated the most common peripherals. This even included motherboards with no upgradeable components, a trend that would continue as smaller systems were introduced after the turn of the century (like the tablet computer and the netbook). Memory, processors, network controllers, power source, and storage would be integrated into some systems.

Motherboards contain a ROM (and later EPROM, EEPROM, NOR flash) to initialize hardware devices and load an operating system from a peripheral device. Microcomputers such as the Apple II and IBM PC used ROM chips mounted in sockets on the motherboard. At power-up, the central processor unit would load its program counter with the address of the Boot ROM and start executing instructions from the Boot ROM. These instructions initialized and tested the system hardware, displays system information on the screen, performed RAM checks, and then loaded an operating system from a peripheral device

https://en.wikipedia.org/wiki/Motherboard

- **First while LED sold (1996)**

In the autumn of 1996, the first white light-emitting diodes (LEDs) were offered for sale.

https://onlinelibrary.wiley.com/doi/10.1002/lpor.201600147

## Energy
**[`^        back to top        ^`](#)**



## Telecommunications
**[`^        back to top        ^`](#)**

- **Connection of customers to ISPs (1990s)**

In the 1990s, with the advances of the Internet, leased lines were also used to **connect customer premises to ISP point of presence** 

https://en.wikipedia.org/wiki/Leased_line

- **Dial-up internet Access (1992+)**

Dial-up was first offered commercially in 1992 by Pipex in the United Kingdom and Sprint in the United States.[4][5] After the introduction of commercial broadband in the late 1990s,[6] dial-up Internet access became less popular in the mid-2000s. It is still used where other forms are not available or where the cost is too high, as in some rural or remote areas.[7][8]

https://en.wikipedia.org/wiki/Dial-up_Internet_access

- **Telecommunications Act of 1996**

The Telecommunications Act of 1996 was the first significant overhaul of United States telecommunications law in more than sixty years, amending the Communications Act of 1934. The Act, signed by President Bill Clinton, represented a major change in American telecommunication law, since it was the first time that the Internet was included in broadcasting and spectrum allotment.[1]

According to the Federal Communications Commission (FCC), the goal of the law was to "let anyone enter any communications business – to let any communications business compete in any market against any other."[2] The legislation's primary goal was deregulation of the converging broadcasting and telecommunications markets.[3] However, the law's regulatory policies have been questioned, including the effects of dualistic re-regulation of the communications market.[4][5]

A purpose of the 1996 Act was to foster competition among companies that use similar underlying network technologies (e.g., circuit-switched telephone networks) to provide a single type of service (e.g., voice). For example, it creates separate regulatory regimes for carriers providing voice telephone service and providers of cable television, and a third for information services.

The Telecommunications Act of 1996 did not foster competition among ILECs as the bill had hoped. Instead, of ILECs encroaching on each other, the opposite occurred  – mergers. Before the 1996 Act was passed, the largest four ILECs owned less than half of all the lines in the country while, five years later, the largest four local telephone companies owned about 85% of all the lines in the country.

https://en.wikipedia.org/wiki/Telecommunications_Act_of_1996

- **Local loop unbundling**

Local loop unbundling (LLU or LLUB) is the regulatory process of allowing multiple telecommunications operators to use connections from the telephone exchange to the customer's premises. The physical wire connection between the local exchange and the customer is known as a "local loop", and is owned by the incumbent local exchange carrier (also referred to as the "ILEC", "local exchange", or in the United States either a "Baby Bell" or an independent telephone company). To increase competition, other providers are granted unbundled access.

Pursuant to the Telecommunications Act of 1996, the Federal Communications Commission (FCC) requires that ILECs lease local loops to competitors (CLECs). Prices are set through a market mechanism.[12]

- **Dot com bubble - Internet bubble (1995-2000)**

The dot-com bubble, also known as the dot-com boom,[1] the tech bubble,[2] and the Internet bubble, was a stock market bubble in the late 1990s, a period of massive growth in the use and adoption of the Internet.[2][3]

Between 1995 and its peak in March 2000, the Nasdaq Composite stock market index rose 400%, only to fall 78% from its peak by October 2002, giving up all its gains during the bubble. During the dot-com crash, many online shopping companies, such as Pets.com, Webvan, and Boo.com, as well as several communication companies, such as Worldcom, NorthPoint Communications, and Global Crossing, failed and shut down.[4][5] Some companies that survived, such as Amazon.com, lost large portions of their market capitalization, with Cisco Systems alone losing 80% of its stock value.[6][7]

https://en.wikipedia.org/wiki/Dot-com_bubble

After congress passed the telecommunications Act of 1996, capital flooded into telecom, as existing firms and new ones began building networks over land, undersea and in the air. "Business plans all looked alike," one industry insider recalls. "Massively parallel systems were being built up."

By 2000, however, companies began to realize that there simply wasn't enough business to go around, and they raced "to gain market share" in a burst of "hypercompetition" and "vicious price wars" that drove down revenues,

Around this time, some executives started to engage in the practices that have since led them to contemplate long prison terms instead of the "long boom" predicted for the New Economy. To inflate their profits, some counted operating expenses as capital investment. Or two companies with excess capacity would sell each other the right to use a share of each other's networks. In such a swap, for example, each firm might book $150 million in revenue from the transaction when, in fact, there was no real revenue at all. Not only did such a swap allow each firm to deceive investors about its business, it also created the impression that the industry as a whole was $300 million larger than it actually was.

But such gimmicks couldn't sustain the illusion of growth and profitability indefinitely. While telecom firms were expanding, they had taken on enormous amounts of debt; one firm after another began having difficulty repaying these obligations and went into bankruptcy.

https://prospect.org/features/great-telecom-implosion/

- **Frame Relay**

Frame Relay is a standardized wide area network (WAN) technology that specifies the physical and data link layers of digital telecommunications channels using a packet switching methodology. Originally designed for transport across Integrated Services Digital Network (ISDN) infrastructure, it may be used today in the context of many other network interfaces.

Network providers commonly implement Frame Relay for voice (VoFR) and data as an encapsulation technique used between local area networks (LANs) over a WAN. Each end-user gets a private line (or leased line) to a Frame Relay node. The Frame Relay network handles the transmission over a frequently changing path transparent to all end-user extensively used WAN protocols. It is less expensive than leased lines and that is one reason for its popularity. The extreme simplicity of configuring user equipment in a Frame Relay network offers another reason for Frame Relay's popularity.

With the advent of Ethernet over fiber optics, MPLS, VPN and dedicated broadband services such as cable modem and DSL, Frame Relay has become less popular in recent years.

Frame Relay began as a stripped-down version of the X.25 protocol, releasing itself from the error-correcting burden most commonly associated with X.25. When Frame Relay detects an error, it simply drops the offending packet. 

Although Frame Relay became very popular in North America, it was never very popular in Europe. X.25 remained the primary standard until the wide availability of IP made packet switching almost obsolete. It was used sometimes as backbone for other services, such as X.25 or IP traffic. Where Frame Relay was used in the USA also as carrier for TCP/IP traffic, in Europe backbones for IP networks often used ATM or PoS, later replaced by Carrier Ethernet[3]

https://en.wikipedia.org/wiki/Frame_Relay

- **Development of MPLS (1994-2001)**

Multiprotocol Label Switching (MPLS) is a routing technique in telecommunications networks that directs data from one node to the next based on labels rather than network addresses.[1] Whereas network addresses identify endpoints the labels identify established paths between endpoints. MPLS can encapsulate packets of various network protocols, hence the multiprotocol component of the name. MPLS supports a range of access technologies, including T1/E1, ATM, Frame Relay, and DSL.

  * 1994: Toshiba presented Cell Switch Router (CSR) ideas to IETF BOF
  * 1996: Ipsilon, Cisco and IBM announced label switching plans
  * 1997: Formation of the IETF MPLS working group
  * 1999: First MPLS VPN (L3VPN) and TE deployments
  * 2000: MPLS Traffic Engineering
  * 2001: First MPLS Request for Comments (RFCs) published[4]

- **PBX to IP-PBX, VOIP**

Two significant developments during the 1990s led to new types of PBX systems. One was the massive growth of data networks and increased public understanding of packet switching. Companies needed packet-switched networks for data, so using them for telephone calls proved tempting, and the availability of the Internet as a global delivery system made packet-switched communications even more attractive. These factors led to the development of the voice over IP PBX, or IP-PBX.

The other trend involved the idea of focusing on core competence. PBX services had always been hard to arrange for smaller companies, and many[quantify] companies realized that handling their own telephony was not their core competence. These considerations gave rise to the concept of the hosted PBX. In wireline telephony, the original hosted PBX was the Centrex service provided by telcos since the 1960s; later competitive offerings evolved into the modern competitive local exchange carrier. In voice over IP, hosted solutions are easier to implement as the PBX may be located at and managed by any telephone service provider, connecting to the individual extensions via the Internet. The upstream provider no longer needs to run direct, local leased lines to the served premises.

Since the advent of Internet telephony (Voice over IP) technologies, PBX development has tended toward the IP PBX, which uses the Internet Protocol to carry calls.[5] Most modern PBXs support VoIP. ISDN PBX systems also replaced some traditional PBXs in the 1990s, as ISDN offers features such as conference calling, call forwarding, and programmable caller ID.

https://en.wikipedia.org/wiki/Business_telephone_system#Private_branch_exchange

## Networking
**[`^        back to top        ^`](#)**

### WAN

- **Transition to IP traffic**

Beginning in March 1991 the JANET IP Service (JIPS) was set up as a pilot project to host IP traffic on the existing network.[69] Within eight months the IP traffic had exceeded the levels of X.25 traffic, and the IP support became official in November. Also in 1991, Dai Davies introduced Internet technology over X.25 into the pan-European NREN, EuropaNet, although he experienced personal opposition to this approach.[70][71] The European Academic and Research Network (EARN) and RARE adopted IP around the same time,[nb 7] and the European Internet backbone EBONE became operational in 1992.[53] OSI usage on the NSFNET remained low when compared to TCP/IP. There was some talk of moving JANET to OSI protocols in the 1990s, but this never happened. The X.25 service was closed in August 1997

https://en.wikipedia.org/wiki/Protocol_Wars

### NFC

- **Early RFID toy (1997) - Innovision Research and Technology - Hasbro**

1997: Early form patented and first used in Star Wars character toys for Hasbro. The patent was originally held by Andrew White and Marc Borrett at Innovision Research and Technology (Patent WO9723060). The device allowed data communication between two units in close proximity.[17]

https://en.wikipedia.org/wiki/Near-field_communication

## Cryptography
**[`^        back to top        ^`](#)**

- **PGP (1991)**

Pretty Good Privacy is an encryption program that provides cryptographic privacy and authentication for data communication

https://en.wikipedia.org/wiki/Pretty_Good_Privacy

- **DSA (1991)**

The Digital Signature Algorithm (DSA) is a Federal Information Processing Standard for digital signatures, based on the mathematical concept of modular exponentiation and the discrete logarithm problem. DSA is a variant of the Schnorr and ElGamal signature schemes.[1]: 486 . 
The National Institute of Standards and Technology (NIST) proposed DSA for use in their Digital Signature Standard (DSS) in 1991, and adopted it as FIPS 186 in 1994

https://en.wikipedia.org/wiki/Digital_Signature_Algorithm

- **SSL and TLS development and deployment**

[SSL and TLS](#standards-and-protocols)

- **RSA released in public domain before patent expiration in 2000**

https://en.wikipedia.org/wiki/RSA_(cryptosystem)

## Computers
**[`^        back to top        ^`](#)**

### Form factor
**[`^        back to top        ^`](#)**

- **Commodity servers instead of mainframes (1990s)**

In 1991, AT&T Corporation briefly owned NCR. During the same period, companies found that servers based on microcomputer designs could be deployed at a fraction of the acquisition price and offer local users much greater control over their own systems given the IT policies and practices at that time. Terminals used for interacting with mainframe systems were gradually replaced by personal computers. Consequently, demand plummeted and new mainframe installations were restricted mainly to financial services and government. In the early 1990s, there was a rough consensus among industry analysts that the mainframe was a dying market as mainframe platforms were increasingly replaced by personal computer networks. InfoWorld's Stewart Alsop infamously predicted that the last mainframe would be unplugged in 1996; in 1993, he cited Cheryl Currid, a computer industry analyst as saying that the last mainframe "will stop working on December 31, 1999",[20] a reference to the anticipated Year 2000 problem (Y2K).

https://en.wikipedia.org/wiki/Mainframe_computer

### OS
**[`^        back to top        ^`](#)**

- **Microsoft Windows 3.0 (1990) : Multi-tasked DOS**

improved the design, mostly because of virtual memory and loadable virtual device drivers (VxDs) that allow Windows to share arbitrary devices between **multi-tasked DOS applications**.

https://en.wikipedia.org/wiki/Microsoft_Windows

**Object Linking & Embedding (OLE) (1990) - Microsoft **

proprietary technology developed by Microsoft that allows embedding and linking to documents and other objects. For developers, it brought OLE Control Extension (OCX), a way to develop and use custom user interface elements. On a technical level, an OLE object is any object that implements the IOleObject interface, possibly along with a wide range of other interfaces, depending on the object's needs.

**OLE 1.0**, released in 1990, was an evolution of the original Dynamic Data Exchange (DDE) concept that Microsoft developed for earlier versions of Windows. While DDE was limited to transferring limited amounts of data between two running applications, OLE was capable of maintaining active links between two documents or even embedding one type of document within another.

- **Microsoft Windows 3.11 (1993) : Windows for Workgroups - peer-to-peer networking features**

Windows 3.1, made generally available on March 1, 1992, featured a facelift. In August 1993, ***Windows for Workgroups, a special version with integrated peer-to-peer networking features*** and a version number of 3.11, was released.

https://en.wikipedia.org/wiki/Microsoft_Windows

**Component Object Model (COM)** is a binary-interface standard for software components introduced by Microsoft in 1993. It is used to enable inter-process communication object creation in a large range of programming languages. COM is the basis for several other Microsoft technologies and frameworks, including OLE, OLE Automation, Browser Helper Object, ActiveX, COM+, DCOM, the Windows shell, DirectX, UMDF and Windows Runtime. The essence of COM is a language-neutral way of implementing objects that can be used in environments different from the one in which they were created, even across machine boundaries. For well-authored components, COM allows reuse of objects with no knowledge of their internal implementation, as it forces component implementers to provide well-defined interfaces that are separated from the implementation. The different allocation semantics of languages are accommodated by making objects responsible for their own creation and destruction through reference-counting. Type conversion casting between different interfaces of an object is achieved through the QueryInterface method. The preferred method of "inheritance" within COM is the creation of sub-objects to which method "calls" are delegated.

COM is an interface technology defined and implemented as standard only on Microsoft Windows and Apple's Core Foundation 1.3 and later plug-in application programming interface (API).[1] The latter only implements a subset of the whole COM interface.[2] For some applications, COM has been replaced at least to some extent by the Microsoft .NET framework, and support for Web Services through the Windows Communication Foundation (WCF). However, COM objects can be used with all .NET languages through .NET COM Interop. Networked DCOM uses binary proprietary formats, while WCF encourages the use of XML-based SOAP messaging. COM is very similar to other component software interface technologies, such as CORBA and Enterprise JavaBeans, although each has its own strengths and weaknesses. Unlike C++, COM provides a stable application binary interface (ABI) that does not change between compiler releases.[3] This makes COM interfaces attractive for object-oriented C++ libraries that are to be used by clients compiled using different compiler versions.

https://en.wikipedia.org/wiki/Component_Object_Model

**OLE custom controls**

introduced in 1994 as a replacement for the now deprecated Visual Basic Extension controls. Instead of upgrading these, the new architecture was based on OLE. In particular, any container that supported OLE 2.0 could already embed OLE custom controls, although these controls cannot react to events unless the container supports this. OLE custom controls are usually shipped in the form of a dynamic link library with the .ocx extension. In 1996 all interfaces for controls (except IUnknown) were made optional to keep the file size of controls down, so they would download faster; these were then called ActiveX Controls.

https://en.wikipedia.org/wiki/Object_Linking_and_Embedding

- **Microsoft Windows 95 (1995) : 32 bit applications, plug and play hardware**

Windows 95 introduced support for native 32-bit applications, plug and play hardware, preemptive multitasking, long file names of up to 255 characters, and provided increased stability over its predecessors. Windows 95 also introduced a redesigned, object oriented user interface, replacing the previous Program Manager with the Start menu, taskbar, and Windows Explorer shell. Windows 95 was a major commercial success for Microsoft.

https://en.wikipedia.org/wiki/Microsoft_Windows

**ActiveX** is a deprecated software framework created by Microsoft that adapts its earlier Component Object Model (COM) and Object Linking and Embedding (OLE) technologies for content downloaded from a network, particularly from the World Wide Web.[1] Microsoft introduced ActiveX in 1996. In principle, ActiveX is not dependent on Microsoft Windows operating systems, but in practice, most ActiveX controls only run on Windows. Most also require the client to be running on an x86-based computer because ActiveX controls contain compiled code.[2]

Faced with the complexity of OLE 2.0 and with poor support for COM in MFC, Microsoft simplified the specification and rebranded the technology as ActiveX in 1996.[6][7] Even after simplification, users still required controls to implement about six core interfaces. In response to this complexity, Microsoft produced wizards, ATL base classes, macros and C++ language extensions to make it simpler to write controls.

ActiveX was one of the major technologies used in component-based software engineering.[4] Compared with JavaBeans, ActiveX supports more programming languages, but JavaBeans supports more platforms.[5] ActiveX is supported in many rapid application development technologies, such as Active Template Library, Delphi, JavaBeans, Microsoft Foundation Class Library, Qt, Visual Basic, Windows Forms and wxWidgets, to enable application developers to embed ActiveX controls into their products.

Many Microsoft Windows applications—including many of those from Microsoft itself, such as Internet Explorer, Microsoft Office, Microsoft Visual Studio, and Windows Media Player—use ActiveX controls to build their feature-set and also encapsulate their own functionality as ActiveX controls which can then be embedded into other applications. Internet Explorer also allows the embedding of ActiveX controls in web pages.

ActiveX was controversial from the start; while Microsoft claimed programming ease and good performance compared to Java applets in its marketing materials, critics of ActiveX were quick to point out security issues and lack of portability, making it impractical for use outside protected intranets.[10] The ActiveX security model relied almost entirely on identifying trusted component developers using a code signing technology called Authenticode. Developers had to register with Verisign (US$20 per year for individuals, $400 for corporations) and sign a contract, promising not to develop malware. Identified code would then run inside the web browser with full permissions, meaning that any bug in the code was a potential security issue; this contrasts with the sandboxing already used in Java at the time.[11]

Microsoft dropped ActiveX support from the Windows Store edition of Internet Explorer 10 in Windows 8. In 2015, Microsoft released Microsoft Edge, the replacement for Internet Explorer with no support for ActiveX, this event marked the end of ActiveX technology in Microsoft's web browser development.[18]

https://en.wikipedia.org/wiki/ActiveX

- **Microsoft Windows 98 (1998) : USB support**

The release of Windows 98 on June 25, 1998, which introduced the Windows Driver Model, support for USB composite devices, support for ACPI, hibernation, and support for multi-monitor configuration**

https://en.wikipedia.org/wiki/Microsoft_Windows

Windows 98 added built-in support for USB Human Interface Device class (USB HID),[105] with native vertical scrolling support.[106] Windows 2000 and Windows Me expanded this built-in support to 5-button mice.[107]

https://en.wikipedia.org/wiki/Computer_mouse


### Peripherals
**[`^        back to top        ^`](#)**


### Storage
**[`^        back to top        ^`](#)**

- **first commercial flash-based SSD (1991) - SanDisk**

The first commercial flash-based SSD was shipped by SanDisk in 1991.[35] It was a 20 MB SSD in a PCMCIA configuration, and sold OEM for around $1,000 and was used by IBM in a ThinkPad laptop.[39] In 1998, SanDisk introduced SSDs in 2.5-inch and 3.5-inch form factors with PATA interfaces.[40]

In 1995, STEC, Inc. entered the flash memory business for consumer electronic devices.[41]

In 1995, M-Systems introduced flash-based solid-state drives[42] as HDD replacements for the military and aerospace industries, as well as for other mission-critical applications. These applications require the SSD's ability to withstand extreme shock, vibration, and temperature ranges.[43]

In 1999, BiTMICRO made a number of introductions and announcements about flash-based SSDs, including an 18 GB[44] 3.5-inch SSD.[45] In 2007, Fusion-io announced a PCIe-based Solid state drive with 100,000 input/output operations per second (IOPS) of performance in a single card, with capacities up to 320 GB.[46]

https://en.wikipedia.org/wiki/Solid-state_drive

- **Storage area network (SAN) (1992-1993)"

A storage area network (SAN) or storage network is a computer network which provides access to consolidated, block-level data storage. SANs are primarily used to access data storage devices, such as disk arrays and tape libraries from servers so that the devices appear to the operating system as direct-attached storage. A SAN typically is a dedicated network of storage devices not accessible through the local area network (LAN).

Although a SAN provides only block-level access, file systems built on top of SANs do provide file-level access and are known as shared-disk file systems.

Newer SAN configurations enable hybrid SAN[1] and allow traditional block storage that appears as local storage but also object storage for web services through APIs.

Storage area networks (SANs) are sometimes referred to as network behind the servers[2]: 11  and historically developed out of a centralized data storage model, but with its own data network. A SAN is, at its simplest, a dedicated network for data storage. 

https://en.wikipedia.org/wiki/Storage_area_network

Storage Systems were evolving since when we were introduced to this technology. It was likely 1993 or 1994 when a new storage technology emerged named Storage Area Network aka SAN. At first, it was not that popular as you know it can happen with any new technology. 

Also, the cost of developing a newly arrived storage system was a crucial factor. As you know the users need various types of components to build a SAN. For instance, you will need SAN switches, servers, storage disks, tape libraries, and JBODS, etc. So, overall the price of this technology was very expensive at that time.

Due to that, only the big companies could take advantage of this brand new storage system. With the passing of time, a lot of data storage system providers from all around the world started to offer pre-built SAN storage to businesses and organizations. As a result, the competition between those companies grew and the price of the components also came to a lower level. And, SAN became a popular alternative to store and manage data.

Added to that, when people saw that it uses fiber channel technology and fiber channel protocol for transferring data, it was a hit. We will talk about the popularity of SAN in the later section of the post in detail. But overall, we can say that SAN came into the scene back in the 90s and dominated the storage system market for over a decade and a half.

https://www.reviewplan.com/evolution-of-storage-area-network/

- **Fibre Channel (FC) (1993) - IBM**

Fibre Channel (FC) is a high-speed data transfer protocol providing in-order, lossless[1] delivery of raw block data.[2] Fibre Channel is primarily used to connect computer data storage to servers[3][4] in storage area networks (SAN) in commercial data centers.
Fibre Channel networks form a switched fabric because the switches in a network operate in unison as one big switch. Fibre Channel typically runs on optical fiber cables within and between data centers, but can also run on copper cabling.[3][4] Supported data rates include 1, 2, 4, 8, 16, 32, 64, and 128 gigabit per second resulting from improvements in successive technology generations. The industry now notates this as Gigabit Fibre Channel (GFC).
There are various upper-level protocols for Fibre Channel, including two for block storage. Fibre Channel Protocol (FCP) is a protocol that transports SCSI commands over Fibre Channel networks.[3][4] FICON is a protocol that transports ESCON commands, used by IBM mainframe computers, over Fibre Channel. Fibre Channel can be used to transport data from storage systems that use solid-state flash memory storage medium by transporting NVMe protocol commands.
133 Mbit/s	0.1328125	8b10b	12.5	1993

https://en.wikipedia.org/wiki/Fibre_Channel

- **iSCSI - Internet Small Computer Systems Interface (1998) - IBM / Cisco**

Internet Small Computer Systems Interface or iSCSI (/ˈaɪskʌzi/ (listen) EYE-skuz-ee) is an Internet Protocol-based storage networking standard for linking data storage facilities. iSCSI provides block-level access to storage devices by carrying SCSI commands over a TCP/IP network. iSCSI facilitates data transfers over intranets and to manage storage over long distances. It can be used to transmit data over local area networks (LANs), wide area networks (WANs), or the Internet and can enable location-independent data storage and retrieval.
The protocol allows clients (called initiators) to send SCSI commands (CDBs) to storage devices (targets) on remote servers. It is a storage area network (SAN) protocol, allowing organizations to consolidate storage into storage arrays while providing clients (such as database and web servers) with the illusion of locally attached SCSI disks.[1] It mainly competes with Fibre Channel, but unlike traditional Fibre Channel which usually requires dedicated cabling,[a] iSCSI can be run over long distances using existing network infrastructure.[2] iSCSI was pioneered by IBM and Cisco in 1998 and submitted as a draft standard in March 2000.[3]

https://en.wikipedia.org/wiki/ISCSI

### Uses
**[`^        back to top        ^`](#)**

#### Servers

- **Headless servers : GUI makes graphic display terminals and terminal emulation obsolete (1990s)**

graphic display terminals, and terminal emulation became obsolete in the 1990s due to the advent of personal computers provided with **GUIs**.

https://en.wikipedia.org/wiki/Mainframe_computer

- **httpd (1990) : First web server goes live** 

https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol

CERN httpd (later also known as W3C httpd) is an early, now discontinued, web server (HTTP) daemon originally developed at CERN from 1990 onwards by Tim Berners-Lee, Ari Luotonen[2] and Henrik Frystyk Nielsen.[1] Implemented in C, it was the first web server software.[3]

https://en.wikipedia.org/wiki/CERN_httpd

- **NSCA HTTPd (1993)**

NCSA HTTPd is an early, now discontinued, web server originally developed at the NCSA at the University of Illinois at Urbana–Champaign by Robert McCool and others.[1] First released in 1993, it was among the earliest web servers developed, following Tim Berners-Lee's CERN httpd, Tony Sanders' Plexus server, and some others

https://en.wikipedia.org/wiki/NCSA_HTTPd

- **Apache HTTP Server (1995): Most successful internet server**

Originally based on the NCSA HTTPd server, development of Apache began in early 1995 after work on the NCSA code stalled. Apache played a key role in the initial growth of the World Wide Web,[11] quickly overtaking NCSA HTTPd as the dominant HTTP server. In 2009, it became the first web server software to serve more than 100 million websites.[12]

https://en.wikipedia.org/wiki/Apache_HTTP_Server

- **Microsoft Active Directory - Directory service (1999) - Microsoft**

Active Directory (AD) is a directory service developed by Microsoft for Windows domain networks. It is included in most Windows Server operating systems as a set of processes and services.[1][2] Initially, Active Directory was used only for centralized domain management. However, Active Directory eventually became an umbrella title for a broad range of directory-based identity-related services.[3]

A server running the Active Directory Domain Service (AD DS) role is called a domain controller. It authenticates and authorizes all users and computers in a Windows domain type network, assigning and enforcing security policies for all computers, and installing or updating software. For example, when a user logs into a computer that is part of a Windows domain, Active Directory checks the submitted username and password and determines whether the user is a system administrator or normal user.[4] Also, it allows management and storage of information, provides authentication and authorization mechanisms and establishes a framework to deploy other related services: Certificate Services, Active Directory Federation Services, Lightweight Directory Services, and Rights Management Services.[5]

Active Directory uses Lightweight Directory Access Protocol (LDAP) versions 2 and 3, Microsoft's version of Kerberos,[6] and DNS.[7]

Like many information-technology efforts, Active Directory originated out of a democratization of design using Request for Comments (RFCs). The Internet Engineering Task Force (IETF), which oversees the RFC process, has accepted numerous RFCs initiated by widespread participants. For example, LDAP underpins Active Directory. Also, X.500 directories and the Organizational Unit preceded the Active Directory concept that makes use of those methods. The LDAP concept began to emerge even before the founding of Microsoft in April 1975, with RFCs as early as 1971. RFCs contributing to LDAP include RFC 1823 (on the LDAP API, August 1995),[8] RFC 2307, RFC 3062, and RFC 4533.[9][10][11]

Microsoft previewed Active Directory in 1999, released it first with **Windows 2000 Server edition**, and revised it to extend functionality and improve administration in **Windows Server 2003**. Active Directory support was also added to Windows 95, Windows 98 and Windows NT 4.0 via patch, with some features being unsupported.[12][13] Additional improvements came with subsequent versions of Windows Server. In **Windows Server 2008**, additional services were added to Active Directory, such as Active Directory Federation Services.[14] The part of the directory in charge of the management of domains, which was previously a core part of the operating system,[14] was renamed Active Directory Domain Services (ADDS) and became a server role like others.[3] "Active Directory" became the umbrella title of a broader range of directory-based services.[15] According to Byron Hynes, everything related to identity was brought under Active Directory's banner.[3]

https://en.wikipedia.org/wiki/Active_Directory

- **Asterisk - IP-PBX server (1999) - Digium**

Asterisk is a software implementation of a private branch exchange (PBX). In conjunction with suitable telephony hardware interfaces and network applications, Asterisk is used to establish and control telephone calls between telecommunication endpoints, such as customary telephone sets, destinations on the public switched telephone network (PSTN), and devices or services on voice over Internet Protocol (VoIP) networks. Its name comes from the asterisk (*) symbol for a signal used in dual-tone multi-frequency (DTMF) dialing.

Asterisk was created in 1999 by Mark Spencer of Digium, which since 2018 is a division of Sangoma Technologies Corporation.[6][7] Originally designed for Linux,[8] Asterisk runs on a variety of operating systems, including NetBSD, OpenBSD, FreeBSD, macOS, and Solaris, and can be installed in embedded systems based on OpenWrt.[9][10]

https://en.wikipedia.org/wiki/Asterisk_(PBX)

#### Clients

##### Remote desktop

- **VNC (1999): Remote desktop application** : 

The remote desktop application VNC (Virtual Network Computing) is made available by the AT&T Laboratories Cambridge. ORL, the Olivetti Research Laboratory was founded 12 years earlier, accquired in 1999 by AT&T to create AT&T; Laboratories Cambridge.

https://en.wikipedia.org/wiki/Virtual_Network_Computing

##### Web browsers

- **NSCA Mosaïc (1993)**: web browser developped at the end of 1992 at the american research center NCSA

https://en.wikipedia.org/wiki/Mosaic_(web_browser)

- **Netscape Navigator (1994): web browser**

https://en.wikipedia.org/wiki/Netscape

- **Microsoft Internet Explorer (1995): Most successful web browser**

Starting in 1995, It was first released as part of the add-on package Plus! for Windows 95 that year. Later versions were available as free downloads, or in-service packs, and included in the original equipment manufacturer (OEM) service releases of Windows 95 and later versions of Windows. 
Internet Explorer was once the most widely used web browser, attaining a peak of 95% usage share by 2003.[12] This came after Microsoft used bundling to win the first browser war against Netscape, which was the dominant browser in the 1990s

https://en.wikipedia.org/wiki/Internet_Explorer

#### CGI

- **Pixar from hardware to 3D software to Toystory - first full-length 3D film (1995)**

In April 1990, Pixar sold its hardware division, including all proprietary hardware technology and imaging software, to Vicom Systems, and transferred 18 of Pixar's approximately 100 employees. That year, Pixar moved from San Rafael to Richmond, California.[33] Pixar released some of its software tools on the open market for Macintosh and Windows systems. RenderMan is one of the leading 3D packages of the early 1990s, and Typestry is a special-purpose 3D text renderer that competed with RayDream.[citation needed]

During this period, Pixar continued its successful relationship with Walt Disney Feature Animation, a studio whose corporate parent would ultimately become its most important partner. As 1991 began, however, the layoff of 30 employees in the company's computer hardware department—including the company's president, Chuck Kolstad,[34] reduced the total number of employees to just 42, approximately its original number.[35] Pixar made a historic $26 million deal with Disney to produce three computer-animated feature films, the first of which was Toy Story, the product of the technological limitations that challenged CGI.[36] By then the software programmers, who were doing RenderMan and IceMan, and Lasseter's animation department, which made television commercials (and four Luxo Jr. shorts for Sesame Street the same year), were all that remained of Pixar.[37]

Even with income from these projects, the company continued to lose money and Steve Jobs, as chairman of the board and now the full owner, often considered selling it. Even as late as 1994, Jobs contemplated selling Pixar to other companies such as Hallmark Cards, Microsoft co-founder Paul Allen, and Oracle CEO and co-founder Larry Ellison.[38] Only after learning from New York critics that Toy Story would probably be a hit—and confirming that Disney would distribute it for the 1995 Christmas season—did he decide to give Pixar another chance.[39][40] For the first time, he also took an active leadership role in the company and made himself CEO.[citation needed] Toy Story grossed more than $373 million worldwide[41] and, when Pixar held its initial public offering on November 29, 1995, it exceeded Netscape's as the biggest IPO of the year. In its first half-hour of trading, Pixar stock shot from $22 to $45, delaying trading because of unmatched buy orders. Shares climbed to US$49 and closed the day at $39.[42]

https://en.wikipedia.org/wiki/Pixar#Early_history

## Consumer Electronics
**[`^        back to top        ^`](#)**

### Gadgets
**[`^        back to top        ^`](#)**

- **Sophisticated calculators, TI, HP**

The two leading manufacturers, HP and TI, released increasingly feature-laden calculators during the 1980s and 1990s. At the turn of the millennium, the line between a graphing calculator and a handheld computer was not always clear, as some very advanced calculators such as the TI-89, the Voyage 200 and HP-49G could differentiate and integrate functions, solve differential equations, run word processing and PIM software, and connect by wire or IR to other calculators/computers.

https://en.wikipedia.org/wiki/Calculator

### Multimedia
**[`^        back to top        ^`](#)**

- **DVD (1996)**

The DVD (common abbreviation for Digital Video Disc or Digital Versatile Disc)[8][9] is a digital optical disc data storage format invented and developed in 1995 and released in late 1996. Currently allowing up to 17.08 GB of storage,[10] the medium can store any kind of digital data and was widely used for software and other computer files as well as video programs watched using DVD players. DVDs offer higher storage capacity than compact discs while having the same dimensions.

https://en.wikipedia.org/wiki/DVD

- **Toshiba SD-3000 - First DVD Player (1996)**

In November 1996, Toshiba introduced the world's first DVD player, the SD-3000, as a result of developments initiated in 1994. At the time, the VHS VCR was dominating the market.Further, the laser disk used analog video.

The DVD (called SD at the time) produced by Toshiba used digital audio and video could fit an entire movie on a disk the same size as a CD: 12 cm in diameter. This was a revolutionary standard that made possible high audio and video quality and multiple functions. I

https://toshiba-mirai-kagakukan.jp/en/learn/history/ichigoki/1996dvd/index.htm

### Screens
**[`^        back to top        ^`](#)**

- **XGA - Extended Graphics Array (1990) - IBM**

The Extended Graphics Array (XGA) is an IBM display standard introduced in 1990. Later it became the most common appellation of the **1024 × 768 pixels** display resolution, but the official definition is broader than that. It was not a new and improved replacement for Super VGA, but rather became one particular subset of the broad range of capabilities covered under the "Super VGA" umbrella.

https://en.wikipedia.org/wiki/Graphics_display_resolution#Extended_Graphics_Array

- **first CRT with HD resolution (1990) - Sony**

In 1990, the first CRTs with HD resolution were released to the market by Sony.[67]

In the mid-1990s, some 160 million CRTs were made per year.[68]

https://en.wikipedia.org/wiki/Cathode-ray_tube

- **large plasma flat-panel TV (1990s)**

In 1992, Fujitsu introduced the world's first 21-inch (53 cm) full-color display. It was based on technology created at the University of Illinois at Urbana–Champaign and NHK Science & Technology Research Laboratories.

In 1994, Weber demonstrated a color plasma display at an industry convention in San Jose. Panasonic Corporation began a joint development project with Plasmaco, which led in 1996 to the purchase of Plasmaco, its color AC technology, and its American factory for US$26 million.

In 1995, Fujitsu introduced the first 42-inch (107 cm) plasma display panel;[61][62] it had 852×480 resolution and was progressively scanned.[63] Two years later, Philips introduced the first large commercially available flat-panel TV, using the Fujitsu panels. It was available at four Sears locations in the US for $14,999, including in-home installation. Pioneer also began selling plasma televisions that year, and other manufacturers followed. By the year 2000 prices had dropped to $10,000.

https://en.wikipedia.org/wiki/Plasma_display

### Broadcast
**[`^        back to top        ^`](#)**

- **Europe: HDTV launch - Italy (1990)**

Between 1988 and 1991, several European organizations were working on discrete cosine transform (DCT) based digital video coding standards for both SDTV and HDTV. The EU 256 project by the CMTT and ETSI, along with research by Italian broadcaster RAI, developed a DCT video codec that broadcast near-studio-quality HDTV transmission at about 70–140 Mbit/s.[23][40] The first HDTV transmissions in Europe, albeit not direct-to-home, began in 1990, when RAI broadcast the 1990 FIFA World Cup using several experimental HDTV technologies, including the digital DCT-based EU 256 codec,[23] 

https://en.wikipedia.org/wiki/High-definition_television

- **Europe : DVB - Digital Video Broadcasting (1993-1997) : Satelitte, Cable, Terrestrial**

DVB-S and DVB-C were ratified in 1994. DVB-T was ratified in early 1997. The first commercial DVB-T broadcasts were performed by the United Kingdom's Digital TV Group in late 1998. In 2003 Berlin, Germany was the first area to completely stop broadcasting analog TV signals. Most European countries are fully covered by digital television and many have switched off PAL/SECAM services.
Digital Video Broadcasting (DVB) is a set of international open standards for digital television. DVB standards are maintained by the DVB Project, an international industry consortium,[1] and are published by a Joint Technical Committee (JTC) of the European Telecommunications Standardisé Institute (ETSI), European Committee for Electrotechnical Standardization (CENELEC) and European Broadcasting Union (EBU).
Satellite: DVB-S, DVB-S2, and DVB-SH
DVB-SMATV for distribution via SMATV
Cable: DVB-C, DVB-C2

https://en.wikipedia.org/wiki/Digital_Video_Broadcasting

DVB-T, short for Digital Video Broadcasting — Terrestrial, is the DVB European-based consortium standard for the broadcast transmission of digital terrestrial television that was first published in 1997[1] and first broadcast in Singapore in February, 1998.[2][3][4][5][6][7][8] This system transmits compressed digital audio, digital video and other data in an MPEG transport stream, using coded orthogonal frequency-division multiplexing (COFDM or OFDM) modulation. It is also the format widely used worldwide (including North America) for Electronic News Gathering for transmission of video and audio from a mobile newsgathering vehicle to a central receive point.

https://en.wikipedia.org/wiki/DVB-T

- **HDTV deployment start (1994)**

HDTV technology was introduced in the United States in the early 1990s and made official in 1993 by the Digital HDTV Grand Alliance, a group of television, electronic equipment, communications companies consisting of AT&T Bell Labs, General Instrument, Philips, Sarnoff, Thomson, Zenith and the Massachusetts Institute of Technology. Field testing of HDTV at 199 sites in the United States was completed August 14, 1994.[34] The first public HDTV broadcast in the United States occurred on July 23, 1996, when the Raleigh, North Carolina television station WRAL-HD began broadcasting from the existing tower of WRAL-TV southeast of Raleigh, winning a race to be first with the HD Model Station in Washington, D.C., which began broadcasting July 31, 1996 with the callsign WHD-TV, based out of the facilities of NBC owned and operated station WRC-TV.[35][36][37] The American Advanced Television Systems Committee (ATSC) HDTV system had its public launch on October 29, 1998, during the live coverage of astronaut John Glenn's return mission to space on board the Space Shuttle Discovery.[38] The signal was transmitted coast-to-coast, and was seen by the public in science centers, and other public theaters specially equipped to receive and display the broadcast.[38][39]

https://en.wikipedia.org/wiki/High-definition_television

- **USA : ATSC HDTV Standards (1996)**

Advanced Television Systems Committee (ATSC) standards are an American set of standards for digital television transmission over terrestrial, cable and satellite networks. It is largely a replacement for the analog NTSC standard and, like that standard, is used mostly in the United States, Mexico, Canada, and South Korea. Several former NTSC users, in particular Japan, have not used ATSC during their digital television transition, because they adopted their own system called ISDB.
The ATSC standards were developed in the early 1990s by the Grand Alliance, a consortium of electronics and telecommunications companies that assembled to develop a specification for what is now known as HDTV.
The high-definition television standards defined by the ATSC produce widescreen 16:9 images up to 1920×1080 pixels in size – more than six times the display resolution of the earlier standard. However, many different image sizes are also supported. The reduced bandwidth requirements of lower-resolution images allow up to six standard-definition "subchannels" to be broadcast on a single 6 MHz TV channel.
ATSC standards are marked A/x (x is the standard number) and can be downloaded for free from the ATSC's website at ATSC.org. ATSC Standard A/53, which implemented the system developed by the Grand Alliance, was published in 1995; the standard was adopted by the Federal Communications Commission in the United States in 1996. It was revised in 2009. ATSC Standard A/72 was approved in 2008 and introduces H.264/AVC video coding to the ATSC system.
ATSC supports 5.1-channel surround sound using Dolby Digital's AC-3 format. Numerous auxiliary datacasting services can also be provided.
Many aspects of ATSC are patented, including elements of the MPEG video coding, the AC-3 audio coding, and the 8VSB modulation.[2] The cost of patent licensing, estimated at up to $50 per digital TV receiver,[3] had prompted complaints by manufacturers.[4]
Companies with patents included : 
LG Electronics, Zenith Electronics, Panasonic, Samsung Electronics, Columbia University, Mitsubishi Electric, JVC Kenwood, Cisco Technology, Inc., Vientos Alisios Co., Ltd., Philips

https://en.wikipedia.org/wiki/ATSC_standards

- **Satelitte : Medium power satellites with smaller dishes - PrimeStar**

In the US in the early 1990s, four large cable companies launched PrimeStar, a direct broadcasting company using medium power satellites. The relatively strong transmissions allowed the use of smaller (90 cm) dishes. Its popularity declined with the 1994 launch of the Hughes DirecTV and Dish Network satellite television systems.
Digital satellite broadcasts began in 1994 in the United States through DirecTV using the DSS format. They were launched (with the DVB-S standard) in South Africa, Middle East, North Africa and Asia-Pacific in 1994 and 1995, and in 1996 and 1997 in European countries including France, Germany, Spain, Portugal, Italy and the Netherlands, as well as Japan, North America and Latin America. Digital DVB-S broadcasts in the United Kingdom and Ireland started in 1998. Japan started broadcasting with the ISDB-S standard in 2000.
On March 4, 1996, EchoStar introduced Digital Sky Highway (Dish Network) using the EchoStar 1 satellite.[80] EchoStar launched a second satellite in September 1996 to increase the number of channels available on Dish Network to 170.[80] These systems provided better pictures and stereo sound on 150–200 video and audio channels, and allowed small dishes to be used. This greatly reduced the popularity of TVRO systems. In the mid-1990s, channels began moving their broadcasts to digital television transmission using the DigiCipher conditional access system.[81]

https://en.wikipedia.org/wiki/Satellite_television

### Video games
**[`^        back to top        ^`](#)**

- **Renaissance of arcade (1990-1996)**
 
Fighting games like Street Fighter II (1991) and Mortal Kombat (1992) helped to revive it in the early 1990s, leading to a renaissance for the arcade industry.[22] 3D graphics were popularized in arcades during the early 1990s with games such as Sega's Virtua Racing and Virtua Fighter,[59] with later arcade systems such as the Sega Model 3 remaining considerably more advanced than home systems through the late 1990s.[60][61] However, the improved capabilities of home consoles and computers to mimic arcade video games during this time drew crowds away from arcades.[22]

https://en.wikipedia.org/wiki/Arcade_game#History

- **SNES - Super Nitendo (1990) - Nintendo - ROM, Cartridge**

The Super Nintendo Entertainment System (SNES),[b] commonly shortened to Super NES or Super Nintendo,[c] is a 16-bit home video game console developed by Nintendo that was released in 1990 in Japan and South Korea,[19] 1991 in North America, 1992 in Europe and Oceania, and 1993 in South America. In Japan, it is called the Super Famicom (SFC).[d] In South Korea, it is called the Super Comboy[e] and was distributed by Hyundai Electronics.[20] The system was released in Brazil on August 30, 1993,[19][21] by Playtronic. Although each version is essentially the same, several forms of regional lockout prevent cartridges for one version from being used in other versions.
The Super NES is Nintendo's second programmable home console, following the Nintendo Entertainment System (NES). The console introduced advanced graphics and sound capabilities compared with other systems at the time. It was designed to accommodate the ongoing development of a variety of enhancement chips integrated into game cartridges to be competitive into the next generation.

https://en.wikipedia.org/wiki/Super_Nintendo_Entertainment_System

- **Text-based multi-player virtual reality (MOO) (1990s)**

A MOO ("MUD, object-oriented"[1][2]) is a text-based online virtual reality system to which multiple users (players) are connected at the same time.

The term MOO is used in two distinct, but related, senses. One is to refer to those programs descended from the original MOO server, and the other is to refer to any MUD that uses object-oriented techniques to organize its database of objects, particularly if it does so in a similar fashion to the original MOO or its derivatives. Most of this article refers to the original MOO and its direct descendants, but see Non-Descendant MOOs for a list of MOO-like systems.

The original MOO server was authored by Stephen White, based on his experience from creating the programmable TinyMUCK system.[3][2] There was additional later development and maintenance from LambdaMOO founder, and former Xerox PARC employee, Pavel Curtis.

One of the most distinguishing features of a MOO is that its users can perform object-oriented programming within the server, ultimately expanding and changing how the server behaves to everyone.[4] Examples of such changes include authoring new rooms and objects, creating new generic objects for others to use, and changing the way the MOO interface operates. The programming language used for extension is the MOO programming language, and many MOOs feature convenient libraries of verbs that can be used by programmers in their coding known as Utilities. The MOO programming language is a domain-specific language.[citation needed]

https://en.wikipedia.org/wiki/MOO

LambdaMOO is an online community[1] of the variety called a MOO. It is the oldest MOO today.[citation needed]

LambdaMOO was founded in 1990 by Pavel Curtis at Xerox PARC.[2][3][4][5] Now hosted in the state of Washington, it is operated and administered entirely on a volunteer basis. Guests are allowed, and membership is free to anyone with an e-mail address.

LambdaMOO gained some notoriety when Julian Dibbell wrote a book called My Tiny Life describing his experiences there.[6] Over its history, LambdaMOO has been highly influential in the examination of virtual-world social issues.[2]

https://en.wikipedia.org/wiki/LambdaMOO

- **Consoles surpasses arcade (1997-1998)**

Up until about 1996, arcade video games had remained the largest sector of the global video game industry, before arcades declined in the late 1990s, with the console market surpassing arcade video games for the first time around 1997–1998.[62

https://en.wikipedia.org/wiki/Arcade_game#History

- **32/64 bits game consoles - Fifth generation video games consoles (1993-2006)**

The fifth-generation era (also known as the 32-bit era, the 64-bit era, or the 3D era) refers to computer and video games, video game consoles, and handheld gaming consoles dating from approximately October 4, 1993 to March 23, 2006.[note 1] For home consoles, the best-selling console was the **Sony PlayStation**, followed by the **Nintendo 64**, and then the **Sega Saturn**. The PlayStation also had a redesigned version, the PSone, which was launched on July 7, 2000.

Some features that distinguished fifth generation consoles from previous fourth generation consoles include:
- 3D polygon graphics with texture mapping
- 3D graphics capabilities – lighting, Gouraud shading, anti-aliasing and texture filtering
- Optical disc (CD-ROM) game storage, allowing much larger storage space (up to 650 MB) than ROM cartridges
- CD quality audio recordings (music and speech) – PCM audio with 16-bit depth and 44.1 kHz sampling rate
- Wide adoption of full motion video, displaying pre-rendered computer animation or live action footage
- Analog controllers
- Display resolutions from 480i/480p to 576i
- Color depth up to 16,777,216 colors (24-bit true color)

https://en.wikipedia.org/wiki/Fifth_generation_of_video_game_consoles

- **Sony PlayStation - First console to ship over 100 million units (1994) - Sony**

PlayStation (Japanese: プレイステーション, Hepburn: Pureisutēshon, officially abbreviated as PS) is a video game brand that consists of five home video game consoles, two handhelds, a media center, and a smartphone, as well as an online service and multiple magazines. The brand is produced by Sony Interactive Entertainment, a division of Sony; the first PlayStation console was released in Japan in December 1994, and worldwide the following year.[1]
The original console in the series was the first console of any type to ship over 100 million units, doing so in under a decade.[2]

https://en.wikipedia.org/wiki/PlayStation

Sony began developing the standalone PlayStation after a failed venture with Nintendo to create a CD-ROM peripheral for the Super Nintendo Entertainment System in the early 1990s. The console was primarily designed by Ken Kutaragi and Sony Computer Entertainment in Japan, while additional development was outsourced in the United Kingdom. An emphasis on 3D polygon graphics was placed at the forefront of the console's design. PlayStation game production was designed to be streamlined and inclusive, enticing the support of many third-party developers.

The console proved popular for its extensive game library, popular franchises, low retail price, and aggressive youth marketing which advertised it as the preferable console for adolescents and adults. Premier PlayStation franchises included Gran Turismo, Crash Bandicoot, Tomb Raider, and Final Fantasy, all of which spawned numerous sequels. PlayStation games continued to sell until Sony ceased production of the PlayStation and its games on 23 March 2006—over eleven years after it had been released, and less than a year before the debut of the PlayStation 3.[8] A total of 3,061 PlayStation games were released, with cumulative sales of 967 million units.

SCE was an upstart in the video game industry in late 1994, as the video game market in the early 1990s was dominated by Nintendo and Sega. Nintendo had been the clear leader in the industry since the introduction of the Nintendo Entertainment System in 1985 and the Nintendo 64 was initially expected to maintain this position. The PlayStation's target audience included the generation which was the first to grow up with mainstream video games, along with 18- to 29-year-olds who were not the primary focus of Nintendo.[202] By the late 1990s, Sony became a highly regarded console brand due to the PlayStation, with a significant lead over second-place Nintendo, while Sega was relegated to a distant third.[203]

The PlayStation became the first "computer entertainment platform" to ship over 100 million units worldwide,[6][204] with many critics attributing the console's success to third-party developers.[76] It remains the fifth best-selling console of all time as of 2022, with a total of 102.49 million units sold.[204] Around 7,900 individual games were published for the console during its 11-year life span, the second-most amount of games ever produced for a console.[6] Its success resulted in a significant financial boon for Sony as profits from its video game division contributed to 23%.[205]

The success of the PlayStation contributed to the demise of cartridge-based home consoles. While not the first system to use an optical disc format, it was the first highly successful one, and ended up going head-to-head with the proprietary cartridge-relying Nintendo 64.[c][207] After the demise of the Sega Saturn, Nintendo was left as Sony's main competitor in Western markets. Nintendo chose not to use CDs for the Nintendo 64; it was likely concerned with the proprietary cartridge format's ability to help enforce copy protection, given its substantial reliance on licensing and exclusive games for its revenue.

https://en.wikipedia.org/wiki/PlayStation_(console)

## Standards and protocols
**[`^        back to top        ^`](#)**

## Some standards and protocols
**[`^        back to top        ^`](#)**

### Physical layer

- **Classic Ethernet (1990-1993)**

  * 802.3i	10BASE-T (1990) : 10 Mbit/s (1.25 MB/s) over twisted pair https://en.wikipedia.org/wiki/IEEE_802.3
  * 10BASEFL (1993) : 10BASE-F specification of Ethernet over optical fiber 
 
https://en.wikipedia.org/wiki/Classic_Ethernet

- **Fast Ethernet (1995-1998)**
  * 802.3u	-	100BASE-TX, 100BASE-T4, 100BASE-FX (1995) : Fast Ethernet at 100 Mbit/s (12.5 MB/s) with autonegotiation 

https://en.wikipedia.org/wiki/Fast_Ethernet

- **Gigabit Ethernet (1998-1999)**
  * 802.3z	- 1000BASE-X Gbit/s (1998): Ethernet over optical fiber at 1 Gbit/s (125 MB/s) 
  * 802.3ab	- 1000BASE-T Gbit/s (1999): Ethernet over twisted pair at 1 Gbit/s (125 MB/s)

https://en.wikipedia.org/wiki/Gigabit_Ethernet

- **Modem speeds : 14.4k to 56k**
  
  * V.32bis 14.4k bps; QAM (1991)
  * V.34 28.8k bps; QAM (1994) AKA V.fast
  * V.34bis 33.6k bps; QAM (1996)
  * V.90 56k bps; Modulus Conversion downstream, QAM upstream (1998)
  * V.92 56k bps; Modulus conversion in both directions (2000)

https://tldp.org/HOWTO/Modem-HOWTO-29.html

### Link layer

- **ATM - Asynchronous Transfer Mode**

Asynchronous Transfer Mode (ATM) is a telecommunications standard defined by American National Standards Institute (ANSI) and ITU-T (formerly CCITT) for digital transmission of multiple types of traffic. ATM was developed to meet the needs of the Broadband Integrated Services Digital Network as defined in the late 1980s,[1] and designed to integrate telecommunication networks. It can handle both traditional high-throughput data traffic and real-time, low-latency content such as telephony (voice) and video.[2][3] ATM provides functionality that uses features of circuit switching and packet switching networks by using asynchronous time-division multiplexing.[4][5]

In the OSI reference model data link layer (layer 2), the basic transfer units are called frames. In ATM these frames are of a fixed length (53 octets) called cells. This differs from approaches such as IP or Ethernet that use variable-sized packets or frames. ATM uses a connection-oriented model in which a virtual circuit must be established between two endpoints before the data exchange begins.[5] These virtual circuits may be either permanent (dedicated connections that are usually preconfigured by the service provider), or switched (set up on a per-call basis using signaling and disconnected when the call is terminated).

The ATM network reference model approximately maps to the three lowest layers of the OSI model: physical layer, data link layer, and network layer.[6] ATM is a core protocol used in the SONET/SDH backbone of the public switched telephone network (PSTN) and in the Integrated Services Digital Network (ISDN) but has largely been superseded in favor of next-generation networks based on Internet Protocol (IP) technology. Wireless and mobile ATM never established a significant foothold.

ATM became popular with telephone companies and many computer makers in the 1990s. However, even by the end of the decade, the better price/performance of Internet Protocol-based products was competing with ATM technology for integrating real-time and bursty network traffic.[17] Companies such as FORE Systems focused on ATM products, while other large vendors such as Cisco Systems provided ATM as an option

https://en.wikipedia.org/wiki/Asynchronous_Transfer_Mode

- **Frame Relay protocols (1993-1998)**

  * RFC 1490 – Multiprotocol Interconnect over Frame Relay (1993) https://datatracker.ietf.org/doc/html/rfc1490
  * RFC 1973 – PPP in Frame Relay (1996) https://datatracker.ietf.org/doc/html/rfc1973
  * RFC 2427 – Multiprotocol Interconnect over Frame Relay (1998) https://datatracker.ietf.org/doc/html/rfc2427

https://en.wikipedia.org/wiki/Frame_Relay

### Network layer

- **IPv6 - Internet Protocol version 6 (1995)**

most recent version of the Internet Protocol (IP), the communications protocol that provides an identification and location system for computers on networks and routes traffic across the Internet. 
Introduced in 1995, IPv6 was developed by the Internet Engineering Task Force (IETF) to deal with the long-anticipated problem of IPv4 address exhaustion, and is intended to replace IPv4.
In December 1998, IPv6 became a Draft Standard for the IETF, which subsequently ratified it as an Internet Standard on 14 July 2017.
Deployment was slowed by the widespread use of NAT (1999).

https://en.wikipedia.org/wiki/IPv6

- **NAT - Network address translation (1999)** : a method of mapping an IP address space into another by modifying network address information in the IP header of packets while they are in transit across a traffic routing device.
Popular with the exhaustion of IPv4 addresses and home routers. Essential to networking. 
https://en.wikipedia.org/wiki/Network_address_translation

### Transport layer

- **SSL - Secure Socket Layer (1995-1998)**

Netscape developed the original SSL protocols, and Taher Elgamal, chief scientist at Netscape Communications from 1995 to 1998. Superseded by TLS.

https://en.wikipedia.org/wiki/Transport_Layer_Security

- **TLS - Transport Layer Security (1999)** 

A cryptographic protocol designed to provide communications security over a computer network. The protocol is widely used 
in applications such as email, instant messaging, and voice over IP, but its use in securing HTTPS remains the most publicly visible.
Essential to networking and the internet.
https://en.wikipedia.org/wiki/Transport_Layer_Security

- **PPTP - Point-to-Point Tunneling Protocol (1999) - Microsoft**

The Point-to-Point Tunneling Protocol (PPTP) is an obsolete method for implementing virtual private networks. PPTP has many well known security issues.

PPTP uses a TCP control channel and a Generic Routing Encapsulation tunnel to encapsulate PPP packets. Many modern VPNs use various forms of UDP for this same functionality.

The PPTP specification does not describe encryption or authentication features and relies on the Point-to-Point Protocol being tunneled to implement any and all security functionalities.

The PPTP implementation that ships with the Microsoft Windows product families implements various levels of authentication and encryption natively as standard features of the Windows PPTP stack. The intended use of this protocol is to provide security levels and remote access levels comparable with typical VPN products.

https://en.wikipedia.org/wiki/Point-to-Point_Tunneling_Protocol

### Session layer

- **SOCKS - SOCKS4 (1992) - SOCKS5 (1996)**

SOCKS is a de facto standard for circuit-level gateways (level 5 gateways).[6]

The circuit/session level nature of SOCKS make it a versatile tool in forwarding any TCP (or UDP since SOCKS5) traffic, creating an interface for all types of routing tools. It can be used as:

A circumvention tool, allowing traffic to bypass Internet filtering to access content otherwise blocked, e.g., by governments, workplaces, schools, and country-specific web services.[7] Since SOCKS is very detectable, a common approach is to present a SOCKS interface for more sophisticated protocols:
The Tor onion proxy software presents a SOCKS interface to its clients.[8]

Providing similar functionality to a virtual private network, allowing connections to be forwarded to a server's "local" network:
Some SSH suites, such as OpenSSH, support dynamic port forwarding that allows the user to create a local SOCKS proxy.[9] This can free the user from the limitations of connecting only to a predefined remote port and server.

The protocol was originally developed/designed by David Koblas, a system administrator of MIPS Computer Systems. After MIPS was taken over by Silicon Graphics in 1992, Koblas presented a paper on SOCKS at that year's Usenix Security Symposium,[2] making SOCKS publicly available.[3] The protocol was extended to version 4 by Ying-Da Lee of NEC.

The SOCKS reference architecture and client are owned by Permeo Technologies,[4] a spin-off from NEC. (Blue Coat Systems bought out Permeo Technologies, and were in turn acquired by Symantec.)

The SOCKS5 protocol was originally a security protocol that made firewalls and other security products easier to administer. It was approved by the IETF in 1996 as RFC 1928 (authored by: M. Leech, M. Ganis, Y. Lee, R. Kuris, D. Koblas, and L. Jones). The protocol was developed in collaboration with Aventail Corporation, which markets the technology outside of Asia.[5]

https://en.wikipedia.org/wiki/SOCKS

### Application layer

- **HTTP/0.9 - Hypertext Transfer Protocol (1991)**

application layer protocol in the Internet protocol suite model for distributed, collaborative, hypermedia information systems. Essential to the internet.

https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol

- **DHCP - Dynamic Host Configuration Protocol (1991)** 

A network management protocol used on Internet Protocol (IP) networks for automatically assigning IP addresses and other communication parameters to devices connected to the network using a client–server architecture.
Essential to networking.

https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol

- **RADIUS (1991)**

Remote Authentication Dial-In User Service (RADIUS) is a networking protocol that provides centralized authentication, authorization, and accounting (AAA) management for users who connect and use a network service. RADIUS was developed by Livingston Enterprises in 1991 as an access server authentication and accounting protocol. It was later brought into IEEE 802 and IETF standards.

RADIUS is a client/server protocol that runs in the application layer, and can use either TCP or UDP. Network access servers, which control access to a network, usually contain a RADIUS client component that communicates with the RADIUS server.[1] RADIUS is often the back-end of choice for 802.1X authentication.[2] A RADIUS server is usually a background process running on UNIX or Microsoft Windows.[1]
As more dial-up customers used the NSFNET a request for proposal was sent out by Merit Network in 1991 to consolidate their various proprietary authentication, authorization and accounting systems. Among the early respondents was Livingston Enterprises and an early version of the RADIUS was written after a meeting. The early RADIUS server was installed on a UNIX operating system. Livingston Enterprises was acquired by Lucent and together with Merit steps were taken to gain industry acceptance for RADIUS as a protocol. Both companies offered a RADIUS server at no charge.[11] In 1997 RADIUS was published as RFC 2058 and RFC 2059, current versions are RFC 2865 and RFC 2866.[12]

https://en.wikipedia.org/wiki/RADIUS

- **LDAP (1993)**

The Lightweight Directory Access Protocol (LDAP /ˈɛldæp/) is an open, vendor-neutral, industry standard application protocol for accessing and maintaining distributed directory information services over an Internet Protocol (IP) network.[1] Directory services play an important role in developing intranet and Internet applications by allowing the sharing of information about users, systems, networks, services, and applications throughout the network.[2] As examples, directory services may provide any organized set of records, often with a hierarchical structure, such as a corporate email directory. Similarly, a telephone directory is a list of subscribers with an address and a phone number.
LDAP is specified in a series of Internet Engineering Task Force (IETF) Standard Track publications called Request for Comments (RFCs), using the description language ASN.1. The latest specification is Version 3, published as RFC 4511[3] (a road map to the technical specifications is provided by RFC4510).
A common use of LDAP is to provide a central place to store usernames and passwords. This allows many different applications and services to connect to the LDAP server to validate users.[4]
LDAP is based on a simpler subset of the standards contained within the X.500 standard. Because of this relationship, LDAP is sometimes called X.500-lite.[5]
The protocol was originally created[7] by Tim Howes of the University of Michigan, Steve Kille of Isode Limited, Colin Robbins of Nexor and Wengyik Yeong of Performance Systems International, circa 1993, as a successor[8] to DIXIE and DAS. Mark Wahl of Critical Angle Inc., Tim Howes, and Steve Kille started work in 1996 on a new version of LDAP, LDAPv3, under the aegis of the Internet Engineering Task Force (IETF). LDAPv3, first published in 1997, superseded LDAPv2 and added support for extensibility, integrated the Simple Authentication and Security Layer, and better aligned the protocol to the 1993 edition of X.500. Further development of the LDAPv3 specifications themselves and of numerous extensions adding features to LDAPv3 has come through the IETF.

https://en.wikipedia.org/wiki/Lightweight_Directory_Access_Protocol

### Data transmission

#### Text

- **HTML (1990-1991) - HyperText Markup Language**

In 1980, physicist Tim Berners-Lee, a contractor at CERN, proposed and prototyped ENQUIRE, a system for CERN researchers to use and share documents. In 1989, Berners-Lee wrote a memo proposing an Internet-based hypertext system.[3] Berners-Lee specified HTML and wrote the browser and server software in late 1990. 
The first publicly available description of HTML was a document called "HTML Tags", first mentioned on the Internet by Tim Berners-Lee in late 1991.

https://en.wikipedia.org/wiki/HTML

- **URL (1992-1994) - Uniform Resource Locators**

Uniform Resource Locators were defined in RFC 1738 in 1994 by Tim Berners-Lee, the inventor of the World Wide Web, and the URI working group of the Internet Engineering Task Force (IETF),[7] as an outcome of collaboration started at the IETF Living Documents birds of a feather session in 1992.[7][8]

https://en.wikipedia.org/wiki/URL

#### Images

- **WMF - Windows Metafile, SVG ancestor (1991) - Microsoft**

Long before the invention of Scalable Vector Graphics, Microsoft Corporation recognized the value of recording images in a format that its applications and operating systems could easily render irrespective of the output device.  With the release of Windows 3.0, Microsoft released its Windows Metafile (WMF) format, which can contain vector and raster graphics in one package.  

https://datatracker.ietf.org/doc/html/rfc7903

- **JPEG (1992)**

Picture compression format.

https://en.wikipedia.org/wiki/JPEG

- **PNG (1997)** 

Portable Network Graphics (PNG, officially pronounced /pɪŋ/[2][3] PING, colloquially pronounced /ˌpiːɛnˈdʒiː/[4] PEE-en-JEE) is a raster-graphics file format that supports lossless data compression.
PNG was published as informational RFC 2083 in March 1997 and as an ISO/IEC 15948 standard in 2004

https://en.wikipedia.org/wiki/Portable_Network_Graphics

#### Music and Video

- **RIFF - Resource Interchange File Format (1991) - Microsoft and IBM**

The Resource Interchange File Format (RIFF) is a generic file container format for storing data in tagged chunks.[2] It is primarily used to store multimedia such as sound and video, though it may also be used to store any arbitrary data.[3]
The Microsoft implementation is mostly known through container formats like AVI, ANI and WAV, which use RIFF as their basis.[4]
RIFF was introduced in 1991 by Microsoft and IBM, and was presented by Microsoft as the default format for Windows 3.1 multimedia files. It is based on Electronic Arts' Interchange File Format, introduced in 1985 on the Commodore Amiga, the only difference being that multi-byte integers are in little-endian format, native to the 80x86 processor series used in IBM PCs, rather than the big-endian format native to the 68k processor series used in Amiga and Apple Macintosh computers, where IFF files were heavily used. A RIFX format, which is big-endian, was also introduced.

https://en.wikipedia.org/wiki/Resource_Interchange_File_Format

- **Quicktime (1991-2018) - Apple**

QuickTime is an extensible multimedia framework developed by Apple Inc., capable of handling various formats of digital video, picture, sound, panoramic images, and interactivity. Created in 1991, the latest Mac version, QuickTime X, is available for Mac OS X Snow Leopard up to macOS Mojave. Apple ceased support for the Windows version of QuickTime in 2016, and ceased support for QuickTime 7 on macOS in 2018

Apple released the first version of QuickTime on December 2, 1991 as a multimedia add-on for System 6 and later. The lead developer of QuickTime, Bruce Leak, ran the first public demonstration at the May 1991 Worldwide Developers Conference, where he played Apple's famous 1984 advertisement in a window at 320×240 pixels resolution.

QuickTime 1.x

The original video codecs included:
  * the Animation codec, which used run-length encoding and was better suited to cartoon-type images with large areas of flat color
  * the Apple Video codec (also known as "Road Pizza"), suited to normal live-action video.[32]
  * the Graphics codec, for 8-bit images, including ones that had undergone dithering
The first commercial project produced using QuickTime 1.0 was the CD-ROM From Alice to Ocean. The first publicly visible use of QuickTime was Ben & Jerry's interactive factory tour (dubbed The Rik & Joe Show after its in-house developers). The Rik and Joe Show was demonstrated onstage at MacWorld in San Francisco when John Sculley announced QuickTime.[33]

Apple released QuickTime 1.5 for Mac OS in the latter part of 1992. This added the SuperMac-developed Cinepak vector-quantization video codec (initially known as Compact Video). It could play video at 320×240 resolution at 30 frames per second on a 25 MHz Motorola 68040 CPU. It also added text tracks, which allowed for captioning, lyrics and other potential uses.

It was followed by several version (up to version 7 and X) in the 2000s and 2010s.

https://en.wikipedia.org/wiki/QuickTime


- **WAV - Uncompressed sound format (1991) - Microsoft and IBM**

Waveform Audio File Format[3] (WAVE,[3] or WAV due to its filename extension;[3][6][7] pronounced "wave"[8]) is an audio file format standard, developed by IBM and Microsoft, for storing an audio bitstream on PCs. It is the main format used on Microsoft Windows systems for uncompressed audio. The usual bitstream encoding is the linear pulse-code modulation (LPCM) format.
WAV is an application of the Resource Interchange File Format (RIFF) bitstream format method for storing data in chunks, and thus is similar to the 8SVX and the AIFF format used on Amiga and Macintosh computers, respectively.
August 1991

https://en.wikipedia.org/wiki/WAV

- **AVI - Audio Video Interleave (1992) - Microsoft**

Audio Video Interleave (also Audio Video Interleaved and known by its initials and filename extension AVI, usually pronounced /ˌeɪ.viːˈaɪ/[3]), is a proprietary multimedia container format and Windows standard[4] introduced by Microsoft in November 1992 as part of its Video for Windows software. AVI files can contain both audio and video data in a file container that allows synchronous audio-with-video playback. Like the DVD video format, AVI files support multiple streaming audio and video, although these features are seldom used.

https://en.wikipedia.org/wiki/Audio_Video_Interleave

- **MP3 (1993)**

MP3 (formally MPEG-1 Audio Layer III or MPEG-2 Audio Layer III)[4] is a coding format for digital audio developed largely by the Fraunhofer Society in Germany, with support from other digital scientists in the United States and elsewhere. Originally defined as the third audio format of the MPEG-1 standard, it was retained and further extended — defining additional bit-rates and support for more audio channels — as the third audio format of the subsequent MPEG-2 standard. A third version, known as MPEG 2.5 — extended to better support lower bit rates — is commonly implemented, but is not a recognized standard.
On 7 July 1994, the Fraunhofer Society released the first software MP3 encoder, called l3enc.[59] The filename extension .mp3 was chosen by the Fraunhofer team on 14 July 1995 (previously, the files had been named .bit).[1] With the first real-time software MP3 player WinPlay3 (released 9 September 1995) many people were able to encode and play back MP3 files on their PCs. Because of the relatively small hard drives of the era (≈500–1000 MB) lossy compression was essential to store multiple albums' worth of music on a home computer as full recordings (as opposed to MIDI notation, or tracker files which combined notation with short recordings of instruments playing single notes).

https://en.wikipedia.org/wiki/MP3

#### File compression

- **DEFLATE (1993-1996)**

In computing, Deflate (stylized as DEFLATE) is a lossless data compression file format that uses a combination of LZ77 and Huffman coding. It was designed by Phil Katz, for version 2 of his PKZIP archiving tool. Deflate was later specified in RFC 1951 (1996).[1]

Katz also designed the original algorithm used to construct Deflate streams. This algorithm was patented as U.S. Patent 5,051,745, and assigned to PKWARE, Inc.[2][3] As stated in the RFC document, an algorithm producing Deflate files was widely thought to be implementable in a manner not covered by patents.[1] This led to its widespread use – for example, in gzip compressed files and PNG image files, in addition to the ZIP file format for which Katz originally designed it. The patent has since expired.

https://en.wikipedia.org/wiki/Deflate

- **Winzip - Popular ZIP Gui on Windows (1991)** 

WinZip is a trialware file archiver and compressor for Windows, macOS, iOS and Android. It is developed by WinZip Computing (formerly Nico Mak Computing), which is owned by Corel Corporation. The program can create archives in Zip file format, unpack some other archive file formats and it also has various tools for system integration.

WinZip 1.0 was released in April 1991 as a Graphical User Interface (GUI) front-end for PKZI.

https://en.wikipedia.org/wiki/WinZip

- **ARJ (1993)**

ARJ (Archived by Robert Jung) is a software tool designed by Robert K. Jung for creating high-efficiency compressed file archives. ARJ is currently on version 2.86 for MS-DOS and 3.20 for Microsoft Windows and supports 16-bit, 32-bit and 64-bit Intel architectures.[1]

ARJ was one of many file compression utilities for MS-DOS and Microsoft Windows during the early and mid-1990s. Parts of ARJ were covered by U.S. Patent 5,140,321 (expired). ARJ is well-documented and includes over 150 command line switches.

https://en.wikipedia.org/wiki/ARJ

- **RAR (1993)**

RAR is a proprietary archive file format that supports data compression, error correction and file spanning.[3] It was developed in 1993 by Russian software engineer Eugene Roshal and the software is licensed by win.rar GmbH.[3] The name RAR stands for Roshal Archive.

https://en.wikipedia.org/wiki/RAR_(file_format)

- **7ZIP - 7z file format, ZIP improvement (1999) - Igor Pavlov**

7-Zip is a free and open-source file archiver, a utility used to place groups of files within compressed containers known as "archives". It is developed by Igor Pavlov and was first released in 1999.[2] 7-Zip has its own archive format called 7z, but can read and write several others.

By default, 7-Zip creates 7z-format archives with a .7z file extension.

In 2011, TopTenReviews found that the 7z compression was at least 17% better than ZIP,[17] and 7-Zip's own site has since 2002 reported that while compression ratio results are very dependent upon the data used for the tests, "Usually, 7-Zip compresses to 7z format 30–70% better than to zip format, and 7-Zip compresses to zip format 2–10% better than most other zip-compatible programs."[18

https://en.wikipedia.org/wiki/7-Zip

#### Voice over IP

- **SIP - Session Initiation Protocol (1996)** 

Signaling protocol used for initiating, maintaining, and terminating communication sessions that include voice, video and messaging applications. 
SIP is used in **Internet telephony**, in private IP telephone systems, as well as mobile phone calling over LTE (**VoLTE**). Essential to telecommunications.

https://en.wikipedia.org/wiki/Session_Initiation_Protocol

#### Video over IP

- **MPEG-1 (1993)**

Since 1972, International Telecommunication Union's radio telecommunications sector (ITU-R) had been working on creating a global recommendation for Analog HDTV. These recommendations, however, did not fit in the broadcasting bands which could reach home users. The standardization of MPEG-1 in 1993 led to the acceptance of recommendations ITU-R BT.709

https://en.wikipedia.org/wiki/High-definition_television

- **MPEG-2 (1995)**

MPEG-2 evolved out of the shortcomings of MPEG-1.
MPEG-1's known weaknesses:
An audio compression system limited to two channels (stereo).
No standardized support for interlaced video with poor compression when used for interlaced video
Only one standardized "profile" (Constrained Parameters Bitstream), which was unsuited for higher resolution video. MPEG-1 could support 4k video but there was no easy way to encode video for higher resolutions, and identify hardware capable of supporting it, as the limitations of such hardware were not defined.
Support for only one chroma subsampling, 4:2:0.
Sakae Okubo of NTT was the ITU-T coordinator for developing the H.262/MPEG-2 Part 2 video coding standard and the requirements chairman in MPEG for the MPEG-2 set of standards.[27] The majority of patents underlying MPEG-2 technology are owned by three companies: Sony (311 patents), Thomson (198 patents) and Mitsubishi Electric (119 patents).[28] Hyundai Electronics (now SK Hynix) developed the first MPEG-2 SAVI (System/Audio/Video) decoder in 1995.[29]

https://en.wikipedia.org/wiki/MPEG-2

- **H.323 (1996)** 

Recommendation from the ITU Telecommunication Standardization Sector (ITU-T) that defines the protocols to provide audio-visual communication sessions on any packet network.[1] The H.323 standard addresses call signaling and control, multimedia transport and control, and bandwidth control for point-to-point and multi-point conferences.[2]
It is widely implemented[3] by **voice and videoconferencing equipment manufacturers**, is used within various **Internet real-time applications** such as GnuGK and NetMeeting and is widely deployed worldwide by service providers and enterprises for both voice and video services over IP networks.

https://en.wikipedia.org/wiki/H.323

- **H.263 (1996)**

The H.263 standard was first designed to be utilized in H.324 based systems (PSTN and other **circuit-switched network videoconferencing and videotelephony)**, but it also found use in H.323 (RTP/IP-based videoconferencing), H.320 (ISDN-based videoconferencing, where it became the most widely used video compression standard),[4] RTSP (streaming media) and SIP (IP-based videoconferencing) solutions.

https://en.wikipedia.org/wiki/H.263

#### Instant messaging

- **XMPP (1999)**
 
Open communication protocol designed for **instant messaging (IM), presence information, and contact list maintenance**. enables the near-real-time exchange of structured data between two or more network entities. Widely in use.

https://en.wikipedia.org/wiki/XMPP

#### Machine-to-machine

- **CORBA - Common Object Request Broker Architecture (1991)**

The Common Object Request Broker Architecture (CORBA) is a standard defined by the Object Management Group (OMG) designed to facilitate the communication of systems that are deployed on diverse platforms. CORBA enables collaboration between systems on different operating systems, programming languages, and computing hardware. CORBA uses an object-oriented model although the systems that use the CORBA do not have to be object-oriented. CORBA is an example of the distributed object paradigm.

CORBA enables communication between software written in different languages and running on different computers. Implementation details from specific operating systems, programming languages, and hardware platforms are all removed from the responsibility of developers who use CORBA. CORBA normalizes the method-call semantics between application objects residing either in the same address-space (application) or in remote address-spaces (same host, or remote host on a network). Version 1.0 was released in October 1991.

https://en.wikipedia.org/wiki/Common_Object_Request_Broker_Architecture

- **XML (1998) - Extensible Markup Language**

A markup language and file format for storing, transmitting, and reconstructing arbitrary data. It defines a set of rules for encoding documents in a format that is both human-readable and machine-readable. The World Wide Web Consortium's XML 1.0 Specification[2] of 1998[3] and several other related specifications[4]—all of them free open standards—define XML.[5]

https://en.wikipedia.org/wiki/XML

- **MQTT (1999)**

Lightweight, **publish-subscribe, machine to machine network protocol**. It is designed for connections with remote locations that have devices with resource constraints or limited network bandwidth. Widely in use.

https://en.wikipedia.org/wiki/MQTT

#### Files

- **CIFS (1996) - Microsoft**

In 1996, Microsoft published a version of SMB 1.0[4] with minor modifications under the Common Internet File System (CIFS /sɪfs/) moniker. CIFS was compatible with even the earliest incarnation of SMB, including LAN Manager's.[4] It supports symbolic links, hard links, and larger file size, but none of the features of SMB 2.0 and later.[4][5] Microsoft's proposal, however, remained an Internet Draft and never achieved standard status.[6] Microsoft has since discontinued use of the CIFS moniker but continues developing SMB and making subsequent specifications publicly available.

https://en.wikipedia.org/wiki/Server_Message_Block

- **Rsync (1999)**

Utility for **efficiently transferring and synchronizing files** between a computer and a storage drive and across networked computers by comparing the modification times and sizes of files. Detailed in the creator's PhD Thesis.

https://en.wikipedia.org/wiki/Rsync

#### Remote access
- **SSH - Secure Shell (1995): remote login and command-line execution**

The Secure Shell Protocol (SSH) is a cryptographic network protocol for operating network services securely over an unsecured network.[1] Its most notable applications are remote login and command-line execution.
SSH applications are based on a client–server architecture, connecting an SSH client instance with an SSH server
SSH was first designed in 1995 by Finnish computer scientist Tatu Ylönen. Subsequent development of the protocol suite proceeded in several developer groups, producing several variants of implementation. The protocol specification distinguishes two major versions, referred to as SSH-1 and SSH-2. The most commonly implemented software stack is **OpenSSH**, released in **1999** as open-source software by the OpenBSD developers. 

https://en.wikipedia.org/wiki/Secure_Shell

- **VNC - Virtual Network Computing (1999)** 

**Remote desktop protocol**. Still widely used. Insecure (not encrypted by default).

https://en.wikipedia.org/wiki/Virtual_Network_Computing

## Programming languages and frameworks
**[`^        back to top        ^`](#)**

- **OpenGL (1991) - Silicon Graphics**

OpenGL (Open Graphics Library[3]) is a cross-language, cross-platform application programming interface (API) for rendering 2D and 3D vector graphics. The API is typically used to interact with a graphics processing unit (GPU), to achieve hardware-accelerated rendering.

Silicon Graphics, Inc. (SGI) began developing OpenGL in 1991 and released it on June 30, 1992;[4][5] applications use it extensively in the fields of computer-aided design (CAD), virtual reality, scientific visualization, information visualization, flight simulation, and video games. Since 2006, OpenGL has been managed by the non-profit technology consortium Khronos Group.[6]

https://en.wikipedia.org/wiki/OpenGL

- **DirectX (1995)**

Microsoft DirectX is a collection of application programming interfaces (APIs) for handling tasks related to multimedia, especially game programming and video, on Microsoft platforms. Originally, the names of these APIs all began with "Direct", such as Direct3D, DirectDraw, DirectMusic, DirectPlay, DirectSound, and so forth. The name DirectX was coined as a shorthand term for all of these APIs (the X standing in for the particular API names) and soon became the name of the collection. When Microsoft later set out to develop a gaming console, the X was used as the basis of the name Xbox to indicate that the console was based on DirectX technology.[3] The X initial has been carried forward in the naming of APIs designed for the Xbox such as XInput and the Cross-platform Audio Creation Tool (XACT), while the DirectX pattern has been continued for Windows APIs such as Direct2D and DirectWrite.

Direct3D (the 3D graphics API within DirectX) is widely used in the development of video games for Microsoft Windows and the Xbox line of consoles. Direct3D is also used by other software applications for visualization and graphics tasks such as CAD/CAM engineering. As Direct3D is the most widely publicized component of DirectX, it is common to see the names "DirectX" and "Direct3D" used interchangeably.

The DirectX software development kit (SDK) consists of runtime libraries in redistributable binary form, along with accompanying documentation and headers for use in coding. Originally, the runtimes were only installed by games or explicitly by the user. Windows 95 did not launch with DirectX, but DirectX was included with Windows 95 OEM Service Release 2.[4] Windows 98 and Windows NT 4.0 both shipped with DirectX, as has every version of Windows released since. The SDK is available as a free download. While the runtimes are proprietary, closed-source software, source code is provided for most of the SDK samples. Starting with the release of Windows 8 Developer Preview, DirectX SDK has been integrated into Windows SDK.[5]

https://en.wikipedia.org/wiki/DirectX

- **Direct3D (1996) - Microsoft**

Direct3D is a graphics application programming interface (API) for Microsoft Windows. Part of DirectX, Direct3D is used to render three-dimensional graphics in applications where performance is important, such as games. Direct3D uses hardware acceleration if it is available on the graphics card, allowing for hardware acceleration of the entire 3D rendering pipeline or even only partial acceleration. Direct3D exposes the advanced graphics capabilities of 3D graphics hardware, including Z-buffering,[1] W-buffering,[2] stencil buffering, spatial anti-aliasing, alpha blending, color blending, mipmapping, texture blending,[3][4] clipping, culling, atmospheric effects, perspective-correct texture mapping, programmable HLSL shaders[5] and effects.[6] Integration with other DirectX technologies enables Direct3D to deliver such features as video mapping, hardware 3D rendering in 2D overlay planes, and even sprites, providing the use of 2D and 3D graphics in interactive media ties.

https://en.wikipedia.org/wiki/Direct3D

- **Unreal Engine (1998) - Unreal**

Unreal Engine (UE) is a 3D computer graphics game engine developed by Epic Games, first showcased in the 1998 first-person shooter game Unreal. Initially developed for PC first-person shooters, it has since been used in a variety of genres of games and has seen adoption by other industries, most notably the film and television industry. Written in C++, the Unreal Engine features a high degree of portability, supporting a wide range of desktop, mobile, console and virtual reality platforms.

https://en.wikipedia.org/wiki/Unreal_Engine

### Automation
- **LonWork (1999)** 

LonWorks or Local Operating Network is an open standard (ISO/IEC 14908) for networking platforms specifically created to address the needs of control applications. The platform is built on a protocol created by Echelon Corporation for networking devices over media such as twisted pair, powerlines, fibre optics , and RF. It is used for the automation of various functions within buildings such as lighting and HVAC; see building automation.
The technology has its origins with chip designs, power line and twisted pair, signaling technology, routers, network management software, and other products from Echelon Corporation. In 1999 the communications protocol (then known as LonTalk) was submitted to ANSI and accepted as a standard for control networking (ANSI/CEA-709.1-B). Echelon's power line and twisted pair signaling technology was also submitted to ANSI for standardization and accepted. Since then, ANSI/CEA-709.1 has been accepted as the basis for IEEE 1473-L (in-train controls), AAR electro-pneumatic braking systems for freight trains, IFSF (European petrol station control), SEMI (semiconductor equipment manufacturing), and in 2005 as EN 14908 (European building automation standard). The protocol is also one of several data link/physical layers of the BACnet ASHRAE/ANSI standard for building automation.
China ratified the technology as a national controls standard, GB/Z 20177.1-2006 and as a building and intelligent community standard, GB/T 20299.4-2006; and in 2007 CECED, the European Committee of Domestic Equipment Manufacturers, adopted the protocol as part of its Household Appliances Control and Monitoring – Application Interworking Specification (AIS) standards.
During 2008 ISO and IEC have granted the communications protocol, twisted pair signaling technology, power line signaling technology, and Internet Protocol (IP) compatibility standard numbers ISO/IEC 14908-1, -2, -3, and -4.[1]

https://en.wikipedia.org/wiki/LonWorks

## Navigation
**[`^        back to top        ^`](#)**

**GPS is operational, military use, miniaturization and shift to dual-use**

The Gulf War from 1990 to 1991 was the first conflict in which the military widely used GPS.[58]

In 1991, a project to create a miniature GPS receiver successfully ended, replacing the previous 16 kg (35 lb) military receivers with a 1.25 kg (2.8 lb) handheld receiver.[26]

In 1992, the 2nd Space Wing, which originally managed the system, was inactivated and replaced by the 50th Space Wing.

By December 1993, GPS achieved initial operational capability (IOC), with a full constellation (24 satellites) available and providing the Standard Positioning Service (SPS).[59]

Full Operational Capability (FOC) was declared by Air Force Space Command (AFSPC) in April 1995, signifying full availability of the military's secure Precise Positioning Service (PPS).[59]

In 1996, recognizing the importance of GPS to civilian users as well as military users, U.S. President Bill Clinton issued a policy directive[60] declaring GPS a dual-use system and establishing an Interagency GPS Executive Board to manage it as a national asset.

In 1998, United States Vice President Al Gore announced plans to upgrade GPS with two new civilian signals for enhanced user accuracy and reliability, particularly with respect to aviation safety, and in 2000 the United States Congress authorized the effort, referring to it as GPS III.

https://en.wikipedia.org/wiki/Global_Positioning_System
