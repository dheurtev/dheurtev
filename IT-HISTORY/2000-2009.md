# 2000s: IP convergence, Legacy free PCs, Widescreen, Transition to SSD, USB/SATA/SAS/PCI-E iterations, Southbridge disappears, Blu-Ray, HDTV in Europe, Multi-mode fiber

## Telecommunications

Whilst the following decade saw a convergence of the aforementioned services (frame relay, ATM, Internet for businesses) with the MPLS integrated offerings.

https://en.wikipedia.org/wiki/Leased_line

## Networking



## Cryptography

## Computers

### Evolution of mainframes

In the late 1990s, corporations found new uses for their existing mainframes and as the price of data networking collapsed in most parts of the world, encouraging trends toward more centralized computing. The growth of e-business also dramatically increased the number of back-end transactions processed by mainframe software as well as the size and throughput of databases. Batch processing, such as billing, became even more important (and larger) with the growth of e-business, and mainframes are particularly adept at large-scale batch computing. Another factor currently increasing mainframe use is the development of the Linux operating system, which arrived on IBM mainframe systems in 1999 and is typically run in scores or up to c. 8,000 virtual machines on a single mainframe. Linux allows users to take advantage of open source software combined with mainframe hardware RAS. Rapid expansion and development in emerging markets, particularly People's Republic of China, is also spurring major mainframe investments to solve exceptionally difficult computing problems, e.g. providing unified, extremely high volume online transaction processing databases for 1 billion consumers across multiple industries (banking, insurance, credit reporting, government services, etc.) In late 2000, IBM introduced 64-bit z/Architecture, acquired numerous software companies such as Cognos and introduced those software products to the mainframe. IBM's quarterly and annual reports in the 2000s usually reported increasing mainframe revenues and capacity shipments. However, IBM's mainframe hardware business has not been immune to the recent overall downturn in the server hardware market or to model cycle effects. For example, in the 4th quarter of 2009, IBM's System z hardware revenues decreased by 27% year over year. But MIPS (millions of instructions per second) shipments increased 4% per year over the past two years.[21] Alsop had himself photographed in 2000, symbolically eating his own words ("death of the mainframe").[22]

https://en.wikipedia.org/wiki/Mainframe_computer

By 2000, modern mainframes partially or entirely phased out classic "green screen" and color display terminal access for end-users in favour of Web-style user interface.

https://en.wikipedia.org/wiki/Mainframe_computer


### Evolution of Hardware

- **Transition to legacy free PCs (2000s)**

As the first decade of the 21st century progressed, the legacy-free PC went mainstream, with legacy ports removed from commonly available computer systems in all form factors. However, the PS/2 keyboard connector still retains some use, as it can offer some uses (e.g. implementation of n-key rollover) not offered by USB.[6]

https://en.wikipedia.org/wiki/Legacy-free_PC

- **Screen Transition from 800x600 to eXtended Graphics Array (XGA) 1024x768 (early 2000s)**

In 2002, 1024 × 768 eXtended Graphics Array was the most common display resolution. Many web sites and multimedia products were re-designed from the previous 800 × 600 format to the layouts optimized for 1024 × 768.
The availability of inexpensive LCD monitors made the 5∶4 aspect ratio resolution of 1280 × 1024 more popular for desktop usage during the first decade of the 21st century. 

https://en.wikipedia.org/wiki/Display_resolution

- **USB 2.0 - Hi-Speed USB (2000)**

USB 2.0 was released in April 2000, adding a higher maximum signaling rate of 480 Mbit/s (maximum theoretical data throughput 53 MByte/s[19]) named High Speed or High Bandwidth, in addition to the USB 1.x Full Speed signaling rate of 12 Mbit/s (maximum theoretical data throughput 1.2 MByte/s[20]).

https://en.wikipedia.org/wiki/USB

- **SATA Hard drives (2000)**

SATA was announced in 2000[5][6] in order to provide several advantages over the earlier PATA interface such as reduced cable size and cost (seven conductors instead of 40 or 80), native hot swapping, faster data transfer through higher signaling rates, and more efficient transfer through an (optional) I/O queuing protocol. Revision 1.0 of the specification was released in January 2003.[3]
Serial ATA (SATA, abbreviated from Serial AT Attachment)[3] is a computer bus interface that connects host bus adapters to mass storage devices such as hard disk drives, optical drives, and solid-state drives. Serial ATA succeeded the earlier Parallel ATA (PATA) standard to become the predominant interface for storage devices.
Serial ATA industry compatibility specifications originate from the Serial ATA International Organization (SATA-IO) which are then promulgated by the INCITS Technical Committee T13, AT Attachment (INCITS T13).

https://en.wikipedia.org/wiki/Serial_ATA

- **Blu-ray Disk (2000-2003) - Sony**

The Blu-ray Disc (BD), often known simply as Blu-ray, is a digital optical disc storage format. It is designed to supersede the DVD format, and capable of storing several hours of high-definition video (HDTV 720p and 1080p). The main application of Blu-ray is as a medium for video material such as feature films and for the physical distribution of video games for the PlayStation 3, PlayStation 4, PlayStation 5, Xbox One, and Xbox Series X. The name "Blu-ray" refers to the blue laser (which is actually a violet laser) used to read the disc, which allows information to be stored at a greater density than is possible with the longer-wavelength red laser used for DVDs.
The BD format was developed by the Blu-ray Disc Association, a group representing makers of consumer electronics, computer hardware, and motion pictures. Sony unveiled the first Blu-ray Disc prototypes in October 2000, and the first prototype player was released in Japan in April 2003. Afterward, it continued to be developed until its official worldwide release on June 20, 2006, beginning the high-definition optical disc format war, where Blu-ray Disc competed with the HD DVD format. Toshiba, the main company supporting HD DVD, conceded in February 2008,[8] and later released its own Blu-ray Disc player in late 2009.[9] According to Media Research, high-definition software sales in the United States were slower in the first two years than DVD software sales.[10] Blu-ray faces competition from video on demand (VOD) and the continued sale of DVDs.[11] In January 2016, 44% of U.S. broadband households had a Blu-ray player.[12]

https://en.wikipedia.org/wiki/Blu-ray

- **HyperTransport (HT) - interconnection of computer processors (2001) - AMD**

HyperTransport (HT), formerly known as Lightning Data Transport (LDT), is a technology for interconnection of computer processors. It is a bidirectional serial/parallel high-bandwidth, low-latency point-to-point link that was introduced on April 2, 2001.[1] The HyperTransport Consortium is in charge of promoting and developing HyperTransport technology.

HyperTransport is best known as the system bus architecture of AMD central processing units (CPUs) from Athlon 64 through AMD FX and the associated motherboard chipsets. HyperTransport has also been used by IBM and Apple for the Power Mac G5 machines, as well as a number of modern MIPS systems.

The current specification HTX 3.1 remained competitive for 2014 high-speed (2666 and 3200 MT/s or about 10.4 GB/s and 12.8 GB/s) DDR4 RAM and slower (around 1 GB/s [1] similar to high end PCIe SSDs ULLtraDIMM flash RAM) technology[clarification needed]—a wider range of RAM speeds on a common CPU bus than any Intel front-side bus. Intel technologies require each speed range of RAM to have its own interface, resulting in a more complex motherboard layout but with fewer bottlenecks. HTX 3.1 at 26 GB/s can serve as a unified bus for as many as four DDR4 sticks running at the fastest proposed speeds. Beyond that DDR4 RAM may require two or more HTX 3.1 buses diminishing its value as unified transport.

https://en.wikipedia.org/wiki/HyperTransport

- **PCI Express 1.0a (2003) - PCI-SIG**

Transfer rate is expressed in transfers per second instead of bits per second because the number of transfers includes the overhead bits, which do not provide additional throughput;[48] PCIe 1.x uses an 8b/10b encoding scheme, resulting in a 20% (= 2/10) overhead on the raw channel bandwidth.[49] So in the PCIe terminology, transfer rate refers to the encoded bit rate: 2.5 GT/s is 2.5 Gbps on the encoded serial link. This corresponds to 2.0 Gbps of pre-coded data or 250 MB/s, which is referred to as throughput in PCIe.
While in early development, PCIe was initially referred to as HSI (for High Speed Interconnect), and underwent a name change to 3GIO (for 3rd Generation I/O) before finally settling on its PCI-SIG name PCI Express. A technical working group named the Arapaho Work Group (AWG) drew up the standard. For initial drafts, the AWG consisted only of Intel engineers; subsequently, the AWG expanded to include industry partners.
In 2003, PCI-SIG introduced PCIe 1.0a, with a per-lane data rate of 250 MB/s and a transfer rate of 2.5 gigatransfers per second (GT/s).
Since, PCIe has undergone several large and smaller revisions, improving on performance and other features.
Transfer rate is expressed in transfers per second instead of bits per second because the number of transfers includes the overhead bits, which do not provide additional throughput;[48] PCIe 1.x uses an 8b/10b encoding scheme, resulting in a 20% (= 2/10) overhead on the raw channel bandwidth.[49] So in the PCIe terminology, transfer rate refers to the encoded bit rate: 2.5 GT/s is 2.5 Gbps on the encoded serial link. This corresponds to 2.0 Gbps of pre-coded data or 250 MB/s, which is referred to as throughput in PCIe.

https://en.wikipedia.org/wiki/PCI_Express

- **Non-volatile RAM prototypes (2003-2004)**

Several new types of non-volatile RAM, which preserve data while powered down, are under development. The technologies used include carbon nanotubes and approaches utilizing Tunnel magnetoresistance. Amongst the 1st generation MRAM, a 128 kbit (128 × 210 bytes) chip was manufactured with 0.18 µm technology in the summer of 2003.[citation needed] 

In June 2004, Infineon Technologies unveiled a 16 MB (16 × 220 bytes) prototype again based on 0.18 µm technology. There are two 2nd generation techniques currently in development: thermal-assisted switching (TAS)[28] which is being developed by Crocus Technology, and spin-transfer torque (STT) on which Crocus, Hynix, IBM, and several other companies are working.[29] Nantero built a functioning carbon nanotube memory prototype 10 GB (10 × 230 bytes) array in 2004. Whether some of these technologies can eventually take significant market share from either DRAM, SRAM, or flash-memory technology, however, remains to be seen.

https://en.wikipedia.org/wiki/Random-access_memory#History

- **SAS-1 3GB/s - Serial Attached SCSI (2004)**

In computing, Serial Attached SCSI (SAS) is a point-to-point serial protocol that moves data to and from computer-storage devices such as hard disk drives and tape drives. SAS replaces the older Parallel SCSI (Parallel Small Computer System Interface, usually pronounced "scuzzy" or "sexy"[3][4]) bus technology that first appeared in the mid-1980s. SAS, like its predecessor, uses the standard SCSI command set. SAS offers optional compatibility with Serial ATA (SATA), versions 2 and later. This allows the connection of SATA drives to most SAS backplanes or controllers. The reverse, connecting SAS drives to SATA backplanes, is not possible.[5]

The T10 technical committee of the International Committee for Information Technology Standards (INCITS) develops and maintains the SAS protocol; the SCSI Trade Association (SCSITA) promotes the technology.
SAS-1: 3.0 Gbit/s, introduced in 2004[7]

https://en.wikipedia.org/wiki/Serial_Attached_SCSI

- **DMI - Link between northbridge and southbridge on computer motherboard (2004) - Intel**

In computing, Direct Media Interface (DMI) is Intel's proprietary link between the northbridge and southbridge on a computer motherboard. It was first used between the 9xx chipsets and the ICH6, released in 2004.

https://en.wikipedia.org/wiki/Direct_Media_Interface

- **16:10 widescreens on latops (2004)**

The popular widescreen format (16:10) appeared in 2004 originally in laptops (14″ / 15″ had 1280×800, 17″ had 1440×900), next year desktop monitors appeared in this format (19″ 1440×900, 22″ 1680×1050, 24″ 1920×1200) and high-end laptops received such resolutions. Widescreen monitors come with VGA and DVI ports, running 4:3 resolutions on VGA cable will make image stretching on full screen, while DVI cable maintain aspect ratio and display black bars.

https://www.teoalida.com/webdesign/screen-resolution/

- **NVMHCI 1.0 - NVMe (2007-2009) Intel**

The first details of a new standard for accessing non-volatile memory emerged at the Intel Developer Forum 2007, when NVMHCI was shown as the host-side protocol of a proposed architectural design that had Open NAND Flash Interface Working Group (ONFI) on the memory (flash) chips side.[15] A NVMHCI working group led by Intel was formed that year. The NVMHCI 1.0 specification was completed in April 2008 and released on Intel's web site.[16][17][18]

Technical work on NVMe began in the second half of 2009.[19] The NVMe specifications were developed by the NVM Express Workgroup, which consists of more than 90 companies; Amber Huffman of Intel was the working group's chair. Version 1.0 of the specification was released on 1 March 2011,[20] while version 1.1 of the specification was released on 11 October 2012.[21] Major features added in version 1.1 are multi-path I/O (with namespace sharing) and arbitrary-length scatter-gather I/O. It is expected that future revisions will significantly enhance namespace management.[19] Because of its feature focus, NVMe 1.1 was initially called "Enterprise NVMHCI".[22] An update for the base NVMe specification, called version 1.0e, was released in January 2013.[23] In June 2011, a Promoter Group led by seven companies was formed.

https://en.wikipedia.org/wiki/NVM_Express

- **Southbridge disappears (2008+) - Intel/AMD**

The southbridge is one of the two chips in the core logic chipset on a personal computer (PC) motherboard, the other being the northbridge. The southbridge typically implements the slower capabilities of the motherboard in a northbridge/southbridge chipset computer architecture. In systems with Intel chipsets, the southbridge is named I/O Controller Hub (ICH), while AMD has named its southbridge Fusion Controller Hub (FCH) since the introduction of its Fusion AMD Accelerated Processing Unit (APU) while moving the functions of the Northbridge onto the CPU die, hence making it similar in function to the Platform hub controller.
The southbridge can usually be distinguished from the northbridge by not being directly connected to the CPU. Rather, the northbridge ties the southbridge to the CPU. Through the use of controller integrated channel circuitry, the northbridge can directly link signals from the I/O units to the CPU for data control and access.
Due to the push for system-on-chip (SoC) processors, modern devices increasingly have the northbridge integrated into the CPU die itself;[further explanation needed] examples are Intel's Sandy Bridge[1] and AMD's Fusion processors,[2] both released in 2011. The southbridge became redundant and it was replaced by the Platform Controller Hub (PCH) architecture introduced with the Intel 5 Series chipset in 2008 while AMD did the same with the release of their first APUs in 2011, naming the PCH the Fusion controller hub (FCH), which was only used on AMD's APUs until 2017 when it began to be used on AMD's Zen architecture while dropping the FCH name. On Intel platforms, all southbridge features and remaining I/O functions are managed by the PCH which is directly connected to the CPU via the Direct Media Interface (DMI).[3] Intel low power processor (Haswell-U and onward) and Ultra low power processor (Haswell-Y and onward) also integrated an on-package PCH. Based on its Chiplet design, AMD Ryzen processors also integrated some southbridge functions, such as some USB interface and some SATA/NVMe interface.[4]

https://en.wikipedia.org/wiki/Southbridge_(computing)

- **USB 3.x - SuperSpeed USB logo (2008)**

The USB 3.0 specification was released on 12 November 2008, with its management transferring from USB 3.0 Promoter Group to the USB Implementers Forum (USB-IF), and announced on 17 November 2008 at the SuperSpeed USB Developers Conference.[23]
USB 3.0 adds a SuperSpeed transfer mode, with associated backward compatible plugs, receptacles, and cables. SuperSpeed plugs and receptacles are identified with a distinct logo and blue inserts in standard format receptacles.

https://en.wikipedia.org/wiki/USB

- **Intel QuickPath Interconnect - Replaced Front-side bus (2008) - Intel**

The Intel QuickPath Interconnect (QPI)[1][2] is a point-to-point processor interconnect developed by Intel which replaced the front-side bus (FSB) in Xeon, Itanium, and certain desktop platforms starting in 2008. It increased the scalability and available bandwidth. Prior to the name's announcement, Intel referred to it as Common System Interface (CSI).[3] Earlier incarnations were known as Yet Another Protocol (YAP) and YAP+.
QPI 1.1 is a significantly revamped version introduced with Sandy Bridge-EP (Romley platform).[4]
QPI was replaced by Intel Ultra Path Interconnect (UPI) in Skylake-SP Xeon processors based on LGA 3647 socket.[5]

https://en.wikipedia.org/wiki/Intel_QuickPath_Interconnect

- **Demonstration of 1TB SSD on PCI Express x8 (2009) - OCZ Technology**

At Cebit 2009, OCZ Technology demonstrated a 1 TB[47] flash SSD using a PCI Express ×8 interface. It achieved a maximum write speed of 0.654 gigabytes per second (GB/s) and maximum read speed of 0.712 GB/s.[48] In December 2009, Micron Technology announced an SSD using a 6 gigabits per second (Gbit/s) SATA interface.[49]

https://en.wikipedia.org/wiki/Solid-state_drive

- **Transition from XGA to 1366x768 screens (late 2000s)**

XGA 1024×768 had over 50% marketshare before 2007 and was overtaken by 1366×768 in March 2012 which reached a peak of 27%+ in 2015. Next popular were 1280×800 with a peak of 19.72% in Nov 2009 and 1280×1024 with peak in 2007

https://www.teoalida.com/webdesign/screen-resolution/

### Evolution of uses

#### Servers


#### Clients
##### Gui

- **WindowsXP SP2 - Bluetooth mice support (2004) - Microsoft**

Windows XP Service Pack 2 introduced a Bluetooth stack, allowing Bluetooth mice to be used without any USB receivers.[108] 
Windows Vista added native support for horizontal scrolling and standardized wheel movement granularity for finer scrolling.[106]

https://en.wikipedia.org/wiki/Computer_mouse

## Consumer electronics

### TV

- **Commercial TV projectors (2004) - Sony**

Sony was offering 4K projectors as early as 2004

https://en.wikipedia.org/wiki/4K_resolution

- **HDTV in Europe (2004) - Belgium**

The first regular broadcasts started on January 1, 2004, when the Belgian company Euro1080 launched the HD1 channel with the traditional Vienna New Year's Concert. Test transmissions had been active since the IBC exhibition in September 2003, but the New Year's Day broadcast marked the official launch of the HD1 channel, and the official start of direct-to-home HDTV in Europe.[44]

https://en.wikipedia.org/wiki/High-definition_television

- **16:9 widescreen TV standards (2008)**

Television industry adopted 16:9 standard in 2008, and because is cheaper to produce both monitors and TV with same aspect ratio, 14″ / 15″ laptops became 1366×768, 17″ laptops became 1600×900, high-end laptops ($1000+) used 1920×1080. New desktop monitors were 18.5″ 1366×768, 20″ 1600×900, 21.5″ / 23″ 1920×1080. QHD, 27″ 2560×1440 appeared in 2009. LG G3 was in 2013 first smartphone with QHD display. Mini-laptops with 10″ screens used 1024×600, called netbooks (smaller than 14-17″ notebooks).

https://www.teoalida.com/webdesign/screen-resolution/

### Automation


## Some standards, languages and protocols

- **G.709 Interfaces for the Optical Transport Network (OTN) (2001)**

ITU-T Recommendation G.709 Interfaces for the Optical Transport Network (OTN) describes a means of communicating data over an optical network.[1] It is a standardized method for transparent transport of services over optical wavelengths in DWDM systems. It is also known as Optical Transport Hierarchy (OTH) standard. The first edition of this protocol was approved in 2001.[2]

https://en.wikipedia.org/wiki/G.709

- **Advanced Video Coding (AVC) - H.264 - MPEG-4 Part 10 (2003)**

Advanced Video Coding (AVC), also referred to as H.264 or MPEG-4 Part 10, is a video compression standard based on block-oriented, motion-compensated coding.[2] It is by far the most commonly used format for the recording, compression, and distribution of video content, used by 91% of video industry developers as of September 2019.[3][4] It supports resolutions up to and including 8K UHD.[5][6]
The intent of the H.264/AVC project was to create a standard capable of providing good video quality at substantially lower bit rates than previous standards (i.e., half or less the bit rate of MPEG-2, H.263, or MPEG-4 Part 2), without increasing the complexity of design so much that it would be impractical or excessively expensive to implement. This was achieved with features such as a reduced-complexity integer discrete cosine transform (integer DCT),[6][7][8][9] variable block-size segmentation, and multi-picture inter-picture prediction. An additional goal was to provide enough flexibility to allow the standard to be applied to a wide variety of applications on a wide variety of networks and systems, including low and high bit rates, low and high resolution video, broadcast, DVD storage, RTP/IP packet networks, and ITU-T multimedia telephony system
H.264 was standardized by the ITU-T Video Coding Experts Group (VCEG) of Study Group 16 together with the ISO/IEC JTC1 Moving Picture Experts Group (MPEG). The project partnership effort is known as the Joint Video Team (JVT). The ITU-T H.264 standard and the ISO/IEC MPEG-4 AVC standard (formally, ISO/IEC 14496-10 – MPEG-4 Part 10, Advanced Video Coding) are jointly maintained so that they have identical technical content. The final drafting work on the first version of the standard was completed in May 2003, and various extensions of its capabilities have been added in subsequent editions. High Efficiency Video Coding (HEVC), a.k.a. H.265 and MPEG-H Part 2 is a successor to H.264/MPEG-4 AVC developed by the same organizations, while earlier standards are still in common use.
H.264 is perhaps best known as being the most commonly used video encoding format on Blu-ray Discs. It is also widely used by streaming Internet sources, such as videos from Netflix, Hulu, Amazon Prime Video, Vimeo, YouTube, and the iTunes Store, Web software such as the Adobe Flash Player and Microsoft Silverlight, and also various HDTV broadcasts over terrestrial (ATSC, ISDB-T, DVB-T or DVB-T2), cable (DVB-C), and satellite (DVB-S and DVB-S2) systems.

https://en.wikipedia.org/wiki/Advanced_Video_Coding

- **G.651.1 - multi-mode optical fiber (MMF) cable (2007)**

an international standard developed by the Standardization Sector of the International Telecommunication Union (ITU-T) that specifies multi-mode optical fiber (MMF) cable.

https://en.wikipedia.org/wiki/G.651.1

- **DVB-T2 - Digital Video Broadcasting — Second Generation Terrestrial (2007)**

DVB-T2 is an abbreviation for "Digital Video Broadcasting — Second Generation Terrestrial"; it is the extension of the television standard DVB-T, issued by the consortium DVB, devised for the broadcast transmission of digital terrestrial television. DVB has been standardized by ETSI.
This system transmits compressed digital audio, video, and other data in "physical layer pipes" (PLPs), using OFDM modulation with concatenated channel coding and interleaving. The higher offered bit rate, with respect to its predecessor DVB-T, makes it a system suited for carrying HDTV signals on the terrestrial TV channel (though many broadcasters still use plain DVB-T for this purpose). As of 2019, it was implemented in broadcasts in the United Kingdom (Freeview HD, eight channels across two multiplexes, plus an extra multiplex in Northern Ireland carrying three SD channels), Italy (Europa 7 HD, twelve channels), Finland (21 channels, five in HD), Germany (six HD (1080p50) channels, with 40 in planning),[1] the Netherlands (Digitenne, 30 HD (1080p50) channels), Sweden (five channels),[2][3] Thailand (41 SD, 9 HD channels),[4] Flanders (18 SD channels), Serbia (eight channels),[5] Ukraine (32 SD and HD channels in four nationwide multiplexes), Croatia (all national, local and pay-TV channels), Denmark (two pay-TV multiplexes with 20 channels), Romania (8 SD channels, 1 HD channel), and some other countries.
In March 2006 DVB decided to study options for an upgraded DVB-T standard. In June 2006, a formal study group named TM-T2 (Technical Module on Next Generation DVB-T) was established by the DVB Group to develop an advanced modulation scheme that could be adopted by a second generation digital terrestrial television standard, to be named DVB-T2.[6]
According to the commercial requirements and call for technologies[7] issued in April 2007, the first phase of DVB-T2 would be devoted to provide optimum reception for stationary (fixed) and portable receivers (i.e., units which can be nomadic, but not fully mobile) using existing aerials, whereas a second and third phase would study methods to deliver higher payloads (with new aerials) and the mobile reception issue. The novel system should provide a minimum 30% increase in payload, under similar channel conditions already used for DVB-T.
The BBC, ITV, Channel 4 and Channel 5 agreed with the regulator Ofcom to convert one UK multiplex (B, or PSB3) to DVB-T2 to increase capacity for HDTV via DTT.[8] They expected the first TV region to use the new standard would be Granada in November 2009 (with existing switched over regions being changed at the same time). It was expected that over time there would be enough DVB-T2 receivers sold to switch all DTT transmissions to DVB-T2, and H.264.
Ofcom published its final decision on 3 April 2008, for HDTV using DVB-T2 and H.264:[9] BBC HD would have one HD slot after digital switchover (DSO) at Granada. ITV and C4 had, as expected, applied to Ofcom for the 2 additional HD slots available from 2009 to 2012.[10]
Ofcom indicated that it found an unused channel covering 3.7 million households in London, which could be used to broadcast the DVB-T2 HD multiplex from 2010, i.e., before DSO in London. Ofcom indicated that they would look for more unused UHF channels in other parts of the UK, that can be used for the DVB-T2 HD multiplex from 2010 until DSO.[11]

https://en.wikipedia.org/wiki/DVB-T2

- **DivX Codec (1998-2007)**

DivX ;-) (not DivX) 3.11 Alpha and later 3.xx versions refers to a hacked version of the Microsoft MPEG-4 Version 3 video codec (not to be confused with MPEG-4 Part 3) from Windows Media Tools 4 codecs.[4][5] The video codec, which was actually not MPEG-4 compliant, was extracted around 1998 by French hacker Jerome Rota (also known as Gej) at Montpellier. The Microsoft codec originally required that the compressed output be put in an ASF file. It was altered to allow other containers such as Audio Video Interleave (AVI).[6] Rota hacked the Microsoft codec because newer versions of the Windows Media Player would not play his video portfolio and résumé that were encoded with it. Instead of re-encoding his portfolio, Rota and German hacker Max Morice decided to reverse engineer the codec, which "took about a week".[7]
In early 2000, Jordan Greenhall recruited Rota to form a company (originally called DivXNetworks, Inc., renamed to DivX, Inc. in 2005) to develop an MPEG-4 codec, from scratch, that would still be backward-compatible with the Microsoft MPEG-4 Version 3 format. This effort resulted first in the release of the "OpenDivX" codec and source code on 15 January 2001. OpenDivX was hosted as an open-source project on the Project Mayo web site hosted at projectmayo.com[8] (the name comes from "mayonnaise", because, according to Rota, DivX and mayonnaise are both "French and very hard to make."[7]). The company's internal developers and some external developers worked jointly on OpenDivX for the next several months, but the project eventually stagnated.
In early 2001, DivX employee "Sparky" wrote a new and improved version of the codec's encoding algorithm known as "encore2". This code was included in the OpenDivX public source repository for a brief time, but then was abruptly removed. The explanation from DivX at the time was that "the community really wants a Winamp, not a Linux." It was at this point that the project forked. That summer, Rota left the French Riviera and moved to San Diego "with nothing but a pack of cigarettes"[9] where he and Greenhall founded what would eventually become DivX, Inc.[7]
On 4 December 2007, native MPEG-4 ASP playback support was added to the Xbox 360,[27] allowing it to play video encoded with DivX and other MPEG-4 ASP codecs.[28]
On 17 December 2007, firmware upgrade 2.10 was released for the Sony PlayStation 3, which included official DivX Certification. Firmware version 2.50 (released on 15 October 2008) included support for the DivX Video on Demand (DivX VOD) service, and firmware version 2.60 (released on 20 January 2009) included official DivX Certification and updated Profile support to version 3.11.[29]

https://en.wikipedia.org/wiki/DivX

