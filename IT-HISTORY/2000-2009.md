# 2000s

**In short:**
- Telecommunications :  All-IP convergence, Fiber optic to home, Multi-mode fiber, Passive fiber optic network, 
- Networking : Internet everywhere, Power over Ethernet, RFID tags + payments + transportation, Industrial Ethernet, MPLS deployment
- Cryptography : 
- Computers :
  * Hardware : Legacy free PCs, 
  * CPU : AMD x86-64, multi-cores begins, virtualization support (AMD-V, VT-x)
  * Motherboard : USB/SATA/SAS/PCI-E iterations, Southbridge disappears, UEFI
  * Storage : Transition to SSD starts, SATA/SAS hard drives
  * Printers : ink color printers are dominant
- Uses :
  * Servers : NginX (web server)
  * GUI : Microsoft Windows XP, Vista, 7
  * Web browsers : Mozilla Firefox
- Consumer Electronics :
  * Hardware : first OLED prototypes and manufacturings, discovery of the potential of quantum dots for display, Inorganic semiconductor microLED invention
  * Distribution media : Blu-Ray, decline of CDs
  * TV / Screens : computer/TV screen convergence, video projectors, HDTV in Europe, first OLED PDA, first OLED TV, LCD displays dominance, LED-backlit LCD ("LED TVs"), CRT peak and demise, plasma peak, 16:9 TV and widescreen laptop (1366x768), Full HD (1080p = 1920x1080)  
  * Satellite : 
  * Video games : Sixth generation video games consoles (Sony PlayStation 2, Microsoft Xbox, Nintendo Wii)
  * Automation : Real-time protocols and automation protocols
- Standards and protocols : L2TP/IPSEC, DivX, MPEG-4, RDP, JSON, SVG, ISO, SMB 2.0, HTML5, DVB-T2
- Programming languages and frameworks : Microsoft .Net Framework, Nvidia CUDA  
- Navigation : GPS for civilians in smartphone, Europe's Galileo, aging GPS infrastructure

## Telecommunications
**[`^        back to top        ^`](#)**

Whilst the following decade saw a convergence of the aforementioned services (frame relay, ATM, Internet for businesses) with the MPLS integrated offerings.

https://en.wikipedia.org/wiki/Leased_line

New development focused on:
- Fiber optic at home (ETTH, PON with 802.3ah)
- MPLS deployment
- Passive fiber optic network (G.983, G.984) deployments

### MPLS Deployment

Multiprotocol Label Switching (MPLS) is a routing technique in telecommunications networks that directs data from one node to the next based on labels rather than network addresses.[1] Whereas network addresses identify endpoints the labels identify established paths between endpoints. MPLS can encapsulate packets of various network protocols, hence the multiprotocol component of the name. MPLS supports a range of access technologies, including T1/E1, ATM, Frame Relay, and DSL.

-2001: First MPLS Request for Comments (RFCs) published[4]
-2002: AToM (L2VPN)
-2004: GMPLS; Large-scale L3VPN
-2006: Large-scale TE "Harsh"
-2007: Large-scale L2VPN
-2009: Label Switching Multicast
-2011: MPLS transport profile

MPLS is currently (as of March 2012) in use in IP-only networks and is standardized by the IETF in RFC 3031. It is deployed to connect as few as two facilities to very large deployments.

In practice, MPLS is mainly used to forward IP protocol data units (PDUs) and Virtual Private LAN Service (VPLS) Ethernet traffic. Major applications of MPLS are telecommunications traffic engineering, and MPLS VPN.

https://en.wikipedia.org/wiki/Multiprotocol_Label_Switching

## Networking
**[`^        back to top        ^`](#)**

New development focused on:
- Internet everywhere with Wifi networks and Home networking over legacy wires (G.hn).
- IP/power convergence with Power over Ethernet (PoE) and Broadband over power lines (IEEE 1675-2008).
- Link aggregation (802.3ad, 802.1AX)
- Industrial Ethernet
- Speed increase over optic fiber : 10 Gb/s (802.3ae)
- Virtual Private Networks : L2TP/IPSEC

### Industrial Ethernet

- **Industrial Ethernet**

Industrial Ethernet (IE) is the use of Ethernet in an industrial environment with protocols that provide determinism and real-time control.[1] Protocols for industrial Ethernet include EtherCAT, EtherNet/IP, PROFINET, POWERLINK, SERCOS III, CC-Link IE, and Modbus TCP.[1][2] Many industrial Ethernet protocols use a modified Media Access Control (MAC) layer to provide low latency and determinism.[1] Some microcontrollers such as Sitara provide industrial Ethernet support.

Industrial Ethernet can also refer to the use of standard Ethernet protocols with rugged connectors and extended temperature switches in an industrial environment, for automation or process control. Components used in plant process areas must be designed to work in harsh environments of temperature extremes, humidity, and vibration that exceed the ranges for information technology equipment intended for installation in controlled environments. The use of fiber-optic Ethernet variants reduces the problems of electrical noise and provides electrical isolation.

Some industrial networks emphasized deterministic delivery of transmitted data, whereas Ethernet used collision detection which made transport time for individual data packets difficult to estimate with increasing network traffic. Typically, industrial uses of Ethernet employ full-duplex standards and other methods so that collisions do not unacceptably influence transmission times.

https://en.wikipedia.org/wiki/Industrial_Ethernet

### NFC

- **NFC / RFID standards and innovation (2002-2010)**

  * March 25, 2002: Philips and Sony agreed to establish a technology specification and created a technical outline.[18] Philips Semiconductors applied for the six fundamental patents of NFC, invented by the Austrian and French engineers Franz Amtmann and Philippe Maugars who received the European Inventor Award in 2015.[19]
  * December 8, 2003: NFC was approved as an ISO/IEC standard and later as an ECMA standard.
  * 2004: Nokia, Philips and Sony established the NFC Forum[20]
  * 2004: Nokia launched NFC shell add-on for Nokia 5140 and later Nokia 3220 models, to be shipped in 2005.[21][22]
  * 2005: Mobile phone experimentations in transports, with payment in May in Hanau (Nokia) and as well validation aboard in October in Nice with Orange and payment in 
shops in October in Caen (Samsung) with first reception of "Fly Tag" informations[23][24][25]
  * 2006: Initial specifications for NFC Tags[26]
  * 2006: Specification for "SmartPoster" records[27]
  * 2007: Innovision’s NFC tags used in the first consumer trial in the UK, in the Nokia 6131 handset.[28]
  * 2008: AirTag launched what it called the first NFC SDK.[29]
  * 2009: In January, NFC Forum released Peer-to-Peer standards to transfer contacts, URLs, initiate Bluetooth, etc.[30]
  * 2009: NFC first used in transports by China Unicom and Yucheng Transportation Card in the tramways and bus of Chongqing on 19 January 2009,[31] then implemented for the first time in a metro network, by China Unicom in Beijing on 31 December 2010.[32]

https://en.wikipedia.org/wiki/Near-field_communication

## Cryptography
**[`^        back to top        ^`](#)**

## Computers
**[`^        back to top        ^`](#)**

### Evolution of mainframes

In the late 1990s, corporations found new uses for their existing mainframes and as the price of data networking collapsed in most parts of the world, encouraging trends toward more centralized computing. The growth of e-business also dramatically increased the number of back-end transactions processed by mainframe software as well as the size and throughput of databases. Batch processing, such as billing, became even more important (and larger) with the growth of e-business, and mainframes are particularly adept at large-scale batch computing. Another factor currently increasing mainframe use is the development of the Linux operating system, which arrived on IBM mainframe systems in 1999 and is typically run in scores or up to c. 8,000 virtual machines on a single mainframe. Linux allows users to take advantage of open source software combined with mainframe hardware RAS. Rapid expansion and development in emerging markets, particularly People's Republic of China, is also spurring major mainframe investments to solve exceptionally difficult computing problems, e.g. providing unified, extremely high volume online transaction processing databases for 1 billion consumers across multiple industries (banking, insurance, credit reporting, government services, etc.) In late 2000, IBM introduced 64-bit z/Architecture, acquired numerous software companies such as Cognos and introduced those software products to the mainframe. IBM's quarterly and annual reports in the 2000s usually reported increasing mainframe revenues and capacity shipments. However, IBM's mainframe hardware business has not been immune to the recent overall downturn in the server hardware market or to model cycle effects. For example, in the 4th quarter of 2009, IBM's System z hardware revenues decreased by 27% year over year. But MIPS (millions of instructions per second) shipments increased 4% per year over the past two years.[21] Alsop had himself photographed in 2000, symbolically eating his own words ("death of the mainframe").[22]

https://en.wikipedia.org/wiki/Mainframe_computer

By 2000, modern mainframes partially or entirely phased out classic "green screen" and color display terminal access for end-users in favour of Web-style user interface.

https://en.wikipedia.org/wiki/Mainframe_computer


### Evolution of Hardware

- **Transition to legacy free PCs (2000s)**

As the first decade of the 21st century progressed, the legacy-free PC went mainstream, with legacy ports removed from commonly available computer systems in all form factors. However, the PS/2 keyboard connector still retains some use, as it can offer some uses (e.g. implementation of n-key rollover) not offered by USB.[6]

https://en.wikipedia.org/wiki/Legacy-free_PC

- **x86-64 architecture**

x86-64 (also known as x64, x86_64, AMD64, and Intel 64)[note 1] is a 64-bit version of the x86 instruction set, first released in 1999. It introduced two new modes of operation, 64-bit mode and compatibility mode, along with a new 4-level paging mode.

With 64-bit mode and the new paging mode, it supports vastly larger amounts of virtual memory and physical memory than was possible on its 32-bit predecessors, allowing programs to store larger amounts of data in memory. x86-64 also expands general-purpose registers to 64-bit, and expands the number of them from 8 (some of which had limited or fixed functionality, e.g. for stack management) to 16 (fully general), and provides numerous other enhancements. Floating-point arithmetic is supported via mandatory SSE2-like instructions, and x87/MMX style registers are generally not used (but still available even in 64-bit mode); instead, a set of 16 vector registers, 128 bits each, is used. (Each register can store one or two double-precision numbers or one to four single-precision numbers, or various integer formats.) In 64-bit mode, instructions are modified to support 64-bit operands and 64-bit addressing mode.

The compatibility mode defined in the architecture allows 16- and 32-bit user applications to run unmodified, coexisting with 64-bit applications if the 64-bit operating system supports them.[11][note 2] As the full x86 16-bit and 32-bit instruction sets remain implemented in hardware without any intervening emulation, these older executables can run with little or no performance penalty,[13] while newer or modified applications can take advantage of new features of the processor design to achieve performance improvements. Also, a processor supporting x86-64 still powers on in real mode for full backward compatibility with the 8086, as x86 processors supporting protected mode have done since the 80286.

The original specification, created by AMD and released in 2000, has been implemented by AMD, Intel, and VIA. The AMD K8 microarchitecture, in the Opteron and Athlon 64 processors, was the first to implement it. This was the first significant addition to the x86 architecture designed by a company other than Intel. Intel was forced to follow suit and introduced a modified NetBurst family which was software-compatible with AMD's specification. VIA Technologies introduced x86-64 in their VIA Isaiah architecture, with the VIA Nano.

The x86-64 architecture is distinct from the Intel Itanium architecture (formerly IA-64). The architectures are not compatible on the native instruction set level, and operating systems and applications compiled for one cannot be run on the other.

https://en.wikipedia.org/wiki/X86-64

- **AMD Opteron - First x86-64 CPU (2000) - AMD**

Opteron is AMD's x86 former server and workstation processor line, and was the first processor which supported the AMD64 instruction set architecture (known generically as x86-64 or AMD64). It was released on April 22, 2003, with the SledgeHammer core (K8) and was intended to compete in the server and workstation markets, particularly in the same segment as the Intel Xeon processor. Processors based on the AMD K10 microarchitecture (codenamed Barcelona) were announced on September 10, 2007, featuring a new quad-core configuration. The most-recently released Opteron CPUs are the Piledriver-based Opteron 4300 and 6300 series processors, codenamed "Seoul" and "Abu Dhabi" respectively.

https://en.wikipedia.org/wiki/Opteron

- **USB 2.0 - Hi-Speed USB (2000)**

USB 2.0 was released in April 2000, adding a higher maximum signaling rate of 480 Mbit/s (maximum theoretical data throughput 53 MByte/s[19]) named High Speed or High Bandwidth, in addition to the USB 1.x Full Speed signaling rate of 12 Mbit/s (maximum theoretical data throughput 1.2 MByte/s[20]).

https://en.wikipedia.org/wiki/USB

- **SATA Hard drives (2000)**

SATA was announced in 2000[5][6] in order to provide several advantages over the earlier PATA interface such as reduced cable size and cost (seven conductors instead of 40 or 80), native hot swapping, faster data transfer through higher signaling rates, and more efficient transfer through an (optional) I/O queuing protocol. Revision 1.0 of the specification was released in January 2003.[3]
Serial ATA (SATA, abbreviated from Serial AT Attachment)[3] is a computer bus interface that connects host bus adapters to mass storage devices such as hard disk drives, optical drives, and solid-state drives. Serial ATA succeeded the earlier Parallel ATA (PATA) standard to become the predominant interface for storage devices.
Serial ATA industry compatibility specifications originate from the Serial ATA International Organization (SATA-IO) which are then promulgated by the INCITS Technical Committee T13, AT Attachment (INCITS T13).

https://en.wikipedia.org/wiki/Serial_ATA

- **Blu-ray Disk (2000-2003) - Sony**

The Blu-ray Disc (BD), often known simply as Blu-ray, is a digital optical disc storage format. It is designed to supersede the DVD format, and capable of storing several hours of high-definition video (HDTV 720p and 1080p). The main application of Blu-ray is as a medium for video material such as feature films and for the physical distribution of video games for the PlayStation 3, PlayStation 4, PlayStation 5, Xbox One, and Xbox Series X. The name "Blu-ray" refers to the blue laser (which is actually a violet laser) used to read the disc, which allows information to be stored at a greater density than is possible with the longer-wavelength red laser used for DVDs.
The BD format was developed by the Blu-ray Disc Association, a group representing makers of consumer electronics, computer hardware, and motion pictures. Sony unveiled the first Blu-ray Disc prototypes in October 2000, and the first prototype player was released in Japan in April 2003. Afterward, it continued to be developed until its official worldwide release on June 20, 2006, beginning the high-definition optical disc format war, where Blu-ray Disc competed with the HD DVD format. Toshiba, the main company supporting HD DVD, conceded in February 2008,[8] and later released its own Blu-ray Disc player in late 2009.[9] According to Media Research, high-definition software sales in the United States were slower in the first two years than DVD software sales.[10] Blu-ray faces competition from video on demand (VOD) and the continued sale of DVDs.[11] In January 2016, 44% of U.S. broadband households had a Blu-ray player.[12]

https://en.wikipedia.org/wiki/Blu-ray

- **POWER4 - First non-embedded microprocessor with two cores (2001) - IBM**

The POWER4 is a microprocessor developed by International Business Machines (IBM) that implemented the 64-bit PowerPC and PowerPC AS instruction set architectures. Released in 2001, the POWER4 succeeded the POWER3 and RS64 microprocessors, enabling RS/6000 and eServer iSeries models of AS/400 computer servers to run on the same processor, as a step toward converging the two lines. The POWER4 was a multicore microprocessor, with two cores on a single die, the first non-embedded microprocessor to do so.[1] POWER4 Chip was first commercially available multiprocessor chip.[2] The original POWER4 had a clock speed of 1.1 and 1.3 GHz, while an enhanced version, the POWER4+, reached a clock speed of 1.9 GHz. The PowerPC 970 is a derivative of the POWER4.

https://en.wikipedia.org/wiki/POWER4

- **HyperTransport (HT) - interconnection of computer processors (2001) - AMD**

HyperTransport (HT), formerly known as Lightning Data Transport (LDT), is a technology for interconnection of computer processors. It is a bidirectional serial/parallel high-bandwidth, low-latency point-to-point link that was introduced on April 2, 2001.[1] The HyperTransport Consortium is in charge of promoting and developing HyperTransport technology.

HyperTransport is best known as the system bus architecture of AMD central processing units (CPUs) from Athlon 64 through AMD FX and the associated motherboard chipsets. HyperTransport has also been used by IBM and Apple for the Power Mac G5 machines, as well as a number of modern MIPS systems.

The current specification HTX 3.1 remained competitive for 2014 high-speed (2666 and 3200 MT/s or about 10.4 GB/s and 12.8 GB/s) DDR4 RAM and slower (around 1 GB/s [1] similar to high end PCIe SSDs ULLtraDIMM flash RAM) technology[clarification needed]—a wider range of RAM speeds on a common CPU bus than any Intel front-side bus. Intel technologies require each speed range of RAM to have its own interface, resulting in a more complex motherboard layout but with fewer bottlenecks. HTX 3.1 at 26 GB/s can serve as a unified bus for as many as four DDR4 sticks running at the fastest proposed speeds. Beyond that DDR4 RAM may require two or more HTX 3.1 buses diminishing its value as unified transport.

https://en.wikipedia.org/wiki/HyperTransport

- **PCI Express 1.0a (2003) - PCI-SIG**

Transfer rate is expressed in transfers per second instead of bits per second because the number of transfers includes the overhead bits, which do not provide additional throughput;[48] PCIe 1.x uses an 8b/10b encoding scheme, resulting in a 20% (= 2/10) overhead on the raw channel bandwidth.[49] So in the PCIe terminology, transfer rate refers to the encoded bit rate: 2.5 GT/s is 2.5 Gbps on the encoded serial link. This corresponds to 2.0 Gbps of pre-coded data or 250 MB/s, which is referred to as throughput in PCIe.
While in early development, PCIe was initially referred to as HSI (for High Speed Interconnect), and underwent a name change to 3GIO (for 3rd Generation I/O) before finally settling on its PCI-SIG name PCI Express. A technical working group named the Arapaho Work Group (AWG) drew up the standard. For initial drafts, the AWG consisted only of Intel engineers; subsequently, the AWG expanded to include industry partners.
In 2003, PCI-SIG introduced PCIe 1.0a, with a per-lane data rate of 250 MB/s and a transfer rate of 2.5 gigatransfers per second (GT/s).
Since, PCIe has undergone several large and smaller revisions, improving on performance and other features.
Transfer rate is expressed in transfers per second instead of bits per second because the number of transfers includes the overhead bits, which do not provide additional throughput;[48] PCIe 1.x uses an 8b/10b encoding scheme, resulting in a 20% (= 2/10) overhead on the raw channel bandwidth.[49] So in the PCIe terminology, transfer rate refers to the encoded bit rate: 2.5 GT/s is 2.5 Gbps on the encoded serial link. This corresponds to 2.0 Gbps of pre-coded data or 250 MB/s, which is referred to as throughput in PCIe.

https://en.wikipedia.org/wiki/PCI_Express

- **Non-volatile RAM prototypes (2003-2004)**

Several new types of non-volatile RAM, which preserve data while powered down, are under development. The technologies used include carbon nanotubes and approaches utilizing Tunnel magnetoresistance. Amongst the 1st generation MRAM, a 128 kbit (128 × 210 bytes) chip was manufactured with 0.18 µm technology in the summer of 2003.[citation needed] 

In June 2004, Infineon Technologies unveiled a 16 MB (16 × 220 bytes) prototype again based on 0.18 µm technology. There are two 2nd generation techniques currently in development: thermal-assisted switching (TAS)[28] which is being developed by Crocus Technology, and spin-transfer torque (STT) on which Crocus, Hynix, IBM, and several other companies are working.[29] Nantero built a functioning carbon nanotube memory prototype 10 GB (10 × 230 bytes) array in 2004. Whether some of these technologies can eventually take significant market share from either DRAM, SRAM, or flash-memory technology, however, remains to be seen.

https://en.wikipedia.org/wiki/Random-access_memory#History

- **SAS-1 3GB/s - Serial Attached SCSI (2004)**

In computing, Serial Attached SCSI (SAS) is a point-to-point serial protocol that moves data to and from computer-storage devices such as hard disk drives and tape drives. SAS replaces the older Parallel SCSI (Parallel Small Computer System Interface, usually pronounced "scuzzy" or "sexy"[3][4]) bus technology that first appeared in the mid-1980s. SAS, like its predecessor, uses the standard SCSI command set. SAS offers optional compatibility with Serial ATA (SATA), versions 2 and later. This allows the connection of SATA drives to most SAS backplanes or controllers. The reverse, connecting SAS drives to SATA backplanes, is not possible.[5]

The T10 technical committee of the International Committee for Information Technology Standards (INCITS) develops and maintains the SAS protocol; the SCSI Trade Association (SCSITA) promotes the technology.
SAS-1: 3.0 Gbit/s, introduced in 2004[7]

https://en.wikipedia.org/wiki/Serial_Attached_SCSI

- **DMI - Link between northbridge and southbridge on computer motherboard (2004) - Intel**

In computing, Direct Media Interface (DMI) is Intel's proprietary link between the northbridge and southbridge on a computer motherboard. It was first used between the 9xx chipsets and the ICH6, released in 2004.

https://en.wikipedia.org/wiki/Direct_Media_Interface

- **Unified Extensible Firmware Interface (UEFI), BIOS replacement (2005) - Intel**
The Unified Extensible Firmware Interface (UEFI)[1] is a publicly available specification that defines a software interface between an operating system and platform firmware. UEFI replaces the legacy Basic Input/Output System (BIOS) boot firmware originally present in all IBM PC-compatible personal computers,[2][3] with most UEFI firmware implementations providing support for legacy BIOS services. UEFI can support remote diagnostics and repair of computers, even with no operating system installed.[4]

Intel developed the original Extensible Firmware Interface (EFI) specifications. Some of the EFI's practices and data formats mirror those of Microsoft Windows.[5][6] In 2005, UEFI deprecated EFI 1.10 (the final release of EFI). The Unified EFI Forum is the industry body that manages the UEFI specifications throughout.

https://en.wikipedia.org/wiki/UEFI

- **Dual core processors (2005) - Intel / AMD**

The first dual core chips for x86-based PCs and servers were introduced in 2005 and included the Pentium D and Pentium Processor Extreme Edition 840 from Intel and the Opteron 800 and Athlon 64 X2 from AMD. A year later, Intel added dual cores to its Itanium line.

https://www.pcmag.com/encyclopedia/term/dual-core

The Athlon 64 X2 is the first native dual-core desktop central processing unit (CPU) designed by Advanced Micro Devices (AMD). It was designed from scratch as native dual-core by using an already multi-CPU enabled Athlon 64, joining it with another functional core on one die, and connecting both via a shared dual-channel memory controller/north bridge and additional control logic. The initial versions are based on the E stepping model of the Athlon 64 and, depending on the model, have either 512 or 1024 KB of L2 cache per core. The Athlon 64 X2 can decode instructions for Streaming SIMD Extensions 3 (SSE3), except those few specific to Intel's architecture. The first Athlon 64 X2 CPUs were released in May 2005, in the same month as Intel's first dual-core processor, the Pentium D.

https://en.wikipedia.org/wiki/Athlon_64_X2

The Pentium Dual-Core brand was used for mainstream x86-architecture microprocessors from Intel from 2006 to 2009 when it was renamed to Pentium. The processors are based on either the 32-bit Yonah or (with quite different microarchitectures) 64-bit Merom-2M, Allendale, and Wolfdale-3M core, targeted at mobile or desktop computers.

https://en.wikipedia.org/wiki/Pentium_Dual-Core

- **Intel virtualization (VT-x) (2005) - Intel**

Previously codenamed "Vanderpool", VT-x represents Intel's technology for virtualization on the x86 platform. On November 13, 2005, Intel released two models of Pentium 4 (Model 662 and 672) as the first Intel processors to support VT-x. The CPU flag for VT-x capability is "vmx"; in Linux, this can be checked via /proc/cpuinfo, or in macOS via sysctl machdep.cpu.features.[19]

https://en.wikipedia.org/wiki/X86_virtualization#Intel_virtualization_(VT-x)

- **AMD virtualization (AMD-V) (2006) - AMD**

AMD developed its first generation virtualization extensions under the code name "Pacifica", and initially published them as AMD Secure Virtual Machine (SVM),[16] but later marketed them under the trademark AMD Virtualization, abbreviated AMD-V.

On May 23, 2006, AMD released the Athlon 64 ("Orleans"), the Athlon 64 X2 ("Windsor") and the Athlon 64 FX ("Windsor") as the first AMD processors to support this technology.

https://en.wikipedia.org/wiki/X86_virtualization

- **Intel Core 2 Extreme - Quad core processor (2006) - Intel**

Intel Core 2 is the processor family encompassing a range of Intel's consumer 64-bit x86-64 single-, dual-, and quad-core microprocessors based on the Core microarchitecture. The single- and dual-core models are single-die, whereas the quad-core models comprise two dies, each containing two cores, packaged in a multi-chip module.[2] The Core 2 range was the last flagship range of Intel desktop processors to use a front-side bus.

The introduction of Core 2 relegated the Pentium brand to the mid-range market, and reunified laptop and desktop CPU lines for marketing purposes under the same product name, which were formerly divided into the Pentium 4, Pentium D, and Pentium M brands.

The Core 2 processor line was introduced on July 27, 2006,[3] comprising the Duo (dual-core) and Extreme (dual- or quad-core CPUs for enthusiasts), and in 2007, the Quad (quad-core) and Solo (single-core) sub-brands.[4] Intel Core 2 processors with vPro technology (designed for businesses) include the dual-core and quad-core branches.[5]

Core 2 and other LGA 775 processors can support virtualization if the virtual machine (VM) software supports those processors, e.g. if the processor supports VT-x.

https://en.wikipedia.org/wiki/Intel_Core_2

- **NVMHCI 1.0 - NVMe (2007-2009) Intel**

The first details of a new standard for accessing non-volatile memory emerged at the Intel Developer Forum 2007, when NVMHCI was shown as the host-side protocol of a proposed architectural design that had Open NAND Flash Interface Working Group (ONFI) on the memory (flash) chips side.[15] A NVMHCI working group led by Intel was formed that year. The NVMHCI 1.0 specification was completed in April 2008 and released on Intel's web site.[16][17][18]

Technical work on NVMe began in the second half of 2009.[19] The NVMe specifications were developed by the NVM Express Workgroup, which consists of more than 90 companies; Amber Huffman of Intel was the working group's chair. Version 1.0 of the specification was released on 1 March 2011,[20] while version 1.1 of the specification was released on 11 October 2012.[21] Major features added in version 1.1 are multi-path I/O (with namespace sharing) and arbitrary-length scatter-gather I/O. It is expected that future revisions will significantly enhance namespace management.[19] Because of its feature focus, NVMe 1.1 was initially called "Enterprise NVMHCI".[22] An update for the base NVMe specification, called version 1.0e, was released in January 2013.[23] In June 2011, a Promoter Group led by seven companies was formed.

https://en.wikipedia.org/wiki/NVM_Express

- **Southbridge disappears (2008+) - Intel/AMD**

The southbridge is one of the two chips in the core logic chipset on a personal computer (PC) motherboard, the other being the northbridge. The southbridge typically implements the slower capabilities of the motherboard in a northbridge/southbridge chipset computer architecture. In systems with Intel chipsets, the southbridge is named I/O Controller Hub (ICH), while AMD has named its southbridge Fusion Controller Hub (FCH) since the introduction of its Fusion AMD Accelerated Processing Unit (APU) while moving the functions of the Northbridge onto the CPU die, hence making it similar in function to the Platform hub controller.
The southbridge can usually be distinguished from the northbridge by not being directly connected to the CPU. Rather, the northbridge ties the southbridge to the CPU. Through the use of controller integrated channel circuitry, the northbridge can directly link signals from the I/O units to the CPU for data control and access.
Due to the push for system-on-chip (SoC) processors, modern devices increasingly have the northbridge integrated into the CPU die itself;[further explanation needed] examples are Intel's Sandy Bridge[1] and AMD's Fusion processors,[2] both released in 2011. The southbridge became redundant and it was replaced by the Platform Controller Hub (PCH) architecture introduced with the Intel 5 Series chipset in 2008 while AMD did the same with the release of their first APUs in 2011, naming the PCH the Fusion controller hub (FCH), which was only used on AMD's APUs until 2017 when it began to be used on AMD's Zen architecture while dropping the FCH name. On Intel platforms, all southbridge features and remaining I/O functions are managed by the PCH which is directly connected to the CPU via the Direct Media Interface (DMI).[3] Intel low power processor (Haswell-U and onward) and Ultra low power processor (Haswell-Y and onward) also integrated an on-package PCH. Based on its Chiplet design, AMD Ryzen processors also integrated some southbridge functions, such as some USB interface and some SATA/NVMe interface.[4]

https://en.wikipedia.org/wiki/Southbridge_(computing)

- **USB 3.x - SuperSpeed USB logo (2008)**

The USB 3.0 specification was released on 12 November 2008, with its management transferring from USB 3.0 Promoter Group to the USB Implementers Forum (USB-IF), and announced on 17 November 2008 at the SuperSpeed USB Developers Conference.[23]
USB 3.0 adds a SuperSpeed transfer mode, with associated backward compatible plugs, receptacles, and cables. SuperSpeed plugs and receptacles are identified with a distinct logo and blue inserts in standard format receptacles.

https://en.wikipedia.org/wiki/USB

- **Intel QuickPath Interconnect - Replaced Front-side bus (2008) - Intel**

The Intel QuickPath Interconnect (QPI)[1][2] is a point-to-point processor interconnect developed by Intel which replaced the front-side bus (FSB) in Xeon, Itanium, and certain desktop platforms starting in 2008. It increased the scalability and available bandwidth. Prior to the name's announcement, Intel referred to it as Common System Interface (CSI).[3] Earlier incarnations were known as Yet Another Protocol (YAP) and YAP+.
QPI 1.1 is a significantly revamped version introduced with Sandy Bridge-EP (Romley platform).[4]
QPI was replaced by Intel Ultra Path Interconnect (UPI) in Skylake-SP Xeon processors based on LGA 3647 socket.[5]

https://en.wikipedia.org/wiki/Intel_QuickPath_Interconnect

- **Demonstration of 1TB SSD on PCI Express x8 (2009) - OCZ Technology**

At Cebit 2009, OCZ Technology demonstrated a 1 TB[47] flash SSD using a PCI Express ×8 interface. It achieved a maximum write speed of 0.654 gigabytes per second (GB/s) and maximum read speed of 0.712 GB/s.[48] In December 2009, Micron Technology announced an SSD using a 6 gigabits per second (Gbit/s) SATA interface.[49]

https://en.wikipedia.org/wiki/Solid-state_drive

- **Widespread ink color printers (2000s)**

Inkjet printers were the most commonly used type of printer in 2008,[2] and range from small inexpensive consumer models to expensive professional machines

https://en.wikipedia.org/wiki/Inkjet_printing

## Evolution of uses
**[`^        back to top        ^`](#)**

### Servers
- **Nginx - Most popular web server, superseded Apache (2004) - Nginx**

Nginx (pronounced "engine x"[9] /ˌɛndʒɪnˈɛks/ EN-jin-EKS) is a web server that can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache. The software was created by Igor Sysoev and publicly released in 2004.[10

As of June 2022 W3Tech's web server count of all web sites ranked Nginx first with 33.6%. Apache was second at 31.4% and Cloudflare Server third at 21.6%

https://en.wikipedia.org/wiki/Nginx

### Clients
#### Gui

- **WindowsXP (2001) - Microsoft**

Windows XP is a major release of Microsoft's Windows NT operating system. It was released to manufacturing on August 24, 2001, and later to retail on October 25, 2001. It is a direct upgrade to its predecessors, Windows 2000 for high-end and business users and Windows Me for home users, available for any devices running Windows NT 4.0, Windows 98, Windows 2000 and Windows Me that meet the new Windows XP system requirements.

Development of Windows XP began in the late 1990s under the codename "Neptune", built on the Windows NT kernel explicitly intended for mainstream consumer use. An updated version of Windows 2000 was also initially planned for the business market. However, in January 2000, both projects were scrapped in favor of a single OS codenamed "Whistler", which would serve as a single platform for both consumer and business markets. As a result, Windows XP is the first consumer edition of Windows not based on the Windows 95 kernel and MS-DOS. Windows XP removed support for PC-98, i486 and SGI Visual Workstation 320 and 540 and will only run on 32-bit x86 CPUs and devices which use BIOS firmware.

Upon its release, Windows XP received critical acclaim, noting increased performance and stability (especially compared to Windows Me), a more intuitive user interface, improved hardware support, and expanded multimedia capabilities. Windows XP and Windows Server 2003 were succeeded by Windows Vista and Windows Server 2008, released in 2007 and 2008, respectively. Market share of Windows XP fell below 1% by the end of 2021.[9]

https://en.wikipedia.org/wiki/Windows_XP

- **WindowsXP SP2 - Bluetooth mice support (2004) - Microsoft**

Windows XP Service Pack 2 introduced a Bluetooth stack, allowing Bluetooth mice to be used without any USB receivers.[108] 
Windows Vista added native support for horizontal scrolling and standardized wheel movement granularity for finer scrolling.[106]

https://en.wikipedia.org/wiki/Computer_mouse

- **Windows Vista (2006) - Microsoft**

Windows Vista is a major release of the Windows NT operating system developed by Microsoft. It was the direct successor to Windows XP, which was released five years before, at the time being the longest time span between successive releases of Microsoft Windows desktop operating systems. Development was completed on November 8, 2006, and over the following three months, it was released in stages to computer hardware and software manufacturers, business customers and retail channels. On January 30, 2007, it was released internationally and was made available for purchase and download from the Windows Marketplace; it is the first release of Windows to be made available through a digital distribution platform.[7]

New features of Windows Vista include an updated graphical user interface and visual style dubbed Aero, a new search component called Windows Search, redesigned networking, audio, print and display sub-systems, and new multimedia tools such as Windows DVD Maker. Vista aimed to increase the level of communication between machines on a home network, using peer-to-peer technology to simplify sharing files and media between computers and devices. Windows Vista included version 3.0 of the .NET Framework, allowing software developers to write applications without traditional Windows APIs. Windows Vista removed support for devices without ACPI.

https://en.wikipedia.org/wiki/Windows_Vista

- **Windows 7 (2009) - Microsoft**

Windows 7 is a major release of the Windows NT operating system developed by Microsoft. It was released to manufacturing on July 22, 2009, and became generally available on October 22, 2009.[9] It is the successor to Windows Vista, released nearly three years earlier. It remained an operating system for use on personal computers, including home and business desktops, laptops, tablet PCs and media center PCs, and itself was replaced in November 2012 by Windows 8, the name spanning more than three years of the product.

Windows 7's server counterpart, Windows Server 2008 R2, was released at the same time. The last supported version of Windows based on this operating system was released on July 1, 2011, entitled Windows Embedded POSReady 7.[10][11][12]

Windows 7 was intended to be an incremental upgrade to Microsoft Windows, addressing Windows Vista's poor critical reception while maintaining hardware and software compatibility. Windows 7 continued improvements on Windows Aero user interface with the addition of a redesigned taskbar that allows pinned applications, and new window management features. Other new features were added to the operating system, including libraries, the new file-sharing system HomeGroup, and support for multitouch input. A new "Action Center" was also added to provide an overview of system security and maintenance information, and tweaks were made to the User Account Control system to make it less intrusive. Windows 7 also shipped with updated versions of several stock applications, including Internet Explorer 8, Windows Media Player, and Windows Media Center.

Unlike Vista, Windows 7 received critical acclaim, with critics considering the operating system to be a major improvement over its predecessor because of its improved performance, its more intuitive interface, fewer User Account Control popups, and other improvements made across the platform. Windows 7 was a major success for Microsoft; 

Windows 7 is the final version of Windows which supports processors without SSE2 or NX (although an update released in 2018 dropped support for non-SSE2 processors). Its successor, Windows 8, requires a processor with SSE2 and NX in any supported architecture.

https://en.wikipedia.org/wiki/Windows_7

#### Web browsers

- **Mozilla Firefox (2002)**

Mozilla Firefox, or simply Firefox, is a free and open-source[20] web browser developed by the Mozilla Foundation and its subsidiary, the Mozilla Corporation. It uses the Gecko rendering engine to display web pages, which implements current and anticipated web standards.[21] In 2017, Firefox began incorporating new technology under the code name Quantum to promote parallelism and a more intuitive user interface.[22] Firefox is available for Windows 7 and later versions, macOS, and Linux. Its unofficial ports are available for various Unix and Unix-like operating systems, including FreeBSD,[9] OpenBSD,[10] NetBSD,[11] illumos,[12] and Solaris Unix.[14] It is also available for Android and iOS. However, as with all other iOS web browsers, the iOS version uses the WebKit layout engine instead of Gecko due to platform requirements. An optimized version is also available on the Amazon Fire TV, as one of the two main browsers available with Amazon's Silk Browser.[23]

Firefox was created in 2002 under the code name "Phoenix" by the Mozilla community members who desired a standalone browser, rather than the Mozilla Application Suite bundle. During its beta phase, it proved to be popular with its testers and was praised for its speed, security, and add-ons compared to Microsoft's then-dominant Internet Explorer 6. It was released on November 9, 2004,[24] and challenged Internet Explorer's dominance with 60 million downloads within nine months.[25] It is the spiritual successor of Netscape Navigator, as the Mozilla community was created by Netscape in 1998 before their acquisition by AOL.[26]

Firefox usage share grew to a peak of 32.21% in November 2009,[27] with Firefox 3.5 overtaking Internet Explorer 7, although not all versions of Internet Explorer as a whole;[28][29] its usage then declined in competition with Google Chrome.[27] 

https://en.wikipedia.org/wiki/Firefox


## Consumer electronics
**[`^        back to top        ^`](#)**

### Hardware

- **OLED display prototype and manufacturing (early 2000s)**

In 1999, Kodak and Sanyo had entered into a partnership to jointly research, develop, and produce OLED displays. They announced the world's first 2.4-inch active-matrix, full-color OLED display in September the same year.[34] In September 2002, they presented a prototype of 15-inch HDTV format display based on white OLEDs with color filters at the CEATEC Japan.[35]

Manufacturing of small molecule OLEDs was started in 1997 by Pioneer Corporation, followed by TDK in 2001 and Samsung-NEC Mobile Display (SNMD), which later became one of the world's largest OLED display manufacturers - Samsung Display, in 2002.[36]

**Universal Display Corporation**, one of the OLED materials companies, holds a number of patents concerning the commercialization of OLEDs that are used by major OLED manufacturers around the world.[38][39]

https://en.wikipedia.org/wiki/OLED

Back in 2009 UDC claimed that "virtually all AMOLEDs on the market use our technology".

https://en.wikipedia.org/wiki/Universal_Display_Corporation

- **Quantum dots for light sources and displays (2000s)**

Starting in the early 2000s, scientists started to realize the potential of developing quantum dots for light sources and displays.[13]

QDs are either photo-emissive (photoluminescent) or electro-emissive (electroluminescent) allowing them to be readily incorporated into new emissive display architectures.[14] Quantum dots naturally produce monochromatic light, so they are more efficient than white light sources when color filtered and allow more saturated colors that reach nearly 100% of Rec. 2020 color gamut.[10][11][9]

https://en.wikipedia.org/wiki/Quantum_dot

- **Inorganic semiconductor microLED invention**

Inorganic semiconductor microLED (µLED) technology[1][23][24][25] was first invented in 2000 by the research group of Hongxing Jiang and Jingyu Lin of Texas Tech University while they were at Kansas State University. Following their first report of electrical injection microLEDs based on indium gallium nitride (InGaN) semiconductors, several groups have quickly engaged in pursuing this concept.[26][27] Many related potential applications have been identified. Various on-chip connection schemes of microLED pixel arrays have been employed by AC LED Lighting, LLC (a company funded by Hongxing Jiang and Jingyu Lin) allowing for the development of single-chip high voltage DC/AC-LEDs[28][29][30][31][32][33][34] to address the compatibility issue between the high voltage electrical infrastructure and low voltage operation nature of LEDs and high brightness self-emissive microdisplays.[35][2]

The microLED array has also been explored as a light source for optogenetics applications[36][37] and for visible light communications.[38]

Early InGaN based microLED arrays and microdisplays were primarily passively driven. The first actively driven video-capable self-emissive InGaN microLED microdisplay in VGA format (640 × 480 pixels, each 12 µm in size with 15 µm between them) possessing low voltage requirements was patented and realized in 2009 by Hongxing Jiang and Jingyu Lin and their colleagues at III-N Technology, Inc. (a company funded by Hongxing Jiang and Jingyu Lin) and Texas Tech University, via heterogeneous integration between microLED array and CMOS integrated circuit (IC)[2] and the work was also published in the following years.[39][40][41][42]

https://en.wikipedia.org/wiki/MicroLED

### TV

- **Screen Transition from 800x600 to eXtended Graphics Array (XGA) 1024x768 (early 2000s)**

In 2002, 1024 × 768 eXtended Graphics Array was the most common display resolution. Many web sites and multimedia products were re-designed from the previous 800 × 600 format to the layouts optimized for 1024 × 768.
The availability of inexpensive LCD monitors made the 5∶4 aspect ratio resolution of 1280 × 1024 more popular for desktop usage during the first decade of the 21st century. 

https://en.wikipedia.org/wiki/Display_resolution

- **LCD displays dominance (2000s)**

In 2007 the image quality of LCD televisions surpassed the image quality of cathode-ray-tube-based (CRT) TVs.[86] In the fourth quarter of 2007, LCD televisions surpassed CRT TVs in worldwide sales for the first time.[87] LCD TVs were projected to account 50% of the 200 million TVs to be shipped globally in 2006, according to Displaybank.[88][89] In October 2011, Toshiba announced 2560 × 1600 pixels on a 6.1-inch (155 mm) LCD panel, suitable for use in a tablet computer,[90] especially for Chinese character display. The 2010s also saw the wide adoption of TGP (Tracking Gate-line in Pixel), which moves the driving circuitry from the borders of the display to in between the pixels, allowing for narrow bezels.[91] LCDs can be made transparent and flexible, but they cannot emit light without a backlight like OLED and microLED, which are other technologies that can also be made flexible and transparent.[92][93][94][95] Special films can be used to increase the viewing angles of LCDs.[96][97]

https://en.wikipedia.org/wiki/Liquid-crystal_display

- **Sony Qualia 005 - LED-backlit LCD ("LED TVs") (2004) - Sony **

The prismatic and reflective polarization films are generally achieved using so called DBEF films manufactured and supplied by 3M.[23][24] These reflective polarization films using uniaxial oriented polymerized liquid crystals (birefringent polymers or birefringent glue) were invented in 1989 by Philips researchers Dirk Broer, Adrianus de Vaan and Joerg Brambring.[25]

A first dynamic "local dimming" LED backlight was public demonstrated by BrightSide Technologies in 2003,[26] and later commercially introduced for professional markets (such as video post-production).[27] Edge LED lighting was first introduced by Sony in September 2008 on the 40-inch (1,000 mm) BRAVIA KLV-40ZX1M (known as the ZX1 in Europe). Edge-LED lighting for LCDs allows thinner housing; the Sony BRAVIA KLV-40ZX1M is 1 cm thick, and others are also extremely thin.

LED-backlit LCDs have longer life and better energy efficiency than plasma and CCFL LCD TVs.[28] Unlike CCFL backlights, LEDs use no mercury (an environmental pollutant) in their manufacture. However, other elements (such as gallium and arsenic) are used in the manufacture of the LED emitters; there is debate over whether they are a better long-term solution to the problem of screen disposal.

Because LEDs can be switched on and off more quickly than CCFLs and can offer a higher light output, it is theoretically possible to offer very high contrast ratios. They can produce deep blacks (LEDs off) and high brightness (LEDs on). However, measurements made from pure-black and pure-white outputs are complicated by the fact that edge-LED lighting does not allow these outputs to be reproduced simultaneously on screen.[clarification needed]

LED-backlit LCDs are not self-illuminating (unlike pure-LED systems). There are several methods of backlighting an LCD panel using LEDs, including the use of either white or RGB (Red, Green, and Blue) LED arrays behind the panel and edge-LED lighting (which uses white LEDs around the inside frame of the TV and a light-diffusion panel to spread the light evenly behind the LCD panel). Variations in LED backlighting offer different benefits. The first commercial full-array LED-backlit LCD TV was the Sony Qualia 005 (introduced in 2004), which used RGB LED arrays to produce a color gamut about twice that of a conventional CCFL LCD television. This was possible because red, green and blue LEDs have sharp spectral peaks which (combined with the LCD panel filters) result in significantly less bleed-through to adjacent color channels. Unwanted bleed-through channels do not "whiten" the desired color as much, resulting in a larger gamut. RGB LED technology continues to be used on Sony BRAVIA LCD models. LED backlighting using white LEDs produces a broader spectrum source feeding the individual LCD panel filters (similar to CCFL sources), resulting in a more limited display gamut than RGB LEDs at lower cost.

The commercially called "LED TVs" are LCDs-based television sets where the LEDs are dynamically controlled using the video information[10] (dynamic backlight control or dynamic "local dimming" LED backlight, also marketed as HDR, high dynamic range television, invented by Philips researchers Douglas Stanton, Martinus Stroomer and Adrianus de Vaan[11][12][13]).

https://en.wikipedia.org/wiki/LED-backlit_LCD

- **CRT peak and demise (2000s-2015)**

Beginning in the late 90s to the early 2000s, CRTs began to be replaced with LCDs, starting first with computer monitors smaller than 15 inches in size,[90] largely because of their lower bulk.[91] Among the first[92] manufacturers to stop CRT production was Hitachi in 2001,[93][94] followed by Sony in Japan in 2004,[95] Thomson in the US in 2004,[96][97] Matsushita Toshiba picture display in 2005 in the US,[98] 2006 in Malaysia[99] and 2007 in China,[100] Sony in the US in 2006,[101] Sony in Singapore and Malaysia for the Latin American and Asian markets in 2008,[95][102] Samsung SDI in 2007[103][104] and 2012[105][106] and Cathode Ray Technology (formerly Philips) in 2012[107][108] and Videocon in 2015–16.[109][110][111][84] Ekranas in Lithuania[112] and LG.Philips Displays[113] went bankrupt in 2005 and 2006, respectively. Matsushita Toshiba stopped in the US in 2004 due to losses of $109 million,[114] and in Malaysia in 2006 due to losses that almost equaled their sales.[99] The last CRT TVs at CES were shown by Samsung in 2007[115] and the last mass-produced model was introduced by LG in 2008 for developing markets due to its low price.[116][117][better source needed] The last CRT TV by a major manufacturer was introduced by LG in 2010.[118][119]

CRTs were first replaced by LCD in first world countries such as Japan and Europe in the 2000s and continued to be popular in third world countries such as Latin America,[120][89] China, Asia and the Middle East due to their low price compared to contemporary flat panel TVs,[121] and later in markets like rural India. However, in around 2014, even rural markets started favoring LCD over CRT, leading to the demise of the technology.[122]

Most high-end CRT production had ceased by around 2010,[129] including high-end Sony and Panasonic product lines.[130][131] In Canada and the United States, the sale and production of high-end CRT TVs (30-inch (76 cm) screens) in these markets had all but ended by 2007. Just a couple of years later, inexpensive "combo" CRT TVs (20-inch (51 cm) screens with an integrated VHS player) disappeared from discount stores.

https://en.wikipedia.org/wiki/Cathode-ray_tube

- **Sony CLIÉ PEG-VZ90 - fist PDA with OLED screen (2004) - Sony**

The Sony CLIÉ PEG-VZ90 was released in 2004, being the first PDA to feature an OLED screen

https://en.wikipedia.org/wiki/OLED

- **Commercial TV projectors (2004) - Sony**

Sony was offering 4K projectors as early as 2004

https://en.wikipedia.org/wiki/4K_resolution

- **HDTV in Europe (2004) - Belgium**

The first regular broadcasts started on January 1, 2004, when the Belgian company Euro1080 launched the HD1 channel with the traditional Vienna New Year's Concert. Test transmissions had been active since the IBC exhibition in September 2003, but the New Year's Day broadcast marked the official launch of the HD1 channel, and the official start of direct-to-home HDTV in Europe.[44]

https://en.wikipedia.org/wiki/High-definition_television

- **16:10 widescreens on latops (2004)**

The popular widescreen format (16:10) appeared in 2004 originally in laptops (14″ / 15″ had 1280×800, 17″ had 1440×900), next year desktop monitors appeared in this format (19″ 1440×900, 22″ 1680×1050, 24″ 1920×1200) and high-end laptops received such resolutions. Widescreen monitors come with VGA and DVI ports, running 4:3 resolutions on VGA cable will make image stretching on full screen, while DVI cable maintain aspect ratio and display black bars.

https://www.teoalida.com/webdesign/screen-resolution/

- **Sony XEL-1 - First OLED TV (2007)**

The Sony XEL-1, released in 2007, was the first OLED television.

https://en.wikipedia.org/wiki/OLED

- **16:9 widescreen TV standards (2008)**

Television industry adopted 16:9 standard in 2008, and because is cheaper to produce both monitors and TV with same aspect ratio, 14″ / 15″ laptops became 1366×768, 17″ laptops became 1600×900, high-end laptops ($1000+) used 1920×1080. New desktop monitors were 18.5″ 1366×768, 20″ 1600×900, 21.5″ / 23″ 1920×1080. QHD, 27″ 2560×1440 appeared in 2009. LG G3 was in 2013 first smartphone with QHD display. Mini-laptops with 10″ screens used 1024×600, called netbooks (smaller than 14-17″ notebooks).

https://www.teoalida.com/webdesign/screen-resolution/

- **Transition from XGA to 1366x768 screens (late 2000s)**

XGA 1024×768 had over 50% marketshare before 2007 and was overtaken by 1366×768 in March 2012 which reached a peak of 27%+ in 2015. Next popular were 1280×800 with a peak of 19.72% in Nov 2009 and 1280×1024 with peak in 2007

https://www.teoalida.com/webdesign/screen-resolution/

- **Plasma peak - largest plasma video display - 150-inch (2008)**

In the year 2000, the first 60-inch plasma display was developed by Plasmaco. Panasonic was also reported to have developed a process to make plasma displays using ordinary window glass instead of the much more expensive "high strain point" glass.[64] High strain point glass is made similarly to conventional float glass, but it is more heat resistant, deforming at higher temperatures. High strain point glass is normally necessary because plasma displays have to be baked during manufacture to dry the rare-earth phosphors after they are applied to the display. However, high strain point glass may be less scratch resistant.[65][66][67][68]


Plasma displays became 75% thinner between 2006 and 2011
In late 2006, analysts noted that LCDs had overtaken plasmas, particularly in the 40-inch (100 cm) and above segment where plasma had previously gained market share.[69] Another industry trend was the consolidation of plasma display manufacturers, with around 50 brands available but only five manufacturers. In the first quarter of 2008, a comparison of worldwide TV sales broke down to 22.1 million for direct-view CRT, 21.1 million for LCD, 2.8 million for plasma, and 0.1 million for rear projection.[70]

Until the early 2000s, plasma displays were the most popular choice for HDTV flat panel display as they had many benefits over LCDs. Beyond plasma's deeper blacks, increased contrast, faster response time, greater color spectrum, and wider viewing angle; they were also much bigger than LCDs, and it was believed that LCDs were suited only to smaller sized televisions. However, improvements in VLSI fabrication narrowed the technological gap. The increased size, lower weight, falling prices, and often lower electrical power consumption of LCDs made them competitive with plasma television sets.

Screen sizes have increased since the introduction of plasma displays. The largest plasma video display in the world at the 2008 Consumer Electronics Show in Las Vegas, Nevada, was a 150-inch (380 cm) unit manufactured by Matsushita Electric Industrial (Panasonic) standing 6 ft (180 cm) tall by 11 ft (330 cm) wide.[71][72]


### Distribution media

- **Declines of CDs**

Beginning in the 2000s, CDs were increasingly being replaced by other forms of digital storage and distribution, with the result that by 2010 the number of audio CDs being sold in the U.S. had dropped about 50% from their peak; however, they remained one of the primary distribution methods for the music industry.

With the advent and popularity of Internet-based distribution of files in lossy-compressed audio formats such as MP3, sales of CDs began to decline in the 2000s. For example, between 2000 and 2008, despite overall growth in music sales and one anomalous year of increase, major-label CD sales declined overall by 20%,[48] although independent and DIY music sales may be tracking better according to figures released 30 March 2009, and CDs still continue to sell greatl

https://en.wikipedia.org/wiki/Compact_Disc_Digital_Audio

### Video Games

- **128 bits console - Sixth generation video games consoles : Dreamcast, PS2, GameCube, Xbox (1998-2005)**

In the history of video games, the sixth-generation era (sometimes called the 128-bit era; see "bits and system power" below) is the era of computer and video games, video game consoles, and handheld gaming devices available at the turn of the 21st century, starting on November 27, 1998. Platforms in the sixth generation include consoles from four companies: the **Sega Dreamcast (DC)**, **Sony PlayStation 2 (PS2)**, **Nintendo GameCube (GC)**, and **Microsoft Xbox**. This era began on November 27, 1998, with the Japanese release of the Dreamcast, which was joined by the **PlayStation 2 on March 4, 2000**, and the Xbox and Gamecube on November 15 and 18, 2001, respectively. In April 2001, the Dreamcast was the first to be discontinued. Xbox was next in 2006, GameCube in 2007 and PlayStation 2 was the last, in January 2013. Meanwhile, the seventh generation of consoles started on November 22, 2005 with the launch of the Xbox 360.[1]

The major innovation of this generation was of full utilization of the internet to allow a fully online gaming experience. While the prior generation had some systems with internet connectivity, such as the Apple Pippin, these had little market penetration and thus had limited success in the area. Services such as Microsoft's Xbox Live became industry standard in this, and future, generations. Another innovation of the Xbox was the first system to utilize an internal hard disk drive to store game data. This caused many improvements to the gaming experience, including the ability to store program data (rather than just save game data) that allowed for faster load times, as well as the ability to download games directly from the internet rather than to purchase physical media such as a disk or cartridge. Soon after its release other systems, like the Sony PlayStation 2, produced peripheral storage devices to allow similar capabilities, and by the next generation internal storage became industry standard.

Bit ratings (i.e. "64-bit" or "32-bit" for the previous generation) for most consoles largely fell by the wayside during this era, with the notable exceptions being promotions for the Dreamcast[2] and PS2[3] that advertised "128-bit graphics" at the start of the generation. 

The sixth generation of handhelds began with the release of the Neo Geo Pocket Color by SNK in 1998 and Bandai's WonderSwan Color, launched in Japan in 1999. Nintendo maintained its dominant share of the handheld market with the release in 2001 of the Game Boy Advance, which featured many upgrades and new features over the Game Boy. The Game Boy Advance was discontinued around in early 2010. The next generation of handheld consoles began in November 2004, with the North American introduction of the Nintendo DS.

https://en.wikipedia.org/wiki/Sixth_generation_of_video_game_consoles

- **Sony PlayStation 2 - Best selling console (2000) - Sony**

Its successor, the PlayStation 2, was released in 2000. The PlayStation 2 is the best-selling home console to date, having reached over 155 million units sold by the end of 2012.[3]

https://en.wikipedia.org/wiki/PlayStation

The PlayStation 2 (PS2) is a home video game console developed and marketed by Sony Computer Entertainment. It was first released in Japan on 4 March 2000, in North America on 26 October 2000, in Europe on 24 November 2000, and in Australia on 30 November 2000. It is the successor to the original PlayStation, as well as the second installment in the PlayStation brand of consoles. As a sixth-generation console, it competed with Sega's Dreamcast, Nintendo's GameCube, and Microsoft's Xbox.

The PlayStation 2 received widespread critical acclaim upon release, and is the best-selling video game console of all time, having sold over 155 million units worldwide. A total of 4,000 game titles were released, with over 1.5 billion copies sold.[14] In 2004, Sony released a smaller, lighter revision of the console known as the PS2 Slim. Even after the release of its successor, the PlayStation 3, it remained popular well into the seventh generation. It continued to be produced until 2013 when Sony finally announced that it had been discontinued after over twelve years of production, one of the longest lifespans of any video game console. New games for the console continued to be made until the end of its life.

Software for the PlayStation 2 was distributed primarily on DVD-ROMs,[59] with some titles being published on blue-tinted CD-ROM format. In addition, the console can play audio CDs and DVD films and is backward-compatible with almost all original PlayStation games.[56] The PlayStation 2 also supports PlayStation memory cards and controllers, although original PlayStation memory cards will only work with original PlayStation games[60] and the controllers may not support all functions (such as analogue buttons) for PlayStation 2 games.

Later reviews, especially after the launch of the competing GameCube and Xbox systems, continued to praise the PlayStation 2's large game library and DVD playback, while routinely criticizing the PlayStation 2's lesser graphics performance compared to the newer systems and its rudimentary online service compared to Xbox Live. 

https://en.wikipedia.org/wiki/PlayStation_2

- **XBox - Microsoft Game console (2001) - Microsoft** 

Xbox is a video gaming brand created and owned by Microsoft. The brand consists of five video game consoles, as well as applications (games), streaming services, an online service by the name of Xbox network, and the development arm by the name of Xbox Game Studios. The brand was first introduced in the United States in November 2001, with the launch of the original Xbox console.
The original device was the first video game console offered by an American company after the Atari Jaguar stopped sales in 1996. It reached over 24 million units sold by May 2006.[1] Microsoft's second console, the Xbox 360, was released in 2005 and has sold 86 million units as of October 2021. 

https://en.wikipedia.org/wiki/Xbox

- **XBox360 -  well-developped online service (Xbox Live) (2005) - Microsoft** 

The Xbox 360 is a home video game console developed by Microsoft. As the successor to the original Xbox, it is the second console in the Xbox series. It competed with Sony's PlayStation 3 and Nintendo's Wii as part of the seventh generation of video game consoles. It was officially unveiled on MTV on May 12, 2005, with detailed launch and game information announced later that month at the 2005 Electronic Entertainment Expo.[17][18][19][20][21]

The Xbox 360 features an online service, Xbox Live, which was expanded from its previous iteration on the original Xbox and received regular updates during the console's lifetime. Available in free and subscription-based varieties, Xbox Live allows users to: play games online; download games (through Xbox Live Arcade) and game demos; purchase and stream music, television programs, and films through the Xbox Music and Xbox Video portals; and access third-party content services through media streaming applications. In addition to online multimedia features, it allows users to stream media from local PCs. Several peripherals have been released, including wireless controllers, expanded hard drive storage, and the Kinect motion sensing camera. The release of these additional services and peripherals helped the Xbox brand grow from gaming-only to encompassing all multimedia, turning it into a hub for living-room computing entertainment.[22][23][24][25][26]
DVD, CD, digital distribution, Add-on: HD DVD

https://en.wikipedia.org/wiki/Xbox_360

- **Sony PlayStation 3 (2006) - Sony**

Sony's next console, the PlayStation 3, was released in 2006, selling over 87.4 million units by March 2017

https://en.wikipedia.org/wiki/PlayStation

The PlayStation 3 (PS3) is a home video game console developed by Sony Computer Entertainment. The successor to the PlayStation 2, it is part of the PlayStation brand of consoles. It was first released on November 11, 2006, in Japan,[9] November 17, 2006, in North America, and March 23, 2007, in Europe and Australia.[10][11][12] The PlayStation 3 competed primarily against Microsoft's Xbox 360 and Nintendo's Wii as part of the seventh generation of video game consoles.

The console was first officially announced at E3 2005, and was released at the end of 2006. It was the first console to use Blu-ray Disk technology as its primary storage medium.[13] The console was the first PlayStation to integrate social gaming services, including the PlayStation Network, as well as the first to be controllable from a handheld console, through its remote connectivity with PlayStation Portable and PlayStation Vita.[14][15][16] In September 2009, the Slim model of the PlayStation 3 was released. It no longer provided the hardware ability to run PS2 games. It was lighter and thinner than the original version, and featured a redesigned logo and marketing design, as well as a minor start-up change in software. A Super Slim variation was then released in late 2012, further refining and redesigning the console.

During its early years, the system received a mixed reception, due to its high price ($599 for a 60-gigabyte model, $499 for a 20 GB model), a complex processor architecture, and lack of quality games but was praised for its Blu-ray capabilities and "untapped potential". The reception would get more positive over time. The system had a slow start in the market[17] but managed to recover, particularly after the introduction of the Slim model. Its successor, the PlayStation 4, was released later in November 2013

https://en.wikipedia.org/wiki/PlayStation_3

- **Nintendo Wii - motion controlled games with wireless controllers, casual gaming (2006) - Nintendo**

The Wii[g] (/wiː/ WEE) is a home video game console developed and marketed by Nintendo. It was released on November 19, 2006, in North America and in December 2006 for most other regions of the world. It is Nintendo's fifth major home game console, following the GameCube and is a seventh generation home console alongside Microsoft's Xbox 360 and Sony's PlayStation 3.

In developing the Wii, Nintendo president Satoru Iwata directed the company to avoid competing with Microsoft and Sony on computational graphics and power and instead to target a broader demographic of players through novel gameplay. Game designers Shigeru Miyamoto and Genyo Takeda led the console's development under the codename Revolution. The primary controller for the Wii is the Wii Remote, a wireless controller with both motion sensing and traditional controls which can be used as a pointing device towards the television screen or for gesture recognition. The Wii was the first Nintendo console to directly support Internet connectivity, supporting both online games and for digital distribution of games and media applications through the Wii Shop Channel. The Wii also supports wireless connectivity with the Nintendo DS handheld console for selected games. Initial Wii models included full backward compatibility support for the GameCube. Later in its lifecycle, two lower-cost Wii models were produced: a revised model with the same design as the original Wii but removed the GameCube compatibility features and the Wii Mini, a compact, budget redesign of the Wii which further removed features including online connectivity and SD card storage.

Because of Nintendo's reduced focus on computational power, the Wii and its games were less expensive to produce than its competitors. The Wii was extremely popular at launch, causing the system to be in short supply in some markets. A bundled game, Wii Sports, was considered the killer app for the console; other flagship games included entries in the Super Mario, Legend of Zelda, Pokémon, and Metroid series. Within a year of launch, the Wii became the best-selling seventh-generation console, and by 2013, had surpassed over 100 million units sold. Total lifetime sales of the Wii had reached over 101 million units, making it Nintendo's best-selling home console until it was surpassed by the Nintendo Switch in 2021. As of 2022, the Wii is the fifth-best-selling home console of all time.

The Wii repositioned Nintendo as a key player in the video game console marketplace. The introduction of motion-controlled games via the Wii Remote led both Microsoft and Sony to develop their own competing products—the Kinect and PlayStation Move, respectively. Nintendo found that, while the Wii had broadened the demographics that they wanted, the core gamer audience had shunned the Wii. The Wii's successor, the Wii U, sought to recapture the core gamer market with additional features atop the Wii. The Wii U was released in 2012, and Nintendo continued to sell both units through the following year. 

https://en.wikipedia.org/wiki/Wii

- **Kinect - controller free (2009) - Microsoft**

Kinect is a "controller-free gaming and entertainment experience" for the Xbox 360. It was first announced on June 1, 2009, at the Electronic Entertainment Expo, under the codename, Project Natal.[102] The add-on peripheral enables users to control and interact with the Xbox 360 without a game controller by using gestures, spoken commands and presented objects and images

https://en.wikipedia.org/wiki/Xbox_360


### Automation

-**CC-Link Open Automation Networks (1996-2000+)**

The CC-Link Open Automation Networks Family are a group of open industrial networks that enable devices from numerous manufacturers to communicate. They are used in a wide variety of industrial automation applications at the machine, cell and line levels.

The CC-Link Partner Association (CLPA) offers a family of open-architecture networks. These originated with the CC-Link (Control & Communication) fieldbus in 1996,[1] developed by Mitsubishi Electric Corporation. In 2000,[2] this was released as an “Open” network so that independent automation equipment manufacturers could incorporate CLPA network compatibility into their products. In the same year, the CC-Link Partner Association (CLPA) was formed to manage and oversee the network technology and support manufacturer members. In 2007,[3] the CLPA was the first organisation to introduce open gigabit Ethernet for automation with CC-Link IE (Industrial Ethernet). In 2018,[4] the CLPA was the first organisation to combine open gigabit Ethernet with Time-Sensitive Networking (TSN) as CC-Link IE TSN. As of May 2020, over 2,100 CLPA compatible products from more than 340 automation manufacturers were available. CLPA offers a variety of open automation network technologies. These are the CC-Link fieldbus, CC-Link Safety fieldbus, CC-Link IE and CC-Link IE TSN. Compatible products include industrial PCs, PLCs, robots, servos, drives, valve manifolds, digital & analogue I/O modules, temperature controllers, mass flow controllers and others. As of May 2020, there was approximately 30 million devices installed worldwide.

https://en.wikipedia.org/wiki/CC-Link_Open_Automation_Networks

- **EtherCAT (2003+)**

EtherCAT (Ethernet for Control Automation Technology) is an Ethernet-based fieldbus system invented by Beckhoff Automation. The protocol is standardized in IEC 61158 and is suitable for both hard and soft real-time computing requirements in automation technology.

The goal during development of EtherCAT was to apply Ethernet for automation applications requiring short data update times (also called cycle times; ≤ 100 μs) with low communication jitter (for precise synchronization purposes; ≤ 1 μs) and reduced hardware costs.

The EtherCAT Technology Group (ETG) was established in 2003, and is the industrial Ethernet user organization with the most members in the world today

The EtherCAT Technology Group (ETG) is an official liaison partner of the IEC (International Electrotechnical Commission) working groups for digital communication. The EtherCAT specification was published as IEC/PAS 62407[8] in 2005, which was removed end of 2007 since EtherCAT had been integrated into the international fieldbus standards IEC 61158[9][10] and IEC 61784-2[11] as well as into the drive profile standard IEC 61800-7.[12] These IEC standards have been approved unanimously in September and October 2007 and were published as IS (International Standards) later that year. In IEC 61800-7

https://en.wikipedia.org/wiki/EtherCAT

- **PROFINET (2003+)**

Profinet (usually styled as PROFINET, as a portmanteau for Process Field Net) is an industry technical standard for data communication over Industrial Ethernet, designed for collecting data from, and controlling equipment in industrial systems, with a particular strength in delivering data under tight time constraints. The standard is maintained and supported by Profibus and Profinet International, an umbrella organization headquartered in Karlsruhe, Germany.

At the general meeting of the Profibus user organisation in 2000, the first concrete discussions for a successor to Profibus based on Ethernet took place. Just one year later, the first specification of Component Based Automation (CBA) was published and presented at the Hanover Fair. In 2002, the Profinet CBA became part of the international standard IEC 61158 / IEC 61784-1.

A Profinet CBA system [27] consists of different automation components. One component comprises all mechanical, electrical and information technology variables. The component may have been created with the usual programming tools. To describe a component, a Profinet Component Description (PCD) file is created in XML. A planning tool loads these descriptions and allows the logical connections between the individual components to be created to implement a plant.

The basic idea behind Profinet CBA was that in many cases it is possible to divide an entire automation system into autonomously operating - and thus manageable - subsystems. The structure and functionality may well be found in several plants in identical or slightly modified form. Such so-called Profinet components are normally controlled by a manageable number of input signals. Within the component, a control program written by the user executes the required functionality and sends the corresponding output signals to another controller. The communication of a component-based system is planned instead of programmed. Communication with Profinet CBA was suitable for bus cycle times of approx. 50 to 100 ms.

Individual systems show how these concepts can be successfully implemented in the application. However, Profinet CBA does not find the expected acceptance in the market and will no longer be listed in the IEC 61784-1 standard from the 4th edition of 2014.

In 2003 the first specification of Profinet IO (IO = Input Output) was published. The application interface of the Profibus DP (DP = Decentralized Periphery), which was successful on the market, was adopted and supplemented with current protocols from the Internet. In the following year, the extension with isochronous transmission follows, which makes Profinet IO suitable for motion control applications. Profisafe is adapted so that it can also be used via Profinet. With the clear commitment of AIDA[28] to Profinet in 2004, acceptance in the market is given. In 2006 Profinet IO becomes part of the international standard IEC 61158 / IEC 61784-2.

In 2007, according to the neutral count, 1 million Profinet devices have already been installed, in the following year this number doubles to 2 million. By 2019, a total of 26 million[29] devices sold by the various manufacturers are reported.

In 2019, the specification for Profinet was completed with Time-Sensitive Networking (TSN),[30] thus introducing the CC-D conformance class.

https://en.wikipedia.org/wiki/Profinet

-**SERCOS (2003-2005)**

Sercos III is the third generation of the Sercos interface, a standardized open digital interface for the communication between industrial controls, motion devices, input/output devices (I/O), and Ethernet nodes, such as PCs. Sercos III applies the hard real-time features of the Sercos interface to Ethernet. It is based upon and conforms to the Ethernet standard (IEEE 802.3 & ISO/IEC 8802-3). Work began on Sercos III in 2003,[1] with vendors releasing first products supporting it in 2005.[2]

https://en.wikipedia.org/wiki/SERCOS_III


## Some standards and protocols
**[`^        back to top        ^`](#)**

### Hardware layers

- **L2TP/IPSE - Layer 2 Tunneling Protocol (2000-2005)**

In computer networking, Layer 2 Tunneling Protocol (L2TP) is a tunneling protocol used to support virtual private networks (VPNs) or as part of the delivery of services by ISPs. It uses encryption ('hiding') only for its own control messages (using an optional pre-shared secret), and does not provide any encryption or confidentiality of content by itself. Rather, it provides a tunnel for Layer 2 (which may be encrypted), and the tunnel itself may be passed over a Layer 3 encryption protocol such as IPsec.[1]

Published in 2000 as proposed standard RFC 2661, L2TP has its origins primarily in two older tunneling protocols for point-to-point communication: Cisco's Layer 2 Forwarding Protocol (L2F) and Microsoft's[2] Point-to-Point Tunneling Protocol (PPTP). A new version of this protocol, L2TPv3, appeared as proposed standard RFC 3931 in 2005. L2TPv3 provides additional security features, improved encapsulation, and the ability to carry data links other than simply Point-to-Point Protocol (PPP) over an IP network (for example: Frame Relay, Ethernet, ATM, etc.).

https://en.wikipedia.org/wiki/Layer_2_Tunneling_Protocol

Because of the lack of confidentiality inherent in the L2TP protocol, it is often implemented along with IPsec. This is referred to as L2TP/IPsec, and is standardized in IETF RFC 3193. The process of setting up an L2TP/IPsec VPN is as follows:

Negotiation of IPsec security association (SA), typically through Internet key exchange (IKE). This is carried out over UDP port 500, and commonly uses either a shared password (so-called "pre-shared keys"), public keys, or X.509 certificates on both ends, although other keying methods exist.
Establishment of Encapsulating Security Payload (ESP) communication in transport mode. The IP protocol number for ESP is 50 (compare TCP's 6 and UDP's 17). At this point, a secure channel has been established, but no tunneling is taking place.
Negotiation and establishment of L2TP tunnel between the SA endpoints. The actual negotiation of parameters takes place over the SA's secure channel, within the IPsec encryption. L2TP uses UDP port 1701.

RFC 3193 Securing L2TP using IPsec

https://datatracker.ietf.org/doc/html/rfc3193

- **Link aggregation : 802.3ad (2000), 802.1AX (2008)**

In computer networking, link aggregation is the combining (aggregating) of multiple network connections in parallel by any of several methods, in order to increase throughput beyond what a single connection could sustain, to provide redundancy in case one of the links should fail, or both. A link aggregation group (LAG) is the combined collection of physical ports.

Other umbrella terms used to describe the concept include trunking,[1] bundling,[2] bonding,[1] channeling[3] or teaming.

Implementation may follow vendor-independent standards such as Link Aggregation Control Protocol (LACP) for Ethernet, defined in IEEE 802.1AX or the previous IEEE 802.3ad, but also proprietary protocols.

Standardization process

By the mid-1990s, most network switch manufacturers had included aggregation capability as a proprietary extension to increase bandwidth between their switches. Each manufacturer developed its own method, which led to compatibility problems. The IEEE 802.3 working group took up a study group to create an interoperable link layer standard (i.e. encompassing the physical and data-link layers both) in a November 1997 meeting.[4] The group quickly agreed to include an automatic configuration feature which would add in redundancy as well. This became known as Link Aggregation Control Protocol (LACP).

802.3ad

As of 2000, most gigabit channel-bonding schemes use the IEEE standard of Link Aggregation which was formerly clause 43 of the IEEE 802.3 standard added in March 2000 by the IEEE 802.3ad task force.[5] Nearly every network equipment manufacturer quickly adopted this joint standard over their proprietary standards.

802.1AX

The 802.3 maintenance task force report for the 9th revision project in November 2006 noted that certain 802.1 layers (such as 802.1X security) were positioned in the protocol stack below Link Aggregation which was defined as an 802.3 sublayer.[6] To resolve this discrepancy, the 802.3ax (802.1AX) task force was formed,[7] resulting in the formal transfer of the protocol to the 802.1 group with the publication of IEEE 802.1AX-2008 on 3 November 2008.[8]

https://en.wikipedia.org/wiki/Link_aggregation#802.1AX

- **RFC 3031 - Multiprotocol Label Switching Architecture (2001)**

Specifies the architecture for Multiprotocol Label Switching (MPLS).

https://en.wikipedia.org/wiki/Multiprotocol_Label_Switching

https://datatracker.ietf.org/doc/html/rfc3031

- **G.709 Interfaces for the Optical Transport Network (OTN) (2001)**

ITU-T Recommendation G.709 Interfaces for the Optical Transport Network (OTN) describes a means of communicating data over an optical network.[1] It is a standardized method for transparent transport of services over optical wavelengths in DWDM systems. It is also known as Optical Transport Hierarchy (OTH) standard. The first edition of this protocol was approved in 2001.[2]

https://en.wikipedia.org/wiki/G.709

- **G.984 GPON - gigabit-capable passive optical network (2003)**

G.984,[1] commonly known as GPON (gigabit-capable passive optical network), is a standard for passive optical networks (PON) published by the ITU-T. It is commonly used to implement the outermost link to the customer (last kilometre or last mile) of fibre-to-the-premises (FTTP) services.[2][3]

GPON puts requirements on the optical medium and the hardware used to access it, and defines the manner in which ethernet frames are converted to an optical signal, as well as the parameters of that signal. The bandwidth of the single connection between the OLT (optical line termination) and the ONTs (optical network terminals) is 2.4 Gbit/s down, 1.2 Gbit/s up, or rarely symmetric 2.4 Gbit/s,[1] shared between up to 128 ONTs using a time-division multiple access (TDMA) protocol, which the standard defines.[4] GPON specifies protocols for error correction (Reed–Solomon) and encryption (AES), and defines a protocol for line control (OMCI) which includes authentication.

https://en.wikipedia.org/wiki/G.984

- **802.3ad - 10 Gigabit Ethernet (2002)**

10 Gigabit Ethernet (10GE, 10GbE, or 10 GigE) is a group of computer networking technologies for transmitting Ethernet frames at a rate of 10 gigabits per second. It was first defined by the IEEE 802.3ae-2002 standard. Unlike previous Ethernet standards, 10 Gigabit Ethernet defines only full-duplex point-to-point links which are generally connected by network switches; shared-medium CSMA/CD operation has not been carried over from the previous generations Ethernet standards[1] so half-duplex operation and repeater hubs do not exist in 10GbE.[2]

The 10 Gigabit Ethernet standard encompasses a number of different physical layer (PHY) standards. A networking device, such as a switch or a network interface controller may have different PHY types through pluggable PHY modules, such as those based on SFP+.[3] Like previous versions of Ethernet, 10GbE can use either copper or fiber cabling. Maximum distance over copper cable is 100 meters but because of its bandwidth requirements, higher-grade cables are required.[a]

The adoption of 10 Gigabit Ethernet has been more gradual than previous revisions of Ethernet: in 2007, one million 10GbE ports were shipped, in 2009 two million ports were shipped, and in 2010 over three million ports were shipped,[4][5] with an estimated nine million ports in 2011.[6] As of 2012, although the price per gigabit of bandwidth for 10 Gigabit Ethernet was about one-third compared to Gigabit Ethernet, the price per port of 10 Gigabit Ethernet still hindered more widespread adoption.[7][8]

https://en.wikipedia.org/wiki/10_Gigabit_Ethernet

- **Power over Ethernet, or PoE (2003)**

Describes any of several standards or ad hoc systems that pass electric power along with data on twisted-pair Ethernet cabling. This allows a single cable to provide both data connection and electric power to devices such as wireless access points (WAPs), Internet Protocol (IP) cameras, and voice over Internet Protocol (VoIP) phones.

There are several common techniques for transmitting power over Ethernet cabling. Three of them have been standardized by Institute of Electrical and Electronics Engineers (IEEE) standard IEEE 802.3 since 2003.

https://en.wikipedia.org/wiki/Power_over_Ethernet

- **IEEE 802.3ah - Ethernet in the first mile, Ethernet to Home (2004)**

With wide, metro, and local area networks using various forms of Ethernet, the goal was to eliminate non-native transport such as Ethernet over Asynchronous Transfer Mode (ATM) from access networks.

One early effort was the EtherLoop technology invented at Nortel Networks in 1996, and then spun off into the company Elastic Networks in 1998.[1][2] Its principal inventor was Jack Terry. The hope was to combine the packet-based nature of Ethernet with the ability of digital subscriber line (DSL) technology to work over existing telephone access wires.[3] The name comes from local loop, which traditionally describes the wires from a telephone company office to a subscriber. The protocol was half-duplex with control from the provider side of the loop. It adapted to line conditions with a peak of 10 Mbit/s advertised, but 4-6 Mbit/s more typical, at a distance of about 12,000 feet (3,700 m). Symbol rates were 1 megabaud or 1.67 megabaud, with 2, 4, or 6 bits per symbol.[1] The EtherLoop product name was registered as a trademark in the US and Canada.[4] The EtherLoop technology was eventually purchased by Paradyne Networks in 2002,[5] which was in turn purchased by Zhone Technologies in 2005.[6]

Another effort was the concept promoted by Michael Silverton of using Ethernet variants that used fiber optic communication to residential as well as business customers. This was an example of what has become known as fiber to the home (FTTH). The Fiberhood Networks company provided this service from 1999 to 2001.[7][8]

Some early products around the year 2000, were marketed as 10BaseS by Infineon Technologies, although they did not technically use baseband signalling, but rather passband as in very-high-bit-rate digital subscriber line (VDSL) technology.[9] A patent was filed in 1997 by Peleg Shimon, Porat Boaz, Noam Alroy, Rubinstain Avinoam and Sfadya Yackow.[10] Long Reach Ethernet was the product name used by Cisco Systems starting in 2001.[11] It supported modes of 5 Mbit/s, 10 Mbit/s, and 15 Mbit/s depending on distance.[12][13]

In October 2000 Howard Frazier issued a call for interest on "Ethernet in the Last Mile".[14] At the November 2000 meeting, IEEE 802.3 created the "Ethernet in the First Mile" study group, and on July 16, 2001 the 802.3ah working group. In parallel participating vendors formed the Ethernet in the First Mile Alliance (EFMA) in December 2001 to promote Ethernet subscriber access technology and support the IEEE standard efforts.[15] At an early meeting, the EtherLoop technology was called 100BASE-CU and another technology called EoVDSL for Ethernet over VDSL.[16]

The working group's EFM standard was approved on June 24, 2004 and published on September 7, 2004 as IEEE 802.3ah-2004. In 2005 it was included into the base IEEE 802.3 standard. In 2005, the EFMA was absorbed by the Metro Ethernet Forum.[17]

https://en.wikipedia.org/wiki/Ethernet_in_the_first_mile

- **G.651.1 - multi-mode optical fiber (MMF) cable (2007)**

an international standard developed by the Standardization Sector of the International Telecommunication Union (ITU-T) that specifies multi-mode optical fiber (MMF) cable.

https://en.wikipedia.org/wiki/G.651.1

- **G.983 - Broadband Passive Optical Networks (BPON) (2007)**

ITU-T Recommendation G.983 is a family of recommendations that defines broadband passive optical network (BPON) for telecommunications Access networks. It originally comprised ten recommendations, G.983.1 through G.983.10, but recommendations .6–.10 were withdrawn when their content was incorporated into G.983.2.

https://en.wikipedia.org/wiki/G.983

- **IEEE 1675-2008 (2008), IEEE 1901 (2011) - broadband over power lines** 

was a standard for broadband over power lines developed by the IEEE Standards Association. It provided electric utility companies with a comprehensive standard for safely installing hardware required for Internet access capabilities over their power lines.
The standard was published 7 January 2008. The IEEE 1901 standard was another related attempt published in 2011.

https://en.wikipedia.org/wiki/IEEE_1675-2008

- **G.hn - 2 Gbit/s over legacy wires (2009)**

Specification for home networking with data rates up to 2 Gbit/s and operation over four types of legacy wires: telephone wiring, coaxial cables, power lines and plastic optical fiber. 
G.hn was developed under the International Telecommunication Union's Telecommunication Standardization sector (the ITU-T) and promoted by the HomeGrid Forum and several other organizations. ITU-T Recommendation (the ITU's term for standard) G.9960, which received approval on October 9, 2009,[2] specified the physical layers and the architecture of G.hn. The Data Link Layer (Recommendation G.9961) was approved on June 11, 2010.[3]

https://en.wikipedia.org/wiki/G.hn

#### Industrial Ethernet

- **Fieldbus - IEC 61784/61158 (2000)**

Fieldbus is the name of a family of industrial computer networks[1] used for real-time distributed control. Fieldbus profiles are standardized by the International Electrotechnical Commission (IEC) as IEC 61784/61158.

A complex automated industrial system is typically structured in hierarchical levels as a distributed control system (DCS). In this hierarchy the upper levels for production managements are linked to the direct control level of programmable logic controllers (PLC) via a non-time-critical communications system (e.g. Ethernet). The fieldbus[2] links the PLCs of the direct control level to the components in the plant of the field level such as sensors, actuators, electric motors, console lights, switches, valves and contactors and replaces the direct connections via current loops or digital I/O signals. The requirement for a fieldbus are therefore time-critical and cost sensitive. Since the new millennium a number of fieldbuses based on Real-time Ethernet have been established. These have the potential to replace traditional fieldbuses in the long term.

In June 1999 the IEC's Committee of Action (CA) decided to take a new structure for the fieldbus standards beginning with a first edition valid at the January 1, 2000, in time for the new millennium: There is a large IEC 61158 standard, where all fieldbuses find their place.[23] The experts have decided that the structure of IEC 61158 is maintained according to different layers, divided into services and protocols. The individual fieldbuses are incorporated into this structure as different types.

https://en.wikipedia.org/wiki/Fieldbus#IEC_61158:_Industrial_communication_networks_-_Fieldbus_specification

- **EtherNet/IP (2000-2009)**

EtherNet/IP (IP = Industrial Protocol)[1] is an industrial network protocol that adapts the Common Industrial Protocol (CIP) to standard Ethernet.[2] EtherNet/IP is one of the leading industrial protocols in the United States and is widely used in a range of industries including factory, hybrid and process. The EtherNet/IP and CIP technologies are managed by ODVA, Inc., a global trade and standards development organization founded in 1995 with over 300 corporate members.

EtherNet/IP uses both of the most widely deployed collections of Ethernet standards –the Internet Protocol suite and IEEE 802.3 – to define the features and functions for its transport, network, data link and physical layers. EtherNet/IP performs at level session and above (level 5, 6 and 7) of the OSI model. CIP uses its object-oriented design to provide EtherNet/IP with the services and device profiles needed for real-time control applications and to promote consistent implementation of automation functions across a diverse ecosystem of products. In addition, EtherNet/IP adapts key elements of Ethernet’s standard capabilities and services to the CIP object model framework, such as the User Datagram Protocol (UDP), which EtherNet/IP uses to transport I/O messages.

Ethernet/IP was estimated to have about 30% share of the industrial ethernet market in 2010[3] and 2018.[4]

Development of EtherNet/IP began in the 1990s within a technical working group of ControlNet International, Ltd.(CI), another trade and standards development organization, In 2000, ODVA and CI formed a joint technology agreement (JTA) for the development of EtherNet/IP. In 2009, the JTA was terminated and EtherNet/IP became under the sole control of ODVA and its members. Today, EtherNet/IP is one of four networks that adapt CIP to an industrial network along with DeviceNet, ControlNet and CompoNet. All of these networks are managed by ODVA, Inc.

https://en.wikipedia.org/wiki/EtherNet/IP

### Application layer

- **Ethernet Powerlink (2001)**

Ethernet Powerlink is a real-time protocol for standard Ethernet. It is an open protocol managed by the Ethernet POWERLINK Standardization Group (EPSG). It was introduced by Austrian automation company B&R in 2001.

https://en.wikipedia.org/wiki/Ethernet_Powerlink

- **RDP - Remote Desktop Protocol (2001)**

Microsoft proprietary protocol to connect to the windows desktop. Released with Windows XP.

https://en.wikipedia.org/wiki/Remote_Desktop_Protocol

- **SSH protocol architecture (2006)**

Includes SCP (superseded by SFTP).

https://www.rfc-editor.org/rfc/rfc4251

### Data layer

#### Web

- **HTML5 (2008)**

HTML5 is a markup language used for structuring and presenting content on the World Wide Web. It is the fifth and final[3] major HTML version that is a World Wide Web Consortium (W3C) recommendation. The current specification is known as the HTML Living Standard. It is maintained by the Web Hypertext Application Technology Working Group (WHATWG), a consortium of the major browser vendors (Apple, Google, Mozilla, and Microsoft).

HTML5 was first released in a public-facing form on 22 January 2008,[2] with a major update and "W3C Recommendation" status in October 2014.[4][5] Its goals were to improve the language with support for the latest multimedia and other new features; to keep the language both easily readable by humans and consistently understood by computers and devices such as web browsers, parsers, etc., without XHTML's rigidity; and to remain backward-compatible with older software. HTML5 is intended to subsume not only HTML 4 but also XHTML 1 and DOM Level 2 HTML.[6]

HTML5 includes detailed processing models to encourage more interoperable implementations; it extends, improves, and rationalizes the markup available for documents and introduces markup and application programming interfaces (APIs) for complex web applications.[7] For the same reasons, HTML5 is also a candidate for cross-platform mobile applications because it includes features designed with low-powered devices in mind.

https://en.wikipedia.org/wiki/HTML5

#### Binary data
- **ISO image (2000s)**

An optical disc image (or ISO image, from the ISO 9660 file system used with CD-ROM media) is a disk image that contains everything that would be written to an optical disc, disk sector by disc sector, including the optical disc file system.[2] ISO images are expected to contain the binary image of an optical media file system (usually ISO 9660 and its extensions or UDF), including the data in its files in binary format, copied exactly as they were stored on the disc. The data inside the ISO image will be structured according to the file system that was used on the optical disc from which it was created.

ISO images can be created from optical discs by disk imaging software, or from a collection of files by optical disc authoring software, or from a different disk image file by means of conversion. Software distributed on bootable discs is often available for download in ISO image format. And like any other ISO image, it may be written to an optical disc such as CD, DVD and Blu-Ray.

https://en.wikipedia.org/wiki/Optical_disc_image

- **ISOBMFF - ISO base media file format (2004)**

The ISO base media file format (ISOBMFF) is a container file format that defines a general structure for files that contain time-based multimedia data such as video and audio.[2][3] It is standardized in ISO/IEC 14496-12, a.k.a. MPEG-4 Part 12, and was formerly also published as ISO/IEC 15444-12, a.k.a. JPEG 2000 Part 12.

It is designed as a flexible, extensible format that facilitates interchange, management, editing and presentation of the media. The presentation may be local, or via a network or other stream delivery mechanism. The file format is designed to be independent of any particular network protocol while enabling support for them in general.[3]

The format has become very widely used for media file storage and as the basis for various other media file formats (e.g. the MP4 and 3GP container formats), and its widespread was recognized by a Technology & Engineering Emmy Award presented on 4 November 2021 by the National Academy of Television Arts and Sciences.[4][5][6]

https://en.wikipedia.org/wiki/ISO_base_media_file_format

#### Images

- **SVG - Scalable Vector Graphics - supersedes WMF (2001)**

Scalable Vector Graphics (SVG) is an XML-based vector image format for defining two-dimensional graphics, having support for interactivity and animation. The SVG specification is an open standard developed by the World Wide Web Consortium (W3C) since 1999.

https://en.wikipedia.org/wiki/Scalable_Vector_Graphics

#### Video

- **DivX Codec (1998-2007)**

DivX ;-) (not DivX) 3.11 Alpha and later 3.xx versions refers to a hacked version of the Microsoft MPEG-4 Version 3 video codec (not to be confused with MPEG-4 Part 3) from Windows Media Tools 4 codecs.[4][5] The video codec, which was actually not MPEG-4 compliant, was extracted around 1998 by French hacker Jerome Rota (also known as Gej) at Montpellier. The Microsoft codec originally required that the compressed output be put in an ASF file. It was altered to allow other containers such as Audio Video Interleave (AVI).[6] Rota hacked the Microsoft codec because newer versions of the Windows Media Player would not play his video portfolio and résumé that were encoded with it. Instead of re-encoding his portfolio, Rota and German hacker Max Morice decided to reverse engineer the codec, which "took about a week".[7]
In early 2000, Jordan Greenhall recruited Rota to form a company (originally called DivXNetworks, Inc., renamed to DivX, Inc. in 2005) to develop an MPEG-4 codec, from scratch, that would still be backward-compatible with the Microsoft MPEG-4 Version 3 format. This effort resulted first in the release of the "OpenDivX" codec and source code on 15 January 2001. OpenDivX was hosted as an open-source project on the Project Mayo web site hosted at projectmayo.com[8] (the name comes from "mayonnaise", because, according to Rota, DivX and mayonnaise are both "French and very hard to make."[7]). The company's internal developers and some external developers worked jointly on OpenDivX for the next several months, but the project eventually stagnated.
In early 2001, DivX employee "Sparky" wrote a new and improved version of the codec's encoding algorithm known as "encore2". This code was included in the OpenDivX public source repository for a brief time, but then was abruptly removed. The explanation from DivX at the time was that "the community really wants a Winamp, not a Linux." It was at this point that the project forked. That summer, Rota left the French Riviera and moved to San Diego "with nothing but a pack of cigarettes"[9] where he and Greenhall founded what would eventually become DivX, Inc.[7]
On 4 December 2007, native MPEG-4 ASP playback support was added to the Xbox 360,[27] allowing it to play video encoded with DivX and other MPEG-4 ASP codecs.[28]
On 17 December 2007, firmware upgrade 2.10 was released for the Sony PlayStation 3, which included official DivX Certification. Firmware version 2.50 (released on 15 October 2008) included support for the DivX Video on Demand (DivX VOD) service, and firmware version 2.60 (released on 20 January 2009) included official DivX Certification and updated Profile support to version 3.11.[29]

https://en.wikipedia.org/wiki/DivX

- **MPEG-4 file format (2001)**

On February 11, 1998, the ISO approved the QuickTime file format as the basis of the MPEG‑4 file format.[16] The MPEG-4 file format specification was created on the basis of the QuickTime format specification published in 2001.[17] The MP4 (.mp4) file format was published in 2001 as the revision of the MPEG-4 Part 1: Systems specification published in 1999 (ISO/IEC 14496-1:2001).[18][19][20] In 2003, the first version of MP4 format was revised and replaced by MPEG-4 Part 14: MP4 file format (ISO/IEC 14496-14:2003).[21] The MP4 file format was generalized into the ISO Base Media File Format ISO/IEC 14496-12:2004, which defines a general structure for time-based media files. It in turn is used as the basis for other multimedia file formats (for example 3GP, Motion JPEG 2000).[22][23][24][25][26] A list of all registered extensions for ISO Base Media File Format is published on the official registration authority website www.mp4ra.org. This registration authority for code-points in "MP4 Family" files is Apple Computer Inc. and it is named in Annex D (informative) in MPEG-4 Part 12.[25]

By 2000, MPEG-4 formats became industry standards, first appearing with support in QuickTime 6 in 2002. Accordingly, the MPEG-4 container is designed to capture, edit, archive, and distribute media, unlike the simple file-as-stream approach of MPEG-1 and MPEG-2.[27]

https://en.wikipedia.org/wiki/QuickTime

- **Advanced Video Coding (AVC) - H.264 - MPEG-4 Part 10 (2003)**

Advanced Video Coding (AVC), also referred to as H.264 or MPEG-4 Part 10, is a video compression standard based on block-oriented, motion-compensated coding.[2] It is by far the most commonly used format for the recording, compression, and distribution of video content, used by 91% of video industry developers as of September 2019.[3][4] It supports resolutions up to and including 8K UHD.[5][6]
The intent of the H.264/AVC project was to create a standard capable of providing good video quality at substantially lower bit rates than previous standards (i.e., half or less the bit rate of MPEG-2, H.263, or MPEG-4 Part 2), without increasing the complexity of design so much that it would be impractical or excessively expensive to implement. This was achieved with features such as a reduced-complexity integer discrete cosine transform (integer DCT),[6][7][8][9] variable block-size segmentation, and multi-picture inter-picture prediction. An additional goal was to provide enough flexibility to allow the standard to be applied to a wide variety of applications on a wide variety of networks and systems, including low and high bit rates, low and high resolution video, broadcast, DVD storage, RTP/IP packet networks, and ITU-T multimedia telephony system
H.264 was standardized by the ITU-T Video Coding Experts Group (VCEG) of Study Group 16 together with the ISO/IEC JTC1 Moving Picture Experts Group (MPEG). The project partnership effort is known as the Joint Video Team (JVT). The ITU-T H.264 standard and the ISO/IEC MPEG-4 AVC standard (formally, ISO/IEC 14496-10 – MPEG-4 Part 10, Advanced Video Coding) are jointly maintained so that they have identical technical content. The final drafting work on the first version of the standard was completed in May 2003, and various extensions of its capabilities have been added in subsequent editions. High Efficiency Video Coding (HEVC), a.k.a. H.265 and MPEG-H Part 2 is a successor to H.264/MPEG-4 AVC developed by the same organizations, while earlier standards are still in common use.
H.264 is perhaps best known as being the most commonly used video encoding format on Blu-ray Discs. It is also widely used by streaming Internet sources, such as videos from Netflix, Hulu, Amazon Prime Video, Vimeo, YouTube, and the iTunes Store, Web software such as the Adobe Flash Player and Microsoft Silverlight, and also various HDTV broadcasts over terrestrial (ATSC, ISDB-T, DVB-T or DVB-T2), cable (DVB-C), and satellite (DVB-S and DVB-S2) systems.

https://en.wikipedia.org/wiki/Advanced_Video_Coding

#### File sharing

- **SMB 2.0 (2006) - Microsoft**

Microsoft introduced a new version of the protocol (**SMB 2.0 or SMB2**) in 2006 with Windows Vista and Windows Server 2008.[26] Although the protocol is proprietary, its specification has been published to allow other systems to interoperate with Microsoft operating systems that use the new protocol.[27]

https://en.wikipedia.org/wiki/Server_Message_Block#SMB_2.0

#### Machine-to-Machine
- **JSON - JavaScript Object Notation - simplifies data interchange compared to XML (2001)**
 
open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other serializable values). First message sent in 2001.

Widely used. Later standardized.

https://en.wikipedia.org/wiki/JSON

#### TV

- **DVB-T2 - Digital Video Broadcasting — Second Generation Terrestrial (2007)**

DVB-T2 is an abbreviation for "Digital Video Broadcasting — Second Generation Terrestrial"; it is the extension of the television standard DVB-T, issued by the consortium DVB, devised for the broadcast transmission of digital terrestrial television. DVB has been standardized by ETSI.
This system transmits compressed digital audio, video, and other data in "physical layer pipes" (PLPs), using OFDM modulation with concatenated channel coding and interleaving. The higher offered bit rate, with respect to its predecessor DVB-T, makes it a system suited for carrying HDTV signals on the terrestrial TV channel (though many broadcasters still use plain DVB-T for this purpose). As of 2019, it was implemented in broadcasts in the United Kingdom (Freeview HD, eight channels across two multiplexes, plus an extra multiplex in Northern Ireland carrying three SD channels), Italy (Europa 7 HD, twelve channels), Finland (21 channels, five in HD), Germany (six HD (1080p50) channels, with 40 in planning),[1] the Netherlands (Digitenne, 30 HD (1080p50) channels), Sweden (five channels),[2][3] Thailand (41 SD, 9 HD channels),[4] Flanders (18 SD channels), Serbia (eight channels),[5] Ukraine (32 SD and HD channels in four nationwide multiplexes), Croatia (all national, local and pay-TV channels), Denmark (two pay-TV multiplexes with 20 channels), Romania (8 SD channels, 1 HD channel), and some other countries.
In March 2006 DVB decided to study options for an upgraded DVB-T standard. In June 2006, a formal study group named TM-T2 (Technical Module on Next Generation DVB-T) was established by the DVB Group to develop an advanced modulation scheme that could be adopted by a second generation digital terrestrial television standard, to be named DVB-T2.[6]
According to the commercial requirements and call for technologies[7] issued in April 2007, the first phase of DVB-T2 would be devoted to provide optimum reception for stationary (fixed) and portable receivers (i.e., units which can be nomadic, but not fully mobile) using existing aerials, whereas a second and third phase would study methods to deliver higher payloads (with new aerials) and the mobile reception issue. The novel system should provide a minimum 30% increase in payload, under similar channel conditions already used for DVB-T.
The BBC, ITV, Channel 4 and Channel 5 agreed with the regulator Ofcom to convert one UK multiplex (B, or PSB3) to DVB-T2 to increase capacity for HDTV via DTT.[8] They expected the first TV region to use the new standard would be Granada in November 2009 (with existing switched over regions being changed at the same time). It was expected that over time there would be enough DVB-T2 receivers sold to switch all DTT transmissions to DVB-T2, and H.264.
Ofcom published its final decision on 3 April 2008, for HDTV using DVB-T2 and H.264:[9] BBC HD would have one HD slot after digital switchover (DSO) at Granada. ITV and C4 had, as expected, applied to Ofcom for the 2 additional HD slots available from 2009 to 2012.[10]
Ofcom indicated that it found an unused channel covering 3.7 million households in London, which could be used to broadcast the DVB-T2 HD multiplex from 2010, i.e., before DSO in London. Ofcom indicated that they would look for more unused UHF channels in other parts of the UK, that can be used for the DVB-T2 HD multiplex from 2010 until DSO.[11]

https://en.wikipedia.org/wiki/DVB-T2

## Some programming languages and frameworks
**[`^        back to top        ^`](#)**

- **.NET Framework (2000-2019) - Microsoft**

The .NET Framework (pronounced as "dot net") is a proprietary software framework developed by Microsoft that runs primarily on Microsoft Windows. It was the predominant implementation of the Common Language Infrastructure (CLI) until being superseded by the cross-platform .NET project. It includes a large class library called Framework Class Library (FCL) and provides language interoperability (each language can use code written in other languages) across several programming languages. Programs written for .NET Framework execute in a software environment (in contrast to a hardware environment) named the Common Language Runtime (CLR). The CLR is an application virtual machine that provides services such as security, memory management, and exception handling. As such, computer code written using .NET Framework is called "managed code". FCL and CLR together constitute the .NET Framework.

FCL provides the user interface, data access, database connectivity, cryptography, web application development, numeric algorithms, and network communications. Programmers produce software by combining their source code with .NET Framework and other libraries. The framework is intended to be used by most new applications created for the Windows platform. Microsoft also produces an integrated development environment for .NET software called Visual Studio.

.NET Framework began as proprietary software, although the firm worked to standardize the software stack almost immediately, even before its first release. Despite the standardization efforts, developers, mainly those in the free and open-source software communities, expressed their unease with the selected terms and the prospects of any free and open-source implementation, especially regarding software patents. Since then, Microsoft has changed .NET development to more closely follow a contemporary model of a community-developed software project, including issuing an update to its patent promising to address the concerns.[2]

Microsoft began developing .NET Framework in the late 1990s, originally under the name of Next Generation Windows Services (NGWS), as part of the .NET strategy. By early 2000, the first beta versions of .NET 1.0 were released.

In August 2000, Microsoft, and Intel worked to standardize Common Language Infrastructure (CLI) and C#. By December 2001, both were ratified Ecma International (ECMA) standards.[4][5] International Organization for Standardization (ISO) followed in April 2003. The current version of ISO standards are ISO/IEC 23271:2012 and ISO/IEC 23270:2006.[6][7]

While Microsoft and their partners hold patents for CLI and C#, ECMA and ISO require that all patents essential to implementation be made available under "reasonable and non-discriminatory terms". The firms agreed to meet these terms, and to make the patents available royalty-free. However, this did not apply to the part of the .NET Framework not covered by ECMA-ISO standards, which included Windows Forms, ADO.NET, and ASP.NET. Patents that Microsoft holds in these areas may have deterred non-Microsoft implementations of the full framework.[8]

On October 3, 2007, Microsoft announced that the source code for .NET Framework 3.5 libraries was to become available under the Microsoft Reference Source License (Ms-RSL[a]).[9] The source code repository became available online on January 16, 2008, and included BCL, ASP.NET, ADO.NET, Windows Forms, WPF, and XML. Scott Guthrie of Microsoft promised that LINQ, WCF, and WF libraries were being added.[10]

The .NET Compact Framework and .NET Micro Framework variants of the .NET Framework provided support for other Microsoft platforms such as Windows Mobile, Windows CE and other resource-constrained embedded devices. Silverlight provided support for web browsers via plug-ins.

https://en.wikipedia.org/wiki/.NET_Framework

- **CUDA (2007) - Nvidia**

CUDA (or Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) that allows software to use certain types of graphics processing units (GPUs) for general purpose processing, an approach called general-purpose computing on GPUs (GPGPU). CUDA is a software layer that gives direct access to the GPU's virtual instruction set and parallel computational elements, for the execution of compute kernels.[1]

CUDA is designed to work with programming languages such as C, C++, and Fortran. This accessibility makes it easier for specialists in parallel programming to use GPU resources, in contrast to prior APIs like Direct3D and OpenGL, which required advanced skills in graphics programming.[2] CUDA-powered GPUs also support programming frameworks such as OpenMP, OpenACC and OpenCL;[3][1] and HIP by compiling such code to CUDA.

CUDA was created by Nvidia.[4] When it was first introduced, the name was an acronym for Compute Unified Device Architecture,[5] but Nvidia later dropped the common use of the acronym.

https://en.wikipedia.org/wiki/CUDA

## Navigation
**[`^        back to top        ^`](#)**
 
- **Civilian use with non-degraded signal (2000)**

On May 2, 2000 "Selective Availability" was discontinued as a result of the 1996 executive order, allowing civilian users to receive a non-degraded signal globally.

- **Europe Galileo (2004)**
In 2004, the United States government signed an agreement with the European Community establishing cooperation related to GPS and Europe's Galileo system.

In 2004, United States President George W. Bush updated the national policy and replaced the executive board with the National Executive Committee for Space-Based Positioning, Navigation, and Timing.[61]

- **GPS for mobile phones (2004)**

November 2004, Qualcomm announced successful tests of assisted GPS for mobile phones.[62]

- **GPS aging infrastructure, modernized GPS satellite**

In 2005, the first modernized GPS satellite was launched and began transmitting a second civilian signal (L2C) for enhanced user performance.[63]

On September 14, 2007, the aging mainframe-based Ground Segment Control System was transferred to the new Architecture Evolution Plan.[64]

On May 19, 2009, the United States Government Accountability Office issued a report warning that some GPS satellites could fail as soon as 2010.[65]

On May 21, 2009, the Air Force Space Command allayed fears of GPS failure, saying "There's only a small risk we will not continue to exceed our performance standard."
[66]
