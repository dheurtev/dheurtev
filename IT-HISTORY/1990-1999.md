# 1990s

**In short:**
- [Inventions](#inventions) : Bluetooth development, LCD displays improvements (TN and IPS), high-efficiency green light-emitting polymer-based device, high brightness blue led, idea of using quantum dots as light source, ADSL, electronic ink
- [Electronics](#electronics) : Integrated ICs to motherboards, Surface mount technology is widely used, PowerPC, Intel Pentium, first while LED sold, Intel Pentium MMX, EFI, Intel Xeon, Intel Hub, AMD 3DNow !, Intel SSE, dual-socket CPU
- [Energy](#energy) : 
- [Telecommunications](#telecommunications) :  Connection of customers to ISPs, Dial-up internet access, Broadband internet (ADSL), Telecommunications Act of 1996, Local loop unbundling, Dot com bubble, Frame Relay, ATM, MPLS development, PBX to IP-PBX, VOIP
- [Networking](#networking) : Transition to IP traffic, birth of IPv6, ethernet to Gigabit Ethernet, Early RFID toy
- [Cryptography](#cryptography) : Cryptography goes mainstream : PGP, DSA, RSA, SSL, TLS
- [Computers](#computers) :
  * [Form factor](#form-factor) : Commodity servers instead of mainframes, PDA with full keyboard (Psion3), Legacy free PC (Apple iMac G3)
  * [OS](#os) :  Microsoft Windows 3.0, 3.1, 95, 98
  * [Peripherals](#peripherals) : Front side bus, PCMIA, PCI, VESA Card, bi-directional printer/scanner cable, USB, CardBus, AGP
  * [Storage](#storage) : first commercial flash-based SSD, flash memory mass storage, Storage area network (SAN), Fiber Channel (FC), iSCSI
  * [Uses](#uses) :
    
    *Servers*: Headless servers, Web/internet (HTTP servers, HTML, URL, XML), Apache (web server), Microsoft Active Directory (Directory service), Asterisk (IP-PBX)
    
    *Clients* : Remote desktop (VNC), Web browsers (Netscape Navigator, Microsoft Internet explorer)
    
    *CGI* : Pixar from hardware to 3D software to Toystory - first full-length 3D film
- [Consumer Electronics](#consumer-electronics) :
  * [Gadgets](#gadgets) : Sophisticated scientific calculators
  * [Multimedia](#multimedia) : DVD, First DVD Player
  * [Screens](#screens) : XGA computer monitors, HD resolution CRT, large plasma flat-panel TVs
  * [Broadcast](#broadcast) : HDTV with ATSC/DVB standards, Medium power satellites with smaller dishes
  * [Video games](#video-games) : Renaissance of arcade games, text-based multi-player virtual reality (Moo), fourth (Nintendo Super NES) to fifth generation video game consoles (Nintento 64, Sony Playstation)
- [Standards and protocols](#standards-and-protocols) : SSL, TLS, JPEG, MPEG-1, MPEG-2, WAV, MP3, PNG, SIP, SSH, VNC, Rsync, CORBA, ZIP compression tools and competitors (Winzip, RAR, ARJ, DEFLATE, 7-zip), CIFS, SOCKS, PPTP, Modems speeds (14.4k to 56k), HDTV (ATSC/DVB)
- [Programming languages and frameworks](#programming-languages-and-frameworks): Haskell (lazy evaluation functional language), OpenGL (rendering 2D and 3D vector graphics), Visual Basic (event-driven programming language for Microsoft, VBA for MS Office), Python (interpreted multi-paradigm dynamically-typed high-level programming language), R (programming language for statistical computing and graphics), Lua (language for extending software applications), Java (client-server applications, class-based object-oriented language, use of virtual machine), JVM (Java Virtual Machine), PHP (scripting language powering the web), Ruby (everything is an object), Javascript (from web-browser scripting language to full stack), Direct X, OCaml, Direct3D, Unreal engine
- [Navigation](#navigation) : GPS is operational, military use, miniaturization and shift to dual-use

## Inventions
**[`^        back to top        ^`](#)**

- **Bluetooth development : from short-link radio technology to Bluetooth (1989-2001) - Ericsson Mobile, IBM**

The development of the "short-link" radio technology, later named Bluetooth, was initiated in 1989 by Nils Rydbeck, CTO at Ericsson Mobile in Lund, Sweden. The purpose was to develop wireless headsets, according to two inventions by Johan Ullman, SE 8902098-6, issued 1989-06-12 and SE 9202239, issued 1992-07-24. Nils Rydbeck tasked Tord Wingren with specifying and Dutchman Jaap Haartsen and Sven Mattisson with developing.[17] Both were working for Ericsson in Lund.[18] Principal design and development began in 1994 and by 1997 the team had a workable solution.[19] From 1997 Örjan Johansson became the project leader and propelled the technology and standardization.[20][21][22][23]

In 1997, Adalio Sanchez, then head of IBM ThinkPad product R&D, approached Nils Rydbeck about collaborating on integrating a mobile phone into a ThinkPad notebook. The two assigned engineers from Ericsson and IBM to study the idea. The conclusion was that power consumption on cellphone technology at that time was too high to allow viable integration into a notebook and still achieve adequate battery life. Instead, the two companies agreed to integrate Ericsson's short-link technology on both a ThinkPad notebook and an Ericsson phone to accomplish the goal. Since neither IBM ThinkPad notebooks nor Ericsson phones were the market share leaders in their respective markets at that time, Adalio Sanchez and Nils Rydbeck agreed to make the short-link technology an open industry standard to permit each player maximum market access. Ericsson contributed the short-link radio technology, and IBM contributed patents around the logical layer. Adalio Sanchez of IBM then recruited Stephen Nachtsheim of Intel to join and then Intel also recruited Toshiba and Nokia. In May 1998, the Bluetooth SIG was launched with IBM and Ericsson as the founding signatories and a total of five members: Ericsson, Intel, Nokia, Toshiba and IBM.

The first Bluetooth device was revealed in 1999. It was a hands-free mobile headset that earned the "Best of show Technology Award" at COMDEX. The first Bluetooth mobile phone was the Ericsson T36 but it was the revised T39 model that actually made it to store shelves in 2001. In parallel, IBM introduced the IBM ThinkPad A30 in October 2001 which was the first notebook with integrated Bluetooth.

https://en.wikipedia.org/wiki/Bluetooth

- **LCD displays improvements (TN and IPS) (1990s)**

In 1990, under different titles, inventors conceived electro optical effects as alternatives to twisted nematic field effect LCDs (TN- and STN- LCDs). One approach was to use interdigital electrodes on one glass substrate only to produce an electric field essentially parallel to the glass substrates.[79][80] To take full advantage of the properties of this In Plane Switching (IPS) technology further work was needed. After thorough analysis, details of advantageous embodiments are filed in Germany by Guenter Baur et al. and patented in various countries.[81][82] The Fraunhofer Institute ISE in Freiburg, where the inventors worked, assigns these patents to Merck KGaA, Darmstadt, a supplier of LC substances. In 1992, shortly thereafter, engineers at Hitachi work out various practical details of the IPS technology to interconnect the thin-film transistor array as a matrix and to avoid undesirable stray fields in between pixels.[83][84]

Hitachi also improved the viewing angle dependence further by optimizing the shape of the electrodes (Super IPS). NEC and Hitachi become early manufacturers of active-matrix addressed LCDs based on the IPS technology. This is a milestone for implementing large-screen LCDs having acceptable visual performance for flat-panel computer monitors and television screens. In 1996, Samsung developed the optical patterning technique that enables multi-domain LCD. Multi-domain and In Plane Switching subsequently remain the dominant LCD designs through 2006.[85] 

In the late 1990s, the LCD industry began shifting away from Japan, towards South Korea and Taiwan,[77] which later shifted to China.

https://en.wikipedia.org/wiki/Liquid-crystal_display

- **High-efficiency green light-emitting polymer-based device (1990)**

Research into polymer electroluminescence culminated in 1990, with J. H. Burroughes et al. at the Cavendish Laboratory at Cambridge University, UK, reporting a high-efficiency green light-emitting polymer-based device using 100 nm thick films of poly(p-phenylene vinylene).[29] Moving from molecular to macromolecular materials solved the problems previously encountered with the long-term stability of the organic films and enabled high-quality films to be easily made.[30] Subsequent research developed multilayer polymers and the new field of plastic electronics and OLED research and device production grew rapidly.[31] White OLEDs, pioneered by J. Kido et al. at Yamagata University, Japan in 1995, achieved the commercialization of OLED-backlit displays and lighting.[32][33]

https://en.wikipedia.org/wiki/OLED

- **High brightness blue led (1993)**

Two years later, in 1993, high-brightness blue LEDs were demonstrated by Shuji Nakamura of Nichia Corporation using a gallium nitride growth process.[48][49][50] In parallel, Isamu Akasaki and Hiroshi Amano of Nagoya University were working on developing the important GaN deposition on sapphire substrates and the demonstration of p-type doping of GaN. This new development revolutionized LED lighting, making high-power blue light sources practical, leading to the development of technologies like Blu-ray.[citation needed]

In 1995, Alberto Barbieri at the Cardiff University Laboratory (GB) investigated the efficiency and reliability of high-brightness LEDs and demonstrated a "transparent contact" LED using indium tin oxide (ITO) on (AlGaInP/GaAs).

https://en.wikipedia.org/wiki/Light-emitting_diode

- **Idea of using quantum dots as a light source (1990s)**

The idea of using quantum dots as a light source emerged in the 1990s. Early applications included imaging using QD infrared photodetectors, light emitting diodes and single-color light emitting devices.[12] 

https://en.wikipedia.org/wiki/Quantum_dot_display

Quantum dots (QDs) are semiconductor particles a few nanometres in size, having optical and electronic properties that differ from larger particles due to quantum mechanics. They are a central topic in nanotechnology. When the quantum dots are illuminated by UV light, an electron in the quantum dot can be excited to a state of higher energy. In the case of a semiconducting quantum dot, this process corresponds to the transition of an electron from the valence band to the conductance band. The excited electron can drop back into the valence band releasing its energy as light. This light emission (photoluminescence) is illustrated in the figure on the right. The color of that light depends on the energy difference between the conductance band and the valence band, or the transition between discrete energy states when band structure is no longer a good definition in QDs.

https://en.wikipedia.org/wiki/Quantum_dot

- **ADSL (1995)**

Lechleider also believed this higher-speed standard would be much more attractive to customers than ISDN had proven. Unfortunately, at these speeds, the systems suffered from a type of crosstalk known as "NEXT", for "near-end crosstalk". This made longer connections on customer lines difficult. Lechleider noted that NEXT only occurred when similar frequencies were being used, and could be diminished if one of the directions used a different carrier rate, but doing so would reduce the potential bandwidth of that channel. Lechleider suggested that most consumer use would be asymmetric anyway, and that providing a high-speed channel towards the user and a lower speed return would be suitable for many uses.[6]
This work in the early 1990s eventually led to the ADSL concept, which emerged in 1995. An early supporter of the concept was Alcatel, who jumped on ADSL while many other companies were still devoted to ISDN. Krish Prabu stated that "Alcatel will have to invest one billion dollars in ADSL before it makes a profit, but it is worth it." They introduced the first DSL Access Multiplexers (DSLAM), the large multi-modem systems used at the telephony offices, and later introduced customer ADSL modems under the Thomson brand. Alcatel remained the primary vendor of ADSL systems for well over a decade.[7]
ADSL quickly replaced ISDN as the customer-facing solution for last-mile connectivity. ISDN has largely disappeared on the customer side, remaining in use only in niche roles like dedicated teleconferencing systems and similar legacy systems.

https://en.wikipedia.org/wiki/Integrated_Services_Digital_Network

- **Electronic Ink (1997) - E Ink**

The notion of a low-power paper-like display had existed since the 1970s, originally conceived by researchers at Xerox PARC, but had never been realized.[4] While a post-doctoral student at Stanford University, physicist Joseph Jacobson envisioned a multi-page book with content that could be changed at the push of a button and required little power to use.[5]

Neil Gershenfeld recruited Jacobson for the MIT Media Lab in 1995, after hearing Jacobson's ideas for an electronic book.[4] Jacobson, in turn, recruited MIT undergrads Barrett Comiskey, a math major, and J.D. Albert, a mechanical engineering major, to create the display technology required to realize his vision.[1]

The initial approach was to create tiny spheres which were half white and half black, and which, depending on the electric charge, would rotate such that the white side or the black side would be visible on the display. Albert and Comiskey were told this approach was impossible by most experienced chemists and materials scientists and they had trouble creating these perfectly half-white, half-black spheres; during his experiments, Albert accidentally created some all-white spheres.[1]

Comiskey experimented with charging and encapsulating those all-white particles in microcapsules mixed in with a dark dye. The result was a system of microcapsules that could be applied to a surface and could then be charged independently to create black and white images.[1] A first patent was filed by MIT for the microencapsulated electrophoretic display in October 1996.[6]

The scientific paper was featured on the cover of Nature, something extremely unusual for work done by undergraduates. The advantage of the microencapsulated electrophoretic display and its potential for satisfying the practical requirements of electronic paper were summarized in the abstract of the Nature paper:

It has for many years been an ambition of researchers in display media to create a flexible low-cost system that is the electronic analogue of paper ... viewing characteristic[s] result in an "ink on paper" look. But such displays have to date suffered from short lifetimes and difficulty in manufacture. Here we report the synthesis of an electrophoretic ink based on the microencapsulation of an electrophoretic dispersion. The use of a microencapsulated electrophoretic medium solves the lifetime issues and permits the fabrication of a bistable electronic display solely by means of printing. This system may satisfy the practical requirements of electronic paper.[7]

A second patent was filed by MIT for the microencapsulated electrophoretic display in March 1997.[8]

Subsequently, Albert, Comiskey and Jacobson along with Russ Wilcox and Jerome Rubin founded the E Ink Corporation in 1997, two months prior to Albert and Comiskey's graduation from MIT.[1]

https://en.wikipedia.org/wiki/E_Ink

## Electronics
**[`^        back to top        ^`](#)**

- **Integrated ICs to Motherboards (1990s)**

By the late 1990s, many personal computer motherboards included consumer-grade embedded audio, video, storage, and networking functions without the need for any expansion cards at all; higher-end systems for 3D gaming and computer graphics typically retained only the graphics card as a separate component. Business PCs, workstations, and servers were more likely to need expansion cards, either for more robust functions, or for higher speeds; those systems often had fewer embedded components.

Laptop and notebook computers that were developed in the 1990s integrated the most common peripherals. This even included motherboards with no upgradeable components, a trend that would continue as smaller systems were introduced after the turn of the century (like the tablet computer and the netbook). Memory, processors, network controllers, power source, and storage would be integrated into some systems.

Motherboards contain a ROM (and later EPROM, EEPROM, NOR flash) to initialize hardware devices and load an operating system from a peripheral device. Microcomputers such as the Apple II and IBM PC used ROM chips mounted in sockets on the motherboard. At power-up, the central processor unit would load its program counter with the address of the Boot ROM and start executing instructions from the Boot ROM. These instructions initialized and tested the system hardware, displays system information on the screen, performed RAM checks, and then loaded an operating system from a peripheral device

https://en.wikipedia.org/wiki/Motherboard

- **Surface mount technology is widely used**

Surface-mount technology emerged in the 1960s, gained momentum in the early 1980s, and became widely used by the mid-1990s. Components were mechanically redesigned to have small metal tabs or end caps that could be soldered directly onto the PCB surface, instead of wire leads to pass through holes. Components became much smaller and component placement on both sides of the board became more common than with through-hole mounting, allowing much smaller PCB assemblies with much higher circuit densities. Surface mounting lends itself well to a high degree of automation, reducing labor costs and greatly increasing production rates compared with through-hole circuit boards. Components can be supplied mounted on carrier tapes. Surface mount components can be about one-quarter to one-tenth of the size and weight of through-hole components, and passive components much cheaper. However, prices of semiconductor surface mount devices (SMDs) are determined more by the chip itself than the package, with little price advantage over larger packages, and some wire-ended components, such as 1N4148 small-signal switch diodes, are actually significantly cheaper than SMD equivalents.

- **PowerPC (1991) - Apple–IBM–Motorola alliance**

PowerPC (with the backronym Performance Optimization With Enhanced RISC – Performance Computing, sometimes abbreviated as PPC) is a reduced instruction set computer (RISC) instruction set architecture (ISA) created by the 1991 Apple–IBM–Motorola alliance, known as AIM. PowerPC, as an evolving instruction set, has been named Power ISA since 2006, while the old name lives on as a trademark for some implementations of Power Architecture–based processors.

PowerPC was the cornerstone of AIM's PReP and Common Hardware Reference Platform (CHRP) initiatives in the 1990s. Originally intended for personal computers, the architecture is well known for being used by Apple's Power Macintosh, PowerBook, iMac, iBook, eMac, and Xserve lines from 1994 until 2005, when Apple migrated to Intel's x86. It has since become a niche in personal computers, but remains popular for embedded and high-performance processors. Its use in 7th generation of video game consoles and embedded applications provide an array of uses, including satellites, and the Curiosity and Perseverance rovers on Mars. In addition, PowerPC CPUs are still used in AmigaOne and third party AmigaOS 4 personal computers.

PowerPC is largely based on the earlier IBM POWER architecture, and retains a high level of compatibility with it; the architectures have remained close enough that the same programs and operating systems will run on both if some care is taken in preparation; newer chips in the Power series use the Power ISA.

https://en.wikipedia.org/wiki/PowerPC

- **Intel Pentium (1993) - Intel**

The Pentium (also referred to as P5, its microarchitecture) is a fifth generation, 32-bit x86 microprocessor that was introduced by Intel on March 22, 1993, as the very first CPU in the Pentium brand.[2][3] It was instruction set compatible with the 80486 but was a new and very different microarchitecture design from previous iterations. The P5 Pentium was the first superscalar x86 microarchitecture and the world's first superscalar microprocessor to be in mass production-meaning it generally executes at least 2 instructions per clock mainly because of a design-first dual integer pipeline design previously thought impossible to implement on a CISC microarchitecture. Additonal features include a faster floating-point unit, wider data bus, separate code and data caches, and many other techniques and features to enhance performance and support security, encryption, and multiprocessing, for workstations and servers when compared to the next best previous industry standard processor implementation before it; the Intel 80486.

Considered the fifth main generation in the 8086 compatible line of processors, its implementation and microarchitecture was called P5. As with all new processors from Intel since the Pentium, some new instructions were added to enhance performance for specific types of workloads.

The Pentium was the first Intel x86 to build in robust hardware support for multiprocessing similar to that of large IBM mainframe computers. Intel worked closely with IBM to define this ability and then Intel designed it into the P5 microarchitecture. This new ability was absent in prior x86 generations and x86 copies from competitors.

To realize its greatest potential, compilers had to be optimized to exploit the instruction level parallelism provided by the new superscalar dual pipelines and applications needed to be recompiled. Intel spent substantial effort and resources working with development tool vendors, and major independent software vendor (ISV) and operating system (OS) companies to optimize their products for Pentium before product launch.

https://en.wikipedia.org/wiki/Pentium_(original)

- **First while LED sold (1996)**

In the autumn of 1996, the first white light-emitting diodes (LEDs) were offered for sale.

https://onlinelibrary.wiley.com/doi/10.1002/lpor.201600147

- **Intel Pentium MMX - MMX instruction set (1996) - Intel**

In October 1996, the similar Pentium MMX[4] was introduced, complementing the same basic microarchitecture with the MMX instruction set, larger caches, and some other enhancements.

https://en.wikipedia.org/wiki/Pentium_(original)

MMX is a single instruction, multiple data (SIMD) instruction set architecture designed by Intel, introduced on January 8, 1997[1][2] with its Pentium P5 (microarchitecture) based line of microprocessors, named "Pentium with MMX Technology".[3] It developed out of a similar unit introduced on the Intel i860,[4] and earlier the Intel i750 video pixel processor. MMX is a processor supplementary capability that is supported on IA-32 processors by Intel and other vendors as of 1997.

https://en.wikipedia.org/wiki/MMX_(instruction_set)

- **EFI - BIOS replacement for servers, superseded by UEFI (1998) - Intel**

The original motivation for EFI came during early development of the first Intel–HP Itanium systems in the mid-1990s. BIOS limitations (such as 16-bit real mode, 1MB addressable memory space,[7] assembly language programming, and PC AT hardware) had become too restrictive for the larger server platforms Itanium was targeting.[8] The effort to address these concerns began in 1998 and was initially called Intel Boot Initiative.[9] It was later renamed to Extensible Firmware Interface (EFI).[10][11]

https://en.wikipedia.org/wiki/UEFI

- **Intel Xeon (1998) - Intel**

Xeon (/ˈziːɒn/ ZEE-on) is a brand of x86 microprocessors designed, manufactured, and marketed by Intel, targeted at the non-consumer workstation, server, and embedded system markets. It was introduced in June 1998. Xeon processors are based on the same architecture as regular desktop-grade CPUs, but have advanced features such as support for ECC memory, higher core counts, more PCI Express lanes, support for larger amounts of RAM, larger cache memory and extra provision for enterprise-grade reliability, availability and serviceability (RAS) features responsible for handling hardware exceptions through the Machine Check Architecture. They are often capable of safely continuing execution where a normal processor cannot due to these extra RAS features, depending on the type and severity of the machine-check exception (MCE). Some also support multi-socket systems with two, four, or eight sockets through use of the Quick Path Interconnect (QPI) bus.

https://en.wikipedia.org/wiki/Xeon

- **Intel 810 - Intel Hub Architecture (1999) - Intel**

Intel Hub Architecture (IHA) was Intel's architecture for the 8xx family of chipsets, starting in 1999 with the Intel 810. It uses a memory controller hub (MCH) that is connected to an I/O controller hub (ICH) via a 266 MB/s bus. The MCH chip supports memory and AGP (replaced by PCI Express in 9xx series chipsets), while the ICH chip provides connectivity for PCI (revision 2.2 before ICH5 series and revision 2.3 since ICH5 series), USB (version 1.1 before ICH4 series and version 2.0 since ICH4 series), sound (originally AC'97, Azalia added in ICH6 series), IDE hard disks (supplemented by Serial ATA since ICH5 series, fully replaced IDE since ICH8 series for desktops and ICH9 series for notebooks) and LAN (uncommonly activated on desktop motherboards and notebooks, usually independent LAN controller were placed instead of PHY chip).

https://en.wikipedia.org/wiki/Intel_Hub_Architecture

- **3D Now (1999) - AMD**

3DNow! is a deprecated extension to the x86 instruction set developed by Advanced Micro Devices (AMD). It adds single instruction multiple data (SIMD) instructions to the base x86 instruction set, enabling it to perform vector processing of floating-point vector-operations using Vector registers, which improves the performance of many graphic-intensive applications. The first microprocessor to implement 3DNow was the AMD K6-2, which was introduced in 1998. When the application was appropriate, this raised the speed by about 2–4 times.[1]

However, the instruction set never gained much popularity, and AMD announced on August 2010 that support for 3DNow would be dropped in future AMD processors, except for two instructions (the PREFETCH and PREFETCHW instructions).[2] The two instructions are also available in Bay-Trail Intel processors.[3]

3DNow was developed at a time when 3D graphics were becoming mainstream in PC multimedia and games. Realtime display of 3D graphics depended heavily on the host CPU's floating-point unit (FPU) to perform floating-point calculations, a task in which AMD's K6 processor was easily outperformed by its competitor, the Intel Pentium II.

As an enhancement to the MMX instruction set, the 3DNow instruction-set augmented the MMX SIMD registers to support common arithmetic operations (add/subtract/multiply) on single-precision (32-bit) floating-point data. Software written to use AMD's 3DNow instead of the slower x87 FPU could execute up to 4x faster, depending on the instruction-mix.

https://en.wikipedia.org/wiki/3DNow!

- **SSE - Streaming SIMD Extensions (1999) - Intel**

In computing, Streaming SIMD Extensions (SSE) is a single instruction, multiple data (SIMD) instruction set extension to the x86 architecture, designed by Intel and introduced in 1999 in their Pentium III series of Central processing units (CPUs) shortly after the appearance of Advanced Micro Devices (AMD's) 3DNow!. SSE contains 70 new instructions (65 unique mnemonics[1] using 70 encodings), most of which work on single precision floating-point data. SIMD instructions can greatly increase performance when exactly the same operations are to be performed on multiple data objects. Typical applications are digital signal processing and graphics processing.

Intel's first IA-32 SIMD effort was the MMX instruction set. MMX had two main problems: it re-used existing x87 floating-point registers making the CPUs unable to work on both floating-point and SIMD data at the same time, and it only worked on integers. SSE floating-point instructions operate on a new independent register set, the XMM registers, and adds a few integer instructions that work on MMX registers.

SSE was subsequently expanded by Intel to SSE2, SSE3, SSSE3 and SSE4. Because it supports floating-point math, it had wider applications than MMX and became more popular. The addition of integer support in SSE2 made MMX largely redundant, though further performance increases can be attained in some situations[when?] by using MMX in parallel with SSE operations.

SSE was originally called Katmai New Instructions (KNI), Katmai being the code name for the first Pentium III core revision. During the Katmai project Intel sought to distinguish it from their earlier product line, particularly their flagship Pentium II. It was later renamed Internet Streaming SIMD Extensions (ISSE[2]), then SSE. AMD eventually added support for SSE instructions, starting with its Athlon XP and Duron (Morgan core) processors.

https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions

- **Abit BP6 - multi-processor board for the consumer market (1999) - Abit**

The legendary Abit BP6, released in 1999 was the first dual-processor socket PPGA 370 motherboard and the first board to finally bring multi-processor boards at an affordable price to the consumer market. There were motherboards prior to the BP6 which featured more than one CPU on the board. Slot 1, Pentium Pro and even so far back as the 386 motherboards were available that could support multiple physical CPUs on board in what was known as SMP processing. Generally these setups were found in very high end and very expensive workstations and were more or less out of reach to the average consumer.

https://ancientelectronics.wordpress.com/2022/03/13/the-abit-bp6-motherboard-dual-cpu-processing-for-the-masses/

## Energy
**[`^        back to top        ^`](#)**



## Telecommunications
**[`^        back to top        ^`](#)**

- **Connection of customers to ISPs (1990s)**

In the 1990s, with the advances of the Internet, leased lines were also used to **connect customer premises to ISP point of presence** 

https://en.wikipedia.org/wiki/Leased_line

- **Dial-up internet Access (1992+)**

Dial-up was first offered commercially in 1992 by Pipex in the United Kingdom and Sprint in the United States.[4][5] After the introduction of commercial broadband in the late 1990s,[6] dial-up Internet access became less popular in the mid-2000s. It is still used where other forms are not available or where the cost is too high, as in some rural or remote areas.[7][8]

https://en.wikipedia.org/wiki/Dial-up_Internet_access

- **Telecommunications Act of 1996**

The Telecommunications Act of 1996 was the first significant overhaul of United States telecommunications law in more than sixty years, amending the Communications Act of 1934. The Act, signed by President Bill Clinton, represented a major change in American telecommunication law, since it was the first time that the Internet was included in broadcasting and spectrum allotment.[1]

According to the Federal Communications Commission (FCC), the goal of the law was to "let anyone enter any communications business – to let any communications business compete in any market against any other."[2] The legislation's primary goal was deregulation of the converging broadcasting and telecommunications markets.[3] However, the law's regulatory policies have been questioned, including the effects of dualistic re-regulation of the communications market.[4][5]

A purpose of the 1996 Act was to foster competition among companies that use similar underlying network technologies (e.g., circuit-switched telephone networks) to provide a single type of service (e.g., voice). For example, it creates separate regulatory regimes for carriers providing voice telephone service and providers of cable television, and a third for information services.

The Telecommunications Act of 1996 did not foster competition among ILECs as the bill had hoped. Instead, of ILECs encroaching on each other, the opposite occurred  – mergers. Before the 1996 Act was passed, the largest four ILECs owned less than half of all the lines in the country while, five years later, the largest four local telephone companies owned about 85% of all the lines in the country.

https://en.wikipedia.org/wiki/Telecommunications_Act_of_1996

- **Local loop unbundling**

Local loop unbundling (LLU or LLUB) is the regulatory process of allowing multiple telecommunications operators to use connections from the telephone exchange to the customer's premises. The physical wire connection between the local exchange and the customer is known as a "local loop", and is owned by the incumbent local exchange carrier (also referred to as the "ILEC", "local exchange", or in the United States either a "Baby Bell" or an independent telephone company). To increase competition, other providers are granted unbundled access.

Pursuant to the Telecommunications Act of 1996, the Federal Communications Commission (FCC) requires that ILECs lease local loops to competitors (CLECs). Prices are set through a market mechanism.[12]

- **Dot com bubble - Internet bubble (1995-2000)**

The dot-com bubble, also known as the dot-com boom,[1] the tech bubble,[2] and the Internet bubble, was a stock market bubble in the late 1990s, a period of massive growth in the use and adoption of the Internet.[2][3]

Between 1995 and its peak in March 2000, the Nasdaq Composite stock market index rose 400%, only to fall 78% from its peak by October 2002, giving up all its gains during the bubble. During the dot-com crash, many online shopping companies, such as Pets.com, Webvan, and Boo.com, as well as several communication companies, such as Worldcom, NorthPoint Communications, and Global Crossing, failed and shut down.[4][5] Some companies that survived, such as Amazon.com, lost large portions of their market capitalization, with Cisco Systems alone losing 80% of its stock value.[6][7]

https://en.wikipedia.org/wiki/Dot-com_bubble

After congress passed the telecommunications Act of 1996, capital flooded into telecom, as existing firms and new ones began building networks over land, undersea and in the air. "Business plans all looked alike," one industry insider recalls. "Massively parallel systems were being built up."

By 2000, however, companies began to realize that there simply wasn't enough business to go around, and they raced "to gain market share" in a burst of "hypercompetition" and "vicious price wars" that drove down revenues,

Around this time, some executives started to engage in the practices that have since led them to contemplate long prison terms instead of the "long boom" predicted for the New Economy. To inflate their profits, some counted operating expenses as capital investment. Or two companies with excess capacity would sell each other the right to use a share of each other's networks. In such a swap, for example, each firm might book $150 million in revenue from the transaction when, in fact, there was no real revenue at all. Not only did such a swap allow each firm to deceive investors about its business, it also created the impression that the industry as a whole was $300 million larger than it actually was.

But such gimmicks couldn't sustain the illusion of growth and profitability indefinitely. While telecom firms were expanding, they had taken on enormous amounts of debt; one firm after another began having difficulty repaying these obligations and went into bankruptcy.

https://prospect.org/features/great-telecom-implosion/

- **Frame Relay**

Frame Relay is a standardized wide area network (WAN) technology that specifies the physical and data link layers of digital telecommunications channels using a packet switching methodology. Originally designed for transport across Integrated Services Digital Network (ISDN) infrastructure, it may be used today in the context of many other network interfaces.

Network providers commonly implement Frame Relay for voice (VoFR) and data as an encapsulation technique used between local area networks (LANs) over a WAN. Each end-user gets a private line (or leased line) to a Frame Relay node. The Frame Relay network handles the transmission over a frequently changing path transparent to all end-user extensively used WAN protocols. It is less expensive than leased lines and that is one reason for its popularity. The extreme simplicity of configuring user equipment in a Frame Relay network offers another reason for Frame Relay's popularity.

With the advent of Ethernet over fiber optics, MPLS, VPN and dedicated broadband services such as cable modem and DSL, Frame Relay has become less popular in recent years.

Frame Relay began as a stripped-down version of the X.25 protocol, releasing itself from the error-correcting burden most commonly associated with X.25. When Frame Relay detects an error, it simply drops the offending packet. 

Although Frame Relay became very popular in North America, it was never very popular in Europe. X.25 remained the primary standard until the wide availability of IP made packet switching almost obsolete. It was used sometimes as backbone for other services, such as X.25 or IP traffic. Where Frame Relay was used in the USA also as carrier for TCP/IP traffic, in Europe backbones for IP networks often used ATM or PoS, later replaced by Carrier Ethernet[3]

https://en.wikipedia.org/wiki/Frame_Relay

- **Development of MPLS (1994-2001)**

Multiprotocol Label Switching (MPLS) is a routing technique in telecommunications networks that directs data from one node to the next based on labels rather than network addresses.[1] Whereas network addresses identify endpoints the labels identify established paths between endpoints. MPLS can encapsulate packets of various network protocols, hence the multiprotocol component of the name. MPLS supports a range of access technologies, including T1/E1, ATM, Frame Relay, and DSL.

  * 1994: Toshiba presented Cell Switch Router (CSR) ideas to IETF BOF
  * 1996: Ipsilon, Cisco and IBM announced label switching plans
  * 1997: Formation of the IETF MPLS working group
  * 1999: First MPLS VPN (L3VPN) and TE deployments
  * 2000: MPLS Traffic Engineering
  * 2001: First MPLS Request for Comments (RFCs) published[4]

- **PBX to IP-PBX, VOIP**

Two significant developments during the 1990s led to new types of PBX systems. One was the massive growth of data networks and increased public understanding of packet switching. Companies needed packet-switched networks for data, so using them for telephone calls proved tempting, and the availability of the Internet as a global delivery system made packet-switched communications even more attractive. These factors led to the development of the voice over IP PBX, or IP-PBX.

The other trend involved the idea of focusing on core competence. PBX services had always been hard to arrange for smaller companies, and many[quantify] companies realized that handling their own telephony was not their core competence. These considerations gave rise to the concept of the hosted PBX. In wireline telephony, the original hosted PBX was the Centrex service provided by telcos since the 1960s; later competitive offerings evolved into the modern competitive local exchange carrier. In voice over IP, hosted solutions are easier to implement as the PBX may be located at and managed by any telephone service provider, connecting to the individual extensions via the Internet. The upstream provider no longer needs to run direct, local leased lines to the served premises.

Since the advent of Internet telephony (Voice over IP) technologies, PBX development has tended toward the IP PBX, which uses the Internet Protocol to carry calls.[5] Most modern PBXs support VoIP. ISDN PBX systems also replaced some traditional PBXs in the 1990s, as ISDN offers features such as conference calling, call forwarding, and programmable caller ID.

https://en.wikipedia.org/wiki/Business_telephone_system#Private_branch_exchange

## Networking
**[`^        back to top        ^`](#)**

### WAN

- **Transition to IP traffic**

Beginning in March 1991 the JANET IP Service (JIPS) was set up as a pilot project to host IP traffic on the existing network.[69] Within eight months the IP traffic had exceeded the levels of X.25 traffic, and the IP support became official in November. Also in 1991, Dai Davies introduced Internet technology over X.25 into the pan-European NREN, EuropaNet, although he experienced personal opposition to this approach.[70][71] The European Academic and Research Network (EARN) and RARE adopted IP around the same time,[nb 7] and the European Internet backbone EBONE became operational in 1992.[53] OSI usage on the NSFNET remained low when compared to TCP/IP. There was some talk of moving JANET to OSI protocols in the 1990s, but this never happened. The X.25 service was closed in August 1997

https://en.wikipedia.org/wiki/Protocol_Wars

### NFC

- **Early RFID toy (1997) - Innovision Research and Technology - Hasbro**

1997: Early form patented and first used in Star Wars character toys for Hasbro. The patent was originally held by Andrew White and Marc Borrett at Innovision Research and Technology (Patent WO9723060). The device allowed data communication between two units in close proximity.[17]

https://en.wikipedia.org/wiki/Near-field_communication

## Cryptography
**[`^        back to top        ^`](#)**

- **PGP (1991)**

Pretty Good Privacy is an encryption program that provides cryptographic privacy and authentication for data communication

https://en.wikipedia.org/wiki/Pretty_Good_Privacy

- **DSA (1991)**

The Digital Signature Algorithm (DSA) is a Federal Information Processing Standard for digital signatures, based on the mathematical concept of modular exponentiation and the discrete logarithm problem. DSA is a variant of the Schnorr and ElGamal signature schemes.[1]: 486 . 
The National Institute of Standards and Technology (NIST) proposed DSA for use in their Digital Signature Standard (DSS) in 1991, and adopted it as FIPS 186 in 1994

https://en.wikipedia.org/wiki/Digital_Signature_Algorithm

- **SSL and TLS development and deployment**

[SSL and TLS](#standards-and-protocols)

- **RSA released in public domain before patent expiration in 2000**

https://en.wikipedia.org/wiki/RSA_(cryptosystem)

## Computers
**[`^        back to top        ^`](#)**

### Form factor
**[`^        back to top        ^`](#)**

- **Commodity servers instead of mainframes (1990s)**

In 1991, AT&T Corporation briefly owned NCR. During the same period, companies found that servers based on microcomputer designs could be deployed at a fraction of the acquisition price and offer local users much greater control over their own systems given the IT policies and practices at that time. Terminals used for interacting with mainframe systems were gradually replaced by personal computers. Consequently, demand plummeted and new mainframe installations were restricted mainly to financial services and government. In the early 1990s, there was a rough consensus among industry analysts that the mainframe was a dying market as mainframe platforms were increasingly replaced by personal computer networks. InfoWorld's Stewart Alsop infamously predicted that the last mainframe would be unplugged in 1996; in 1993, he cited Cheryl Currid, a computer industry analyst as saying that the last mainframe "will stop working on December 31, 1999",[20] a reference to the anticipated Year 2000 problem (Y2K).

https://en.wikipedia.org/wiki/Mainframe_computer

- **Psion3 - PDA with full keyboard (1991) - Psion**

The first PDA, the Organiser, was released in 1984 by Psion, followed by Psion's Series 3, in 1991. The latter began to resemble the more familiar PDA style, including a full keyboard.[4][5] The term PDA was first used on January 7, 1992 by Apple Inc. CEO John Sculley at the Consumer Electronics Show in Las Vegas, Nevada, referring to the Apple Newton.[6] In 1994, IBM introduced the first PDA with analog cellular phone functionality, the IBM Simon, which can also be considered the first smartphone. Then in 1996, Nokia introduced a PDA with digital cellphone functionality, the 9000 Communicator. Another early entrant in this market was Palm, with a line of PDA products which began in March 1996. Palm would eventually be the dominant vendor of PDAs until the rising popularity of Pocket PC devices in the early 2000s.[7] By the mid-2000s most PDAs had morphed into smartphones as classic PDAs without cellular radios were increasingly becoming uncommon.

https://en.wikipedia.org/wiki/Personal_digital_assistant

- **Apple iMac G3 - Legacy free PC (1998) - Apple**

In 1998, Apple's iMac G3 was introduced as the first widely known example of a legacy-free PC,[9][10][11] and drew much criticism for its lack of legacy peripherals such as a floppy drive and Apple Desktop Bus (ADB) connector; [12] However, its success popularizd USB ports.
Compaq released the iPaq desktop in 1999.

From November 1999 to July 2000, Dell's WebPC was an early less-successful Wintel legacy-free PC.
A legacy-free PC is a type of personal computer that lacks a floppy and/or optical disc drive, legacy ports, and an Industry Standard Architecture (ISA) bus (or sometimes, any internal expansion bus at all). According to Microsoft, "The basic goal for these requirements is that the operating system, devices, and end users cannot detect the presence of the following: ISA slots or devices; legacy floppy disk controller (FDC); and PS/2, serial, parallel, and game ports."[1] The legacy ports are usually replaced with Universal Serial Bus (USB) ports. A USB adapter may be used if an older device must be connected to a PC lacking these ports.[2] According to the 2001 edition of Microsoft's PC System Design Guide, a legacy-free PC must be able to boot from a USB device.[3]

https://en.wikipedia.org/wiki/Legacy-free_PC




### OS
**[`^        back to top        ^`](#)**

- **Microsoft Windows 3.0 (1990) : Multi-tasked DOS**

improved the design, mostly because of virtual memory and loadable virtual device drivers (VxDs) that allow Windows to share arbitrary devices between **multi-tasked DOS applications**.

https://en.wikipedia.org/wiki/Microsoft_Windows

**Object Linking & Embedding (OLE) (1990) - Microsoft **

proprietary technology developed by Microsoft that allows embedding and linking to documents and other objects. For developers, it brought OLE Control Extension (OCX), a way to develop and use custom user interface elements. On a technical level, an OLE object is any object that implements the IOleObject interface, possibly along with a wide range of other interfaces, depending on the object's needs.

**OLE 1.0**, released in 1990, was an evolution of the original Dynamic Data Exchange (DDE) concept that Microsoft developed for earlier versions of Windows. While DDE was limited to transferring limited amounts of data between two running applications, OLE was capable of maintaining active links between two documents or even embedding one type of document within another.

- **Microsoft Windows 3.11 (1993) : Windows for Workgroups - peer-to-peer networking features**

Windows 3.1, made generally available on March 1, 1992, featured a facelift. In August 1993, ***Windows for Workgroups, a special version with integrated peer-to-peer networking features*** and a version number of 3.11, was released.

https://en.wikipedia.org/wiki/Microsoft_Windows

**Component Object Model (COM)** is a binary-interface standard for software components introduced by Microsoft in 1993. It is used to enable inter-process communication object creation in a large range of programming languages. COM is the basis for several other Microsoft technologies and frameworks, including OLE, OLE Automation, Browser Helper Object, ActiveX, COM+, DCOM, the Windows shell, DirectX, UMDF and Windows Runtime. The essence of COM is a language-neutral way of implementing objects that can be used in environments different from the one in which they were created, even across machine boundaries. For well-authored components, COM allows reuse of objects with no knowledge of their internal implementation, as it forces component implementers to provide well-defined interfaces that are separated from the implementation. The different allocation semantics of languages are accommodated by making objects responsible for their own creation and destruction through reference-counting. Type conversion casting between different interfaces of an object is achieved through the QueryInterface method. The preferred method of "inheritance" within COM is the creation of sub-objects to which method "calls" are delegated.

COM is an interface technology defined and implemented as standard only on Microsoft Windows and Apple's Core Foundation 1.3 and later plug-in application programming interface (API).[1] The latter only implements a subset of the whole COM interface.[2] For some applications, COM has been replaced at least to some extent by the Microsoft .NET framework, and support for Web Services through the Windows Communication Foundation (WCF). However, COM objects can be used with all .NET languages through .NET COM Interop. Networked DCOM uses binary proprietary formats, while WCF encourages the use of XML-based SOAP messaging. COM is very similar to other component software interface technologies, such as CORBA and Enterprise JavaBeans, although each has its own strengths and weaknesses. Unlike C++, COM provides a stable application binary interface (ABI) that does not change between compiler releases.[3] This makes COM interfaces attractive for object-oriented C++ libraries that are to be used by clients compiled using different compiler versions.

https://en.wikipedia.org/wiki/Component_Object_Model

**OLE custom controls**

introduced in 1994 as a replacement for the now deprecated Visual Basic Extension controls. Instead of upgrading these, the new architecture was based on OLE. In particular, any container that supported OLE 2.0 could already embed OLE custom controls, although these controls cannot react to events unless the container supports this. OLE custom controls are usually shipped in the form of a dynamic link library with the .ocx extension. In 1996 all interfaces for controls (except IUnknown) were made optional to keep the file size of controls down, so they would download faster; these were then called ActiveX Controls.

https://en.wikipedia.org/wiki/Object_Linking_and_Embedding

- **Microsoft Windows 95 (1995) : 32 bit applications, plug and play hardware**

Windows 95 introduced support for native 32-bit applications, plug and play hardware, preemptive multitasking, long file names of up to 255 characters, and provided increased stability over its predecessors. Windows 95 also introduced a redesigned, object oriented user interface, replacing the previous Program Manager with the Start menu, taskbar, and Windows Explorer shell. Windows 95 was a major commercial success for Microsoft.

https://en.wikipedia.org/wiki/Microsoft_Windows

**ActiveX** is a deprecated software framework created by Microsoft that adapts its earlier Component Object Model (COM) and Object Linking and Embedding (OLE) technologies for content downloaded from a network, particularly from the World Wide Web.[1] Microsoft introduced ActiveX in 1996. In principle, ActiveX is not dependent on Microsoft Windows operating systems, but in practice, most ActiveX controls only run on Windows. Most also require the client to be running on an x86-based computer because ActiveX controls contain compiled code.[2]

Faced with the complexity of OLE 2.0 and with poor support for COM in MFC, Microsoft simplified the specification and rebranded the technology as ActiveX in 1996.[6][7] Even after simplification, users still required controls to implement about six core interfaces. In response to this complexity, Microsoft produced wizards, ATL base classes, macros and C++ language extensions to make it simpler to write controls.

ActiveX was one of the major technologies used in component-based software engineering.[4] Compared with JavaBeans, ActiveX supports more programming languages, but JavaBeans supports more platforms.[5] ActiveX is supported in many rapid application development technologies, such as Active Template Library, Delphi, JavaBeans, Microsoft Foundation Class Library, Qt, Visual Basic, Windows Forms and wxWidgets, to enable application developers to embed ActiveX controls into their products.

Many Microsoft Windows applications—including many of those from Microsoft itself, such as Internet Explorer, Microsoft Office, Microsoft Visual Studio, and Windows Media Player—use ActiveX controls to build their feature-set and also encapsulate their own functionality as ActiveX controls which can then be embedded into other applications. Internet Explorer also allows the embedding of ActiveX controls in web pages.

ActiveX was controversial from the start; while Microsoft claimed programming ease and good performance compared to Java applets in its marketing materials, critics of ActiveX were quick to point out security issues and lack of portability, making it impractical for use outside protected intranets.[10] The ActiveX security model relied almost entirely on identifying trusted component developers using a code signing technology called Authenticode. Developers had to register with Verisign (US$20 per year for individuals, $400 for corporations) and sign a contract, promising not to develop malware. Identified code would then run inside the web browser with full permissions, meaning that any bug in the code was a potential security issue; this contrasts with the sandboxing already used in Java at the time.[11]

Microsoft dropped ActiveX support from the Windows Store edition of Internet Explorer 10 in Windows 8. In 2015, Microsoft released Microsoft Edge, the replacement for Internet Explorer with no support for ActiveX, this event marked the end of ActiveX technology in Microsoft's web browser development.[18]

https://en.wikipedia.org/wiki/ActiveX

- **Microsoft Windows 98 (1998) : USB support**

The release of Windows 98 on June 25, 1998, which introduced the Windows Driver Model, support for USB composite devices, support for ACPI, hibernation, and support for multi-monitor configuration**

https://en.wikipedia.org/wiki/Microsoft_Windows

Windows 98 added built-in support for USB Human Interface Device class (USB HID),[105] with native vertical scrolling support.[106] Windows 2000 and Windows Me expanded this built-in support to 5-button mice.[107]

https://en.wikipedia.org/wiki/Computer_mouse


### Peripherals
**[`^        back to top        ^`](#)**

- **Front side bus (1990s)**

The front-side bus was used in all Intel Atom, Celeron, Pentium, Core 2, and Xeon processor models through about 2008. Originally, this bus was a central connecting point for all system devices and the CPU.
A front-side bus (FSB) is a computer communication interface (bus) that was often used in Intel-chip-based computers during the 1990s and 2000s. The EV6 bus served the same function for competing AMD CPUs. Both typically carry data between the central processing unit (CPU) and a memory controller hub, known as the northbridge.[1]
Depending on the implementation, some computers may also have a back-side bus that connects the CPU to the cache. This bus and the cache connected to it are faster than accessing the system memory (or RAM) via the front-side bus. The speed of the front side bus is often used as an important measure of the performance of a computer.
The original front-side bus architecture has been replaced by HyperTransport, Intel QuickPath Interconnect or Direct Media Interface in modern volume CPUs.
The term came into use by Intel Corporation about the time the Pentium Pro and Pentium II products were announced, in the 1990s.
"Front side" refers to the external interface from the processor to the rest of the computer system, as opposed to the back side, where the back-side bus connects the cache (and potentially other CPUs).[2]
A front-side bus (FSB) is mostly used on PC-related motherboards (including personal computers and servers). They are seldom used in embedded systems or similar small computers. The FSB design was a performance improvement over the single system bus designs of the previous decades, but these front-side buses are sometimes referred to as the "system bus."
Front-side buses usually connect the CPU and the rest of the hardware via a chipset, which Intel implemented as a northbridge and a southbridge. Other buses like the Peripheral Component Interconnect (PCI), Accelerated Graphics Port (AGP), and memory buses all connect to the chipset in order for data to flow between the connected devices. These secondary system buses usually run at speeds derived from the front-side bus clock, but are not necessarily synchronized to it.
In response to AMD's Torrenza initiative, Intel opened its FSB CPU socket to third party devices.[3] Prior to this announcement, made in Spring 2007 at Intel Developer Forum in Beijing, Intel had very closely guarded who had access to the FSB, only allowing Intel processors in the CPU socket. The first example was field-programmable gate array (FPGA) co-processors, a result of collaboration between Intel-Xilinx-Nallatech[4] and Intel-Altera-XtremeData (which shipped in 2008).[5][6][7]

https://en.wikipedia.org/wiki/Front-side_bus

- **PCMIA (1990-1992)**

In computing, PC Card is a configuration for computer parallel communication peripheral interface, designed for laptop computers. Originally introduced as PCMCIA, the PC Card standard as well as its successors like CardBus were defined and developed by the Personal Computer Memory Card International Association (PCMCIA).
It was originally designed as a standard for memory-expansion cards for computer storage. The existence of a usable general standard for notebook peripherals led to many kinds of devices being made available based on its configurability, including network cards, modems, and hard disks.
The PCMCIA 1.0 card standard was published by the Personal Computer Memory Card International Association in November 1990 and was soon adopted by more than eighty vendors.[1] [2] It corresponds with the Japanese JEIDA memory card 4.0 standard.[2]
**SanDisk** (operating at the time as "SunDisk") launched its PCMCIA card in October 1992. The company was the first to introduce a writeable Flash RAM card for the HP 95LX (the first MS-DOS pocket computer). These cards conformed to a supplemental PCMCIA-ATA standard that allowed them to appear as more conventional IDE hard drives to the 95LX or a PC. This had the advantage of raising the upper limit on capacity to the full 32M available under DOS 3.22 on the 95LX.[3]
Type II PC Card: IBM V.34 data/fax modem, manufactured by TDK
It soon became clear that the PCMCIA card standard needed expansion to support "smart" I/O cards to address the emerging need for fax, modem, LAN, harddisk and floppy disk cards.[1] It also needed interrupt facilities and hot plugging, which required the definition of new BIOS and operating system interfaces.[1] This led to the introduction of release 2.0 of the PCMCIA standard and JEIDA 4.1 in September 1991,[1][2] which saw corrections and expansion with Card Services (CS) in the PCMCIA 2.1 standard in November 1992.[1][2]

https://en.wikipedia.org/wiki/PC_Card

- **PCI 1.0 - Peripheral Component Interconnect (1992) - Intel**

Peripheral Component Interconnect (PCI)[3] is a local computer bus for attaching hardware devices in a computer and is part of the PCI Local Bus standard. The PCI bus supports the functions found on a processor bus but in a standardized format that is independent of any given processor's native bus. Devices connected to the PCI bus appear to a bus master to be connected directly to its own bus and are assigned addresses in the processor's address space.
Work on PCI began at the Intel Architecture Labs (IAL, also Architecture Development Lab) c. 1990. A team of primarily IAL engineers defined the architecture and developed a proof of concept chipset and platform (Saturn) partnering with teams in the company's desktop PC systems and core logic product organizations.

PCI was immediately put to use in servers, replacing Micro Channel architecture (MCA) and Extended Industry Standard Architecture (EISA) as the server expansion bus of choice. In mainstream PCs, PCI was slower to replace VLB, and did not gain significant market penetration until late 1994 in second-generation Pentium PCs. By 1996, VLB was all but extinct, and manufacturers had adopted PCI even for Intel 80486 (486) computers.[11] EISA continued to be used alongside PCI through 2000. Apple Computer adopted PCI for professional Power Macintosh computers (replacing NuBus) in mid-1995, and the consumer Performa product line (replacing LC Processor Direct Slot (PDS)) in mid-1996.

https://en.wikipedia.org/wiki/Peripheral_Component_Interconnect

- **VESA Card (1993)**

The VESA Local Bus (usually abbreviated to VL-Bus or VLB) is a short-lived expansion bus introduced during the i486 generation of x86 IBM-compatible personal computers. Created by VESA (Video Electronics Standards Association), the VESA Local Bus worked alongside the then-dominant ISA bus to provide a standardized high-speed conduit intended primarily to accelerate video (graphics) operations. VLB provides a standardized fast path that add-in (video) card makers could tap for greatly accelerated memory-mapped I/O and DMA, while still using the familiar ISA bus to handle basic device duties such as interrupts and port-mapped I/O. Some high-end 386dx motherboards also had a VL-Bus slot.

https://en.wikipedia.org/wiki/VESA_Local_Bus

- **IEEE 1284 - bi-directional printer / scanner cable (1994)**

In 1991 the Network Printing Alliance was formed to develop a new standard. In March 1994, the IEEE 1284 specification was released. 1284 included all of these modes, and allowed operation in any of them.
An IEEE 1284 compliant printer cable, with both DB-25 and 36-pin Centronics connectors
The IEEE 1284 standard allows for faster throughput and bidirectional data flow with a theoretical maximum throughput of 4 megabytes per second; actual throughput is around 2 megabytes/second depending on hardware. In the printer venue, this allows for faster printing and back-channel status and management. Since the new standard allowed the peripheral to send large amounts of data back to the host, devices that had previously used SCSI interfaces could be produced at a much lower cost. This included scanners, tape drives, hard disks, computer networks connected directly via parallel interface, network adapters and other devices. No longer was the consumer required to purchase an expensive SCSI card—they could simply use their built-in parallel interface.
The parallel interface has since been mostly displaced by local area network interfaces and USB 2.0.

https://en.wikipedia.org/wiki/IEEE_1284

- **USB 1.X - Universal Serial Bus (1996)**

Universal Serial Bus (USB) is an industry standard that establishes specifications for cables, connectors and protocols for connection, communication and power supply (interfacing) between computers, peripherals and other computers.[2] A broad variety of USB hardware exists, including 14 different connector types, of which USB-C is the most recent and the only one not currently deprecated.
First released in 1996, the USB standards are maintained by the USB Implementers Forum (USB-IF). The four generations of USB are: USB 1.x, USB 2.0, USB 3.x, and USB4.[3]
 * Compaq, DEC, IBM, Intel, Microsoft, NEC, Nortel
USB 1.x
Released in January 1996, USB 1.0 specified signaling rates of 1.5 Mbit/s (Low Bandwidth or Low Speed) and 12 Mbit/s (Full Speed).[15] It did not allow for extension cables or pass-through monitors, due to timing and power limitations. Few USB devices made it to the market until USB 1.1 was released in August 1998. USB 1.1 was the earliest revision that was widely adopted and led to what Microsoft designated the "Legacy-free PC".[16][17][18]

https://en.wikipedia.org/wiki/USB

- **CardBus (1995-1997)**

CardBus are PCMCIA 5.0 or later (JEIDA 4.2 or later) 32-bit PCMCIA devices, introduced in 1995 and present in laptops from late 1997 onward. CardBus is effectively a 32-bit, 33 MHz PCI bus in the PC Card design. CardBus supports bus mastering, which allows a controller on the bus to talk to other devices or memory without going through the CPU. Many chipsets, such as those that support Wi-Fi, are available for both PCI and CardBus

https://en.wikipedia.org/wiki/PC_Card

- **AGP - Accelerated Graphics Port (1997)**

Accelerated Graphics Port (AGP) is a parallel expansion card standard, designed for attaching a video card to a computer system to assist in the acceleration of 3D computer graphics. It was originally designed as a successor to PCI-type connections for video cards. Since 2004, AGP was progressively phased out in favor of PCI Express (PCIe), which is serial, as opposed to parallel; by mid-2008, PCI Express cards dominated the market and only a few AGP models were available,[1] with GPU manufacturers and add-in board partners eventually dropping support for the interface in favor of PCI Express.

https://en.wikipedia.org/wiki/Accelerated_Graphics_Port

The AGP slot first appeared on x86-compatible system boards based on Socket 7 Intel P5 Pentium and Slot 1 P6 Pentium II processors. Intel introduced AGP support with the i440LX Slot 1 chipset on August 26, 1997, and a flood of products followed from all the major system board vendors.[3]
The first Socket 7 chipsets to support AGP were the VIA Apollo VP3, SiS 5591/5592, and the ALI Aladdin V. Intel never released an AGP-equipped Socket 7 chipset. FIC demonstrated the first Socket 7 AGP system board in November 1997 as the FIC PA-2012 based on the VIA Apollo VP3 chipset, followed very quickly by the EPoX P55-VP3 also based on the VIA VP3 chipset which was first to market.[4]
Early video chipsets featuring AGP support included the Rendition Vérité V2200, 3dfx Voodoo Banshee, Nvidia RIVA 128, 3Dlabs PERMEDIA 2, Intel i740, ATI Rage series, Matrox Millennium II, and S3 ViRGE GX/2. Some early AGP boards used graphics processors built around PCI and were simply bridged to AGP. This resulted in the cards benefiting little from the new bus, with the only improvement used being the 66 MHz bus clock, with its resulting doubled bandwidth over PCI, and bus exclusivity. Examples of such cards were the Voodoo Banshee, Vérité V2200, Millennium II, and S3 ViRGE GX/2. Intel's i740 was explicitly designed to exploit the new AGP feature set; in fact it was designed to texture only from AGP memory, making PCI versions of the board difficult to implement (local board RAM had to emulate AGP memory.)
Microsoft first introduced AGP support into Windows 95 OEM Service Release 2 (OSR2 version 1111 or 950B) via the USB SUPPLEMENT to OSR2 patch.[5] After applying the patch the Windows 95 system became Windows 95 version 4.00.950 B. The first Windows NT-based operating system to receive AGP support was Windows NT 4.0 with Service Pack 3, introduced in 1997. Linux support for AGP enhanced fast data transfers was first added in 1999 with the implementation of the AGPgart kernel module.

https://en.wikipedia.org/wiki/Accelerated_Graphics_Port


### Storage
**[`^        back to top        ^`](#)**

- **first commercial flash-based SSD (1991) - SanDisk**

The first commercial flash-based SSD was shipped by SanDisk in 1991.[35] It was a 20 MB SSD in a PCMCIA configuration, and sold OEM for around $1,000 and was used by IBM in a ThinkPad laptop.[39] In 1998, SanDisk introduced SSDs in 2.5-inch and 3.5-inch form factors with PATA interfaces.[40]

In 1995, STEC, Inc. entered the flash memory business for consumer electronic devices.[41]

In 1995, M-Systems introduced flash-based solid-state drives[42] as HDD replacements for the military and aerospace industries, as well as for other mission-critical applications. These applications require the SSD's ability to withstand extreme shock, vibration, and temperature ranges.[43]

In 1999, BiTMICRO made a number of introductions and announcements about flash-based SSDs, including an 18 GB[44] 3.5-inch SSD.[45] In 2007, Fusion-io announced a PCIe-based Solid state drive with 100,000 input/output operations per second (IOPS) of performance in a single card, with capacities up to 320 GB.[46]

https://en.wikipedia.org/wiki/Solid-state_drive

- **Storage area network (SAN) (1992-1993)"

A storage area network (SAN) or storage network is a computer network which provides access to consolidated, block-level data storage. SANs are primarily used to access data storage devices, such as disk arrays and tape libraries from servers so that the devices appear to the operating system as direct-attached storage. A SAN typically is a dedicated network of storage devices not accessible through the local area network (LAN).

Although a SAN provides only block-level access, file systems built on top of SANs do provide file-level access and are known as shared-disk file systems.

Newer SAN configurations enable hybrid SAN[1] and allow traditional block storage that appears as local storage but also object storage for web services through APIs.

Storage area networks (SANs) are sometimes referred to as network behind the servers[2]: 11  and historically developed out of a centralized data storage model, but with its own data network. A SAN is, at its simplest, a dedicated network for data storage. 

https://en.wikipedia.org/wiki/Storage_area_network

Storage Systems were evolving since when we were introduced to this technology. It was likely 1993 or 1994 when a new storage technology emerged named Storage Area Network aka SAN. At first, it was not that popular as you know it can happen with any new technology. 

Also, the cost of developing a newly arrived storage system was a crucial factor. As you know the users need various types of components to build a SAN. For instance, you will need SAN switches, servers, storage disks, tape libraries, and JBODS, etc. So, overall the price of this technology was very expensive at that time.

Due to that, only the big companies could take advantage of this brand new storage system. With the passing of time, a lot of data storage system providers from all around the world started to offer pre-built SAN storage to businesses and organizations. As a result, the competition between those companies grew and the price of the components also came to a lower level. And, SAN became a popular alternative to store and manage data.

Added to that, when people saw that it uses fiber channel technology and fiber channel protocol for transferring data, it was a hit. We will talk about the popularity of SAN in the later section of the post in detail. But overall, we can say that SAN came into the scene back in the 90s and dominated the storage system market for over a decade and a half.

https://www.reviewplan.com/evolution-of-storage-area-network/

- **Fibre Channel (FC) (1993) - IBM**

Fibre Channel (FC) is a high-speed data transfer protocol providing in-order, lossless[1] delivery of raw block data.[2] Fibre Channel is primarily used to connect computer data storage to servers[3][4] in storage area networks (SAN) in commercial data centers.
Fibre Channel networks form a switched fabric because the switches in a network operate in unison as one big switch. Fibre Channel typically runs on optical fiber cables within and between data centers, but can also run on copper cabling.[3][4] Supported data rates include 1, 2, 4, 8, 16, 32, 64, and 128 gigabit per second resulting from improvements in successive technology generations. The industry now notates this as Gigabit Fibre Channel (GFC).
There are various upper-level protocols for Fibre Channel, including two for block storage. Fibre Channel Protocol (FCP) is a protocol that transports SCSI commands over Fibre Channel networks.[3][4] FICON is a protocol that transports ESCON commands, used by IBM mainframe computers, over Fibre Channel. Fibre Channel can be used to transport data from storage systems that use solid-state flash memory storage medium by transporting NVMe protocol commands.
133 Mbit/s	0.1328125	8b10b	12.5	1993

https://en.wikipedia.org/wiki/Fibre_Channel

- **CompactFlash (CF) - flash memory mass storage (1994) - Sandisk**

CompactFlash (CF) is a flash memory mass storage device used mainly in portable electronic devices. The format was specified and the devices were first manufactured by SanDisk in 1994.[3] CompactFlash became one of the most successful of the early memory card formats, surpassing Miniature Card and SmartMedia. Subsequent formats, such as MMC/SD, various Memory Stick formats, and xD-Picture Card offered stiff competition

https://en.wikipedia.org/wiki/CompactFlash

- **iSCSI - Internet Small Computer Systems Interface (1998) - IBM / Cisco**

Internet Small Computer Systems Interface or iSCSI (/ˈaɪskʌzi/ (listen) EYE-skuz-ee) is an Internet Protocol-based storage networking standard for linking data storage facilities. iSCSI provides block-level access to storage devices by carrying SCSI commands over a TCP/IP network. iSCSI facilitates data transfers over intranets and to manage storage over long distances. It can be used to transmit data over local area networks (LANs), wide area networks (WANs), or the Internet and can enable location-independent data storage and retrieval.
The protocol allows clients (called initiators) to send SCSI commands (CDBs) to storage devices (targets) on remote servers. It is a storage area network (SAN) protocol, allowing organizations to consolidate storage into storage arrays while providing clients (such as database and web servers) with the illusion of locally attached SCSI disks.[1] It mainly competes with Fibre Channel, but unlike traditional Fibre Channel which usually requires dedicated cabling,[a] iSCSI can be run over long distances using existing network infrastructure.[2] iSCSI was pioneered by IBM and Cisco in 1998 and submitted as a draft standard in March 2000.[3]

https://en.wikipedia.org/wiki/ISCSI

### Uses
**[`^        back to top        ^`](#)**

#### Servers

- **Headless servers : GUI makes graphic display terminals and terminal emulation obsolete (1990s)**

graphic display terminals, and terminal emulation became obsolete in the 1990s due to the advent of personal computers provided with **GUIs**.

https://en.wikipedia.org/wiki/Mainframe_computer

- **httpd (1990) : First web server goes live** 

https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol

CERN httpd (later also known as W3C httpd) is an early, now discontinued, web server (HTTP) daemon originally developed at CERN from 1990 onwards by Tim Berners-Lee, Ari Luotonen[2] and Henrik Frystyk Nielsen.[1] Implemented in C, it was the first web server software.[3]

https://en.wikipedia.org/wiki/CERN_httpd

- **NSCA HTTPd (1993)**

NCSA HTTPd is an early, now discontinued, web server originally developed at the NCSA at the University of Illinois at Urbana–Champaign by Robert McCool and others.[1] First released in 1993, it was among the earliest web servers developed, following Tim Berners-Lee's CERN httpd, Tony Sanders' Plexus server, and some others

https://en.wikipedia.org/wiki/NCSA_HTTPd

- **Apache HTTP Server (1995): Most successful internet server**

Originally based on the NCSA HTTPd server, development of Apache began in early 1995 after work on the NCSA code stalled. Apache played a key role in the initial growth of the World Wide Web,[11] quickly overtaking NCSA HTTPd as the dominant HTTP server. In 2009, it became the first web server software to serve more than 100 million websites.[12]

https://en.wikipedia.org/wiki/Apache_HTTP_Server

- **Microsoft Active Directory - Directory service (1999) - Microsoft**

Active Directory (AD) is a directory service developed by Microsoft for Windows domain networks. It is included in most Windows Server operating systems as a set of processes and services.[1][2] Initially, Active Directory was used only for centralized domain management. However, Active Directory eventually became an umbrella title for a broad range of directory-based identity-related services.[3]

A server running the Active Directory Domain Service (AD DS) role is called a domain controller. It authenticates and authorizes all users and computers in a Windows domain type network, assigning and enforcing security policies for all computers, and installing or updating software. For example, when a user logs into a computer that is part of a Windows domain, Active Directory checks the submitted username and password and determines whether the user is a system administrator or normal user.[4] Also, it allows management and storage of information, provides authentication and authorization mechanisms and establishes a framework to deploy other related services: Certificate Services, Active Directory Federation Services, Lightweight Directory Services, and Rights Management Services.[5]

Active Directory uses Lightweight Directory Access Protocol (LDAP) versions 2 and 3, Microsoft's version of Kerberos,[6] and DNS.[7]

Like many information-technology efforts, Active Directory originated out of a democratization of design using Request for Comments (RFCs). The Internet Engineering Task Force (IETF), which oversees the RFC process, has accepted numerous RFCs initiated by widespread participants. For example, LDAP underpins Active Directory. Also, X.500 directories and the Organizational Unit preceded the Active Directory concept that makes use of those methods. The LDAP concept began to emerge even before the founding of Microsoft in April 1975, with RFCs as early as 1971. RFCs contributing to LDAP include RFC 1823 (on the LDAP API, August 1995),[8] RFC 2307, RFC 3062, and RFC 4533.[9][10][11]

Microsoft previewed Active Directory in 1999, released it first with **Windows 2000 Server edition**, and revised it to extend functionality and improve administration in **Windows Server 2003**. Active Directory support was also added to Windows 95, Windows 98 and Windows NT 4.0 via patch, with some features being unsupported.[12][13] Additional improvements came with subsequent versions of Windows Server. In **Windows Server 2008**, additional services were added to Active Directory, such as Active Directory Federation Services.[14] The part of the directory in charge of the management of domains, which was previously a core part of the operating system,[14] was renamed Active Directory Domain Services (ADDS) and became a server role like others.[3] "Active Directory" became the umbrella title of a broader range of directory-based services.[15] According to Byron Hynes, everything related to identity was brought under Active Directory's banner.[3]

https://en.wikipedia.org/wiki/Active_Directory

- **Asterisk - IP-PBX server (1999) - Digium**

Asterisk is a software implementation of a private branch exchange (PBX). In conjunction with suitable telephony hardware interfaces and network applications, Asterisk is used to establish and control telephone calls between telecommunication endpoints, such as customary telephone sets, destinations on the public switched telephone network (PSTN), and devices or services on voice over Internet Protocol (VoIP) networks. Its name comes from the asterisk (*) symbol for a signal used in dual-tone multi-frequency (DTMF) dialing.

Asterisk was created in 1999 by Mark Spencer of Digium, which since 2018 is a division of Sangoma Technologies Corporation.[6][7] Originally designed for Linux,[8] Asterisk runs on a variety of operating systems, including NetBSD, OpenBSD, FreeBSD, macOS, and Solaris, and can be installed in embedded systems based on OpenWrt.[9][10]

https://en.wikipedia.org/wiki/Asterisk_(PBX)

#### Clients

##### Remote desktop

- **VNC (1999): Remote desktop application** : 

The remote desktop application VNC (Virtual Network Computing) is made available by the AT&T Laboratories Cambridge. ORL, the Olivetti Research Laboratory was founded 12 years earlier, accquired in 1999 by AT&T to create AT&T; Laboratories Cambridge.

https://en.wikipedia.org/wiki/Virtual_Network_Computing

##### Web browsers

- **NSCA Mosaïc (1993)**: web browser developped at the end of 1992 at the american research center NCSA

https://en.wikipedia.org/wiki/Mosaic_(web_browser)

- **Netscape Navigator (1994): web browser**

https://en.wikipedia.org/wiki/Netscape

- **Microsoft Internet Explorer (1995): Most successful web browser**

Starting in 1995, It was first released as part of the add-on package Plus! for Windows 95 that year. Later versions were available as free downloads, or in-service packs, and included in the original equipment manufacturer (OEM) service releases of Windows 95 and later versions of Windows. 
Internet Explorer was once the most widely used web browser, attaining a peak of 95% usage share by 2003.[12] This came after Microsoft used bundling to win the first browser war against Netscape, which was the dominant browser in the 1990s

https://en.wikipedia.org/wiki/Internet_Explorer

#### CGI

- **Pixar from hardware to 3D software to Toystory - first full-length 3D film (1995)**

In April 1990, Pixar sold its hardware division, including all proprietary hardware technology and imaging software, to Vicom Systems, and transferred 18 of Pixar's approximately 100 employees. That year, Pixar moved from San Rafael to Richmond, California.[33] Pixar released some of its software tools on the open market for Macintosh and Windows systems. RenderMan is one of the leading 3D packages of the early 1990s, and Typestry is a special-purpose 3D text renderer that competed with RayDream.[citation needed]

During this period, Pixar continued its successful relationship with Walt Disney Feature Animation, a studio whose corporate parent would ultimately become its most important partner. As 1991 began, however, the layoff of 30 employees in the company's computer hardware department—including the company's president, Chuck Kolstad,[34] reduced the total number of employees to just 42, approximately its original number.[35] Pixar made a historic $26 million deal with Disney to produce three computer-animated feature films, the first of which was Toy Story, the product of the technological limitations that challenged CGI.[36] By then the software programmers, who were doing RenderMan and IceMan, and Lasseter's animation department, which made television commercials (and four Luxo Jr. shorts for Sesame Street the same year), were all that remained of Pixar.[37]

Even with income from these projects, the company continued to lose money and Steve Jobs, as chairman of the board and now the full owner, often considered selling it. Even as late as 1994, Jobs contemplated selling Pixar to other companies such as Hallmark Cards, Microsoft co-founder Paul Allen, and Oracle CEO and co-founder Larry Ellison.[38] Only after learning from New York critics that Toy Story would probably be a hit—and confirming that Disney would distribute it for the 1995 Christmas season—did he decide to give Pixar another chance.[39][40] For the first time, he also took an active leadership role in the company and made himself CEO.[citation needed] Toy Story grossed more than $373 million worldwide[41] and, when Pixar held its initial public offering on November 29, 1995, it exceeded Netscape's as the biggest IPO of the year. In its first half-hour of trading, Pixar stock shot from $22 to $45, delaying trading because of unmatched buy orders. Shares climbed to US$49 and closed the day at $39.[42]

https://en.wikipedia.org/wiki/Pixar#Early_history

## Consumer Electronics
**[`^        back to top        ^`](#)**

### Gadgets
**[`^        back to top        ^`](#)**

- **Sophisticated calculators, TI, HP**

The two leading manufacturers, HP and TI, released increasingly feature-laden calculators during the 1980s and 1990s. At the turn of the millennium, the line between a graphing calculator and a handheld computer was not always clear, as some very advanced calculators such as the TI-89, the Voyage 200 and HP-49G could differentiate and integrate functions, solve differential equations, run word processing and PIM software, and connect by wire or IR to other calculators/computers.

https://en.wikipedia.org/wiki/Calculator

### Multimedia
**[`^        back to top        ^`](#)**

- **DVD (1996)**

The DVD (common abbreviation for Digital Video Disc or Digital Versatile Disc)[8][9] is a digital optical disc data storage format invented and developed in 1995 and released in late 1996. Currently allowing up to 17.08 GB of storage,[10] the medium can store any kind of digital data and was widely used for software and other computer files as well as video programs watched using DVD players. DVDs offer higher storage capacity than compact discs while having the same dimensions.

https://en.wikipedia.org/wiki/DVD

- **Toshiba SD-3000 - First DVD Player (1996)**

In November 1996, Toshiba introduced the world's first DVD player, the SD-3000, as a result of developments initiated in 1994. At the time, the VHS VCR was dominating the market.Further, the laser disk used analog video.

The DVD (called SD at the time) produced by Toshiba used digital audio and video could fit an entire movie on a disk the same size as a CD: 12 cm in diameter. This was a revolutionary standard that made possible high audio and video quality and multiple functions. I

https://toshiba-mirai-kagakukan.jp/en/learn/history/ichigoki/1996dvd/index.htm

### Screens
**[`^        back to top        ^`](#)**

- **XGA - Extended Graphics Array (1990) - IBM**

The Extended Graphics Array (XGA) is an IBM display standard introduced in 1990. Later it became the most common appellation of the **1024 × 768 pixels** display resolution, but the official definition is broader than that. It was not a new and improved replacement for Super VGA, but rather became one particular subset of the broad range of capabilities covered under the "Super VGA" umbrella.

https://en.wikipedia.org/wiki/Graphics_display_resolution#Extended_Graphics_Array

- **first CRT with HD resolution (1990) - Sony**

In 1990, the first CRTs with HD resolution were released to the market by Sony.[67]

In the mid-1990s, some 160 million CRTs were made per year.[68]

https://en.wikipedia.org/wiki/Cathode-ray_tube

- **large plasma flat-panel TV (1990s)**

In 1992, Fujitsu introduced the world's first 21-inch (53 cm) full-color display. It was based on technology created at the University of Illinois at Urbana–Champaign and NHK Science & Technology Research Laboratories.

In 1994, Weber demonstrated a color plasma display at an industry convention in San Jose. Panasonic Corporation began a joint development project with Plasmaco, which led in 1996 to the purchase of Plasmaco, its color AC technology, and its American factory for US$26 million.

In 1995, Fujitsu introduced the first 42-inch (107 cm) plasma display panel;[61][62] it had 852×480 resolution and was progressively scanned.[63] Two years later, Philips introduced the first large commercially available flat-panel TV, using the Fujitsu panels. It was available at four Sears locations in the US for $14,999, including in-home installation. Pioneer also began selling plasma televisions that year, and other manufacturers followed. By the year 2000 prices had dropped to $10,000.

https://en.wikipedia.org/wiki/Plasma_display

### Broadcast
**[`^        back to top        ^`](#)**

- **Europe: HDTV launch - Italy (1990)**

Between 1988 and 1991, several European organizations were working on discrete cosine transform (DCT) based digital video coding standards for both SDTV and HDTV. The EU 256 project by the CMTT and ETSI, along with research by Italian broadcaster RAI, developed a DCT video codec that broadcast near-studio-quality HDTV transmission at about 70–140 Mbit/s.[23][40] The first HDTV transmissions in Europe, albeit not direct-to-home, began in 1990, when RAI broadcast the 1990 FIFA World Cup using several experimental HDTV technologies, including the digital DCT-based EU 256 codec,[23] 

https://en.wikipedia.org/wiki/High-definition_television

- **Europe : DVB - Digital Video Broadcasting (1993-1997) : Satelitte, Cable, Terrestrial**

DVB-S and DVB-C were ratified in 1994. DVB-T was ratified in early 1997. The first commercial DVB-T broadcasts were performed by the United Kingdom's Digital TV Group in late 1998. In 2003 Berlin, Germany was the first area to completely stop broadcasting analog TV signals. Most European countries are fully covered by digital television and many have switched off PAL/SECAM services.
Digital Video Broadcasting (DVB) is a set of international open standards for digital television. DVB standards are maintained by the DVB Project, an international industry consortium,[1] and are published by a Joint Technical Committee (JTC) of the European Telecommunications Standardisé Institute (ETSI), European Committee for Electrotechnical Standardization (CENELEC) and European Broadcasting Union (EBU).
Satellite: DVB-S, DVB-S2, and DVB-SH
DVB-SMATV for distribution via SMATV
Cable: DVB-C, DVB-C2

https://en.wikipedia.org/wiki/Digital_Video_Broadcasting

DVB-T, short for Digital Video Broadcasting — Terrestrial, is the DVB European-based consortium standard for the broadcast transmission of digital terrestrial television that was first published in 1997[1] and first broadcast in Singapore in February, 1998.[2][3][4][5][6][7][8] This system transmits compressed digital audio, digital video and other data in an MPEG transport stream, using coded orthogonal frequency-division multiplexing (COFDM or OFDM) modulation. It is also the format widely used worldwide (including North America) for Electronic News Gathering for transmission of video and audio from a mobile newsgathering vehicle to a central receive point.

https://en.wikipedia.org/wiki/DVB-T

- **HDTV deployment start (1994)**

HDTV technology was introduced in the United States in the early 1990s and made official in 1993 by the Digital HDTV Grand Alliance, a group of television, electronic equipment, communications companies consisting of AT&T Bell Labs, General Instrument, Philips, Sarnoff, Thomson, Zenith and the Massachusetts Institute of Technology. Field testing of HDTV at 199 sites in the United States was completed August 14, 1994.[34] The first public HDTV broadcast in the United States occurred on July 23, 1996, when the Raleigh, North Carolina television station WRAL-HD began broadcasting from the existing tower of WRAL-TV southeast of Raleigh, winning a race to be first with the HD Model Station in Washington, D.C., which began broadcasting July 31, 1996 with the callsign WHD-TV, based out of the facilities of NBC owned and operated station WRC-TV.[35][36][37] The American Advanced Television Systems Committee (ATSC) HDTV system had its public launch on October 29, 1998, during the live coverage of astronaut John Glenn's return mission to space on board the Space Shuttle Discovery.[38] The signal was transmitted coast-to-coast, and was seen by the public in science centers, and other public theaters specially equipped to receive and display the broadcast.[38][39]

https://en.wikipedia.org/wiki/High-definition_television

- **USA : ATSC HDTV Standards (1996)**

Advanced Television Systems Committee (ATSC) standards are an American set of standards for digital television transmission over terrestrial, cable and satellite networks. It is largely a replacement for the analog NTSC standard and, like that standard, is used mostly in the United States, Mexico, Canada, and South Korea. Several former NTSC users, in particular Japan, have not used ATSC during their digital television transition, because they adopted their own system called ISDB.
The ATSC standards were developed in the early 1990s by the Grand Alliance, a consortium of electronics and telecommunications companies that assembled to develop a specification for what is now known as HDTV.
The high-definition television standards defined by the ATSC produce widescreen 16:9 images up to 1920×1080 pixels in size – more than six times the display resolution of the earlier standard. However, many different image sizes are also supported. The reduced bandwidth requirements of lower-resolution images allow up to six standard-definition "subchannels" to be broadcast on a single 6 MHz TV channel.
ATSC standards are marked A/x (x is the standard number) and can be downloaded for free from the ATSC's website at ATSC.org. ATSC Standard A/53, which implemented the system developed by the Grand Alliance, was published in 1995; the standard was adopted by the Federal Communications Commission in the United States in 1996. It was revised in 2009. ATSC Standard A/72 was approved in 2008 and introduces H.264/AVC video coding to the ATSC system.
ATSC supports 5.1-channel surround sound using Dolby Digital's AC-3 format. Numerous auxiliary datacasting services can also be provided.
Many aspects of ATSC are patented, including elements of the MPEG video coding, the AC-3 audio coding, and the 8VSB modulation.[2] The cost of patent licensing, estimated at up to $50 per digital TV receiver,[3] had prompted complaints by manufacturers.[4]
Companies with patents included : 
LG Electronics, Zenith Electronics, Panasonic, Samsung Electronics, Columbia University, Mitsubishi Electric, JVC Kenwood, Cisco Technology, Inc., Vientos Alisios Co., Ltd., Philips

https://en.wikipedia.org/wiki/ATSC_standards

- **Satelitte : Medium power satellites with smaller dishes - PrimeStar**

In the US in the early 1990s, four large cable companies launched PrimeStar, a direct broadcasting company using medium power satellites. The relatively strong transmissions allowed the use of smaller (90 cm) dishes. Its popularity declined with the 1994 launch of the Hughes DirecTV and Dish Network satellite television systems.
Digital satellite broadcasts began in 1994 in the United States through DirecTV using the DSS format. They were launched (with the DVB-S standard) in South Africa, Middle East, North Africa and Asia-Pacific in 1994 and 1995, and in 1996 and 1997 in European countries including France, Germany, Spain, Portugal, Italy and the Netherlands, as well as Japan, North America and Latin America. Digital DVB-S broadcasts in the United Kingdom and Ireland started in 1998. Japan started broadcasting with the ISDB-S standard in 2000.
On March 4, 1996, EchoStar introduced Digital Sky Highway (Dish Network) using the EchoStar 1 satellite.[80] EchoStar launched a second satellite in September 1996 to increase the number of channels available on Dish Network to 170.[80] These systems provided better pictures and stereo sound on 150–200 video and audio channels, and allowed small dishes to be used. This greatly reduced the popularity of TVRO systems. In the mid-1990s, channels began moving their broadcasts to digital television transmission using the DigiCipher conditional access system.[81]

https://en.wikipedia.org/wiki/Satellite_television

### Video games
**[`^        back to top        ^`](#)**

- **Renaissance of arcade (1990-1996)**
 
Fighting games like Street Fighter II (1991) and Mortal Kombat (1992) helped to revive it in the early 1990s, leading to a renaissance for the arcade industry.[22] 3D graphics were popularized in arcades during the early 1990s with games such as Sega's Virtua Racing and Virtua Fighter,[59] with later arcade systems such as the Sega Model 3 remaining considerably more advanced than home systems through the late 1990s.[60][61] However, the improved capabilities of home consoles and computers to mimic arcade video games during this time drew crowds away from arcades.[22]

https://en.wikipedia.org/wiki/Arcade_game#History

- **SNES - Super Nitendo (1990) - Nintendo - ROM, Cartridge**

The Super Nintendo Entertainment System (SNES),[b] commonly shortened to Super NES or Super Nintendo,[c] is a 16-bit home video game console developed by Nintendo that was released in 1990 in Japan and South Korea,[19] 1991 in North America, 1992 in Europe and Oceania, and 1993 in South America. In Japan, it is called the Super Famicom (SFC).[d] In South Korea, it is called the Super Comboy[e] and was distributed by Hyundai Electronics.[20] The system was released in Brazil on August 30, 1993,[19][21] by Playtronic. Although each version is essentially the same, several forms of regional lockout prevent cartridges for one version from being used in other versions.
The Super NES is Nintendo's second programmable home console, following the Nintendo Entertainment System (NES). The console introduced advanced graphics and sound capabilities compared with other systems at the time. It was designed to accommodate the ongoing development of a variety of enhancement chips integrated into game cartridges to be competitive into the next generation.

https://en.wikipedia.org/wiki/Super_Nintendo_Entertainment_System

- **Text-based multi-player virtual reality (MOO) (1990s)**

A MOO ("MUD, object-oriented"[1][2]) is a text-based online virtual reality system to which multiple users (players) are connected at the same time.

The term MOO is used in two distinct, but related, senses. One is to refer to those programs descended from the original MOO server, and the other is to refer to any MUD that uses object-oriented techniques to organize its database of objects, particularly if it does so in a similar fashion to the original MOO or its derivatives. Most of this article refers to the original MOO and its direct descendants, but see Non-Descendant MOOs for a list of MOO-like systems.

The original MOO server was authored by Stephen White, based on his experience from creating the programmable TinyMUCK system.[3][2] There was additional later development and maintenance from LambdaMOO founder, and former Xerox PARC employee, Pavel Curtis.

One of the most distinguishing features of a MOO is that its users can perform object-oriented programming within the server, ultimately expanding and changing how the server behaves to everyone.[4] Examples of such changes include authoring new rooms and objects, creating new generic objects for others to use, and changing the way the MOO interface operates. The programming language used for extension is the MOO programming language, and many MOOs feature convenient libraries of verbs that can be used by programmers in their coding known as Utilities. The MOO programming language is a domain-specific language.[citation needed]

https://en.wikipedia.org/wiki/MOO

LambdaMOO is an online community[1] of the variety called a MOO. It is the oldest MOO today.[citation needed]

LambdaMOO was founded in 1990 by Pavel Curtis at Xerox PARC.[2][3][4][5] Now hosted in the state of Washington, it is operated and administered entirely on a volunteer basis. Guests are allowed, and membership is free to anyone with an e-mail address.

LambdaMOO gained some notoriety when Julian Dibbell wrote a book called My Tiny Life describing his experiences there.[6] Over its history, LambdaMOO has been highly influential in the examination of virtual-world social issues.[2]

https://en.wikipedia.org/wiki/LambdaMOO

- **Consoles surpasses arcade (1997-1998)**

Up until about 1996, arcade video games had remained the largest sector of the global video game industry, before arcades declined in the late 1990s, with the console market surpassing arcade video games for the first time around 1997–1998.[62

https://en.wikipedia.org/wiki/Arcade_game#History

- **32/64 bits game consoles - Fifth generation video games consoles (1993-2006)**

The fifth-generation era (also known as the 32-bit era, the 64-bit era, or the 3D era) refers to computer and video games, video game consoles, and handheld gaming consoles dating from approximately October 4, 1993 to March 23, 2006.[note 1] For home consoles, the best-selling console was the **Sony PlayStation**, followed by the **Nintendo 64**, and then the **Sega Saturn**. The PlayStation also had a redesigned version, the PSone, which was launched on July 7, 2000.

Some features that distinguished fifth generation consoles from previous fourth generation consoles include:
- 3D polygon graphics with texture mapping
- 3D graphics capabilities – lighting, Gouraud shading, anti-aliasing and texture filtering
- Optical disc (CD-ROM) game storage, allowing much larger storage space (up to 650 MB) than ROM cartridges
- CD quality audio recordings (music and speech) – PCM audio with 16-bit depth and 44.1 kHz sampling rate
- Wide adoption of full motion video, displaying pre-rendered computer animation or live action footage
- Analog controllers
- Display resolutions from 480i/480p to 576i
- Color depth up to 16,777,216 colors (24-bit true color)

https://en.wikipedia.org/wiki/Fifth_generation_of_video_game_consoles

- **Sony PlayStation - First console to ship over 100 million units (1994) - Sony**

PlayStation (Japanese: プレイステーション, Hepburn: Pureisutēshon, officially abbreviated as PS) is a video game brand that consists of five home video game consoles, two handhelds, a media center, and a smartphone, as well as an online service and multiple magazines. The brand is produced by Sony Interactive Entertainment, a division of Sony; the first PlayStation console was released in Japan in December 1994, and worldwide the following year.[1]
The original console in the series was the first console of any type to ship over 100 million units, doing so in under a decade.[2]

https://en.wikipedia.org/wiki/PlayStation

Sony began developing the standalone PlayStation after a failed venture with Nintendo to create a CD-ROM peripheral for the Super Nintendo Entertainment System in the early 1990s. The console was primarily designed by Ken Kutaragi and Sony Computer Entertainment in Japan, while additional development was outsourced in the United Kingdom. An emphasis on 3D polygon graphics was placed at the forefront of the console's design. PlayStation game production was designed to be streamlined and inclusive, enticing the support of many third-party developers.

The console proved popular for its extensive game library, popular franchises, low retail price, and aggressive youth marketing which advertised it as the preferable console for adolescents and adults. Premier PlayStation franchises included Gran Turismo, Crash Bandicoot, Tomb Raider, and Final Fantasy, all of which spawned numerous sequels. PlayStation games continued to sell until Sony ceased production of the PlayStation and its games on 23 March 2006—over eleven years after it had been released, and less than a year before the debut of the PlayStation 3.[8] A total of 3,061 PlayStation games were released, with cumulative sales of 967 million units.

SCE was an upstart in the video game industry in late 1994, as the video game market in the early 1990s was dominated by Nintendo and Sega. Nintendo had been the clear leader in the industry since the introduction of the Nintendo Entertainment System in 1985 and the Nintendo 64 was initially expected to maintain this position. The PlayStation's target audience included the generation which was the first to grow up with mainstream video games, along with 18- to 29-year-olds who were not the primary focus of Nintendo.[202] By the late 1990s, Sony became a highly regarded console brand due to the PlayStation, with a significant lead over second-place Nintendo, while Sega was relegated to a distant third.[203]

The PlayStation became the first "computer entertainment platform" to ship over 100 million units worldwide,[6][204] with many critics attributing the console's success to third-party developers.[76] It remains the fifth best-selling console of all time as of 2022, with a total of 102.49 million units sold.[204] Around 7,900 individual games were published for the console during its 11-year life span, the second-most amount of games ever produced for a console.[6] Its success resulted in a significant financial boon for Sony as profits from its video game division contributed to 23%.[205]

The success of the PlayStation contributed to the demise of cartridge-based home consoles. While not the first system to use an optical disc format, it was the first highly successful one, and ended up going head-to-head with the proprietary cartridge-relying Nintendo 64.[c][207] After the demise of the Sega Saturn, Nintendo was left as Sony's main competitor in Western markets. Nintendo chose not to use CDs for the Nintendo 64; it was likely concerned with the proprietary cartridge format's ability to help enforce copy protection, given its substantial reliance on licensing and exclusive games for its revenue.

https://en.wikipedia.org/wiki/PlayStation_(console)

## Standards and protocols
**[`^        back to top        ^`](#)**

## Some standards and protocols
**[`^        back to top        ^`](#)**

### Physical layer

- **Classic Ethernet (1990-1993)**

  * 802.3i	10BASE-T (1990) : 10 Mbit/s (1.25 MB/s) over twisted pair https://en.wikipedia.org/wiki/IEEE_802.3
  * 10BASEFL (1993) : 10BASE-F specification of Ethernet over optical fiber 
 
https://en.wikipedia.org/wiki/Classic_Ethernet

- **Fast Ethernet (1995-1998)**
  * 802.3u	-	100BASE-TX, 100BASE-T4, 100BASE-FX (1995) : Fast Ethernet at 100 Mbit/s (12.5 MB/s) with autonegotiation 

https://en.wikipedia.org/wiki/Fast_Ethernet

- **Gigabit Ethernet (1998-1999)**
  * 802.3z	- 1000BASE-X Gbit/s (1998): Ethernet over optical fiber at 1 Gbit/s (125 MB/s) 
  * 802.3ab	- 1000BASE-T Gbit/s (1999): Ethernet over twisted pair at 1 Gbit/s (125 MB/s)

https://en.wikipedia.org/wiki/Gigabit_Ethernet

- **Modem speeds : 14.4k to 56k**
  
  * V.32bis 14.4k bps; QAM (1991)
  * V.34 28.8k bps; QAM (1994) AKA V.fast
  * V.34bis 33.6k bps; QAM (1996)
  * V.90 56k bps; Modulus Conversion downstream, QAM upstream (1998)
  * V.92 56k bps; Modulus conversion in both directions (2000)

https://tldp.org/HOWTO/Modem-HOWTO-29.html

### Link layer

- **ATM - Asynchronous Transfer Mode**

Asynchronous Transfer Mode (ATM) is a telecommunications standard defined by American National Standards Institute (ANSI) and ITU-T (formerly CCITT) for digital transmission of multiple types of traffic. ATM was developed to meet the needs of the Broadband Integrated Services Digital Network as defined in the late 1980s,[1] and designed to integrate telecommunication networks. It can handle both traditional high-throughput data traffic and real-time, low-latency content such as telephony (voice) and video.[2][3] ATM provides functionality that uses features of circuit switching and packet switching networks by using asynchronous time-division multiplexing.[4][5]

In the OSI reference model data link layer (layer 2), the basic transfer units are called frames. In ATM these frames are of a fixed length (53 octets) called cells. This differs from approaches such as IP or Ethernet that use variable-sized packets or frames. ATM uses a connection-oriented model in which a virtual circuit must be established between two endpoints before the data exchange begins.[5] These virtual circuits may be either permanent (dedicated connections that are usually preconfigured by the service provider), or switched (set up on a per-call basis using signaling and disconnected when the call is terminated).

The ATM network reference model approximately maps to the three lowest layers of the OSI model: physical layer, data link layer, and network layer.[6] ATM is a core protocol used in the SONET/SDH backbone of the public switched telephone network (PSTN) and in the Integrated Services Digital Network (ISDN) but has largely been superseded in favor of next-generation networks based on Internet Protocol (IP) technology. Wireless and mobile ATM never established a significant foothold.

ATM became popular with telephone companies and many computer makers in the 1990s. However, even by the end of the decade, the better price/performance of Internet Protocol-based products was competing with ATM technology for integrating real-time and bursty network traffic.[17] Companies such as FORE Systems focused on ATM products, while other large vendors such as Cisco Systems provided ATM as an option

https://en.wikipedia.org/wiki/Asynchronous_Transfer_Mode

- **Frame Relay protocols (1993-1998)**

  * RFC 1490 – Multiprotocol Interconnect over Frame Relay (1993) https://datatracker.ietf.org/doc/html/rfc1490
  * RFC 1973 – PPP in Frame Relay (1996) https://datatracker.ietf.org/doc/html/rfc1973
  * RFC 2427 – Multiprotocol Interconnect over Frame Relay (1998) https://datatracker.ietf.org/doc/html/rfc2427

https://en.wikipedia.org/wiki/Frame_Relay

### Network layer

- **IPv6 - Internet Protocol version 6 (1995)**

most recent version of the Internet Protocol (IP), the communications protocol that provides an identification and location system for computers on networks and routes traffic across the Internet. 
Introduced in 1995, IPv6 was developed by the Internet Engineering Task Force (IETF) to deal with the long-anticipated problem of IPv4 address exhaustion, and is intended to replace IPv4.
In December 1998, IPv6 became a Draft Standard for the IETF, which subsequently ratified it as an Internet Standard on 14 July 2017.
Deployment was slowed by the widespread use of NAT (1999).

https://en.wikipedia.org/wiki/IPv6

- **NAT - Network address translation (1999)** : a method of mapping an IP address space into another by modifying network address information in the IP header of packets while they are in transit across a traffic routing device.
Popular with the exhaustion of IPv4 addresses and home routers. Essential to networking. 
https://en.wikipedia.org/wiki/Network_address_translation

### Transport layer

- **SSL - Secure Socket Layer (1995-1998)**

Netscape developed the original SSL protocols, and Taher Elgamal, chief scientist at Netscape Communications from 1995 to 1998. Superseded by TLS.

https://en.wikipedia.org/wiki/Transport_Layer_Security

- **TLS - Transport Layer Security (1999)** 

A cryptographic protocol designed to provide communications security over a computer network. The protocol is widely used 
in applications such as email, instant messaging, and voice over IP, but its use in securing HTTPS remains the most publicly visible.
Essential to networking and the internet.
https://en.wikipedia.org/wiki/Transport_Layer_Security

- **PPTP - Point-to-Point Tunneling Protocol (1999) - Microsoft**

The Point-to-Point Tunneling Protocol (PPTP) is an obsolete method for implementing virtual private networks. PPTP has many well known security issues.

PPTP uses a TCP control channel and a Generic Routing Encapsulation tunnel to encapsulate PPP packets. Many modern VPNs use various forms of UDP for this same functionality.

The PPTP specification does not describe encryption or authentication features and relies on the Point-to-Point Protocol being tunneled to implement any and all security functionalities.

The PPTP implementation that ships with the Microsoft Windows product families implements various levels of authentication and encryption natively as standard features of the Windows PPTP stack. The intended use of this protocol is to provide security levels and remote access levels comparable with typical VPN products.

https://en.wikipedia.org/wiki/Point-to-Point_Tunneling_Protocol

### Session layer

- **SOCKS - SOCKS4 (1992) - SOCKS5 (1996)**

SOCKS is a de facto standard for circuit-level gateways (level 5 gateways).[6]

The circuit/session level nature of SOCKS make it a versatile tool in forwarding any TCP (or UDP since SOCKS5) traffic, creating an interface for all types of routing tools. It can be used as:

A circumvention tool, allowing traffic to bypass Internet filtering to access content otherwise blocked, e.g., by governments, workplaces, schools, and country-specific web services.[7] Since SOCKS is very detectable, a common approach is to present a SOCKS interface for more sophisticated protocols:
The Tor onion proxy software presents a SOCKS interface to its clients.[8]

Providing similar functionality to a virtual private network, allowing connections to be forwarded to a server's "local" network:
Some SSH suites, such as OpenSSH, support dynamic port forwarding that allows the user to create a local SOCKS proxy.[9] This can free the user from the limitations of connecting only to a predefined remote port and server.

The protocol was originally developed/designed by David Koblas, a system administrator of MIPS Computer Systems. After MIPS was taken over by Silicon Graphics in 1992, Koblas presented a paper on SOCKS at that year's Usenix Security Symposium,[2] making SOCKS publicly available.[3] The protocol was extended to version 4 by Ying-Da Lee of NEC.

The SOCKS reference architecture and client are owned by Permeo Technologies,[4] a spin-off from NEC. (Blue Coat Systems bought out Permeo Technologies, and were in turn acquired by Symantec.)

The SOCKS5 protocol was originally a security protocol that made firewalls and other security products easier to administer. It was approved by the IETF in 1996 as RFC 1928 (authored by: M. Leech, M. Ganis, Y. Lee, R. Kuris, D. Koblas, and L. Jones). The protocol was developed in collaboration with Aventail Corporation, which markets the technology outside of Asia.[5]

https://en.wikipedia.org/wiki/SOCKS

### Application layer

- **HTTP/0.9 - Hypertext Transfer Protocol (1991)**

application layer protocol in the Internet protocol suite model for distributed, collaborative, hypermedia information systems. Essential to the internet.

https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol

- **DHCP - Dynamic Host Configuration Protocol (1991)** 

A network management protocol used on Internet Protocol (IP) networks for automatically assigning IP addresses and other communication parameters to devices connected to the network using a client–server architecture.
Essential to networking.

https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol

- **RADIUS (1991)**

Remote Authentication Dial-In User Service (RADIUS) is a networking protocol that provides centralized authentication, authorization, and accounting (AAA) management for users who connect and use a network service. RADIUS was developed by Livingston Enterprises in 1991 as an access server authentication and accounting protocol. It was later brought into IEEE 802 and IETF standards.

RADIUS is a client/server protocol that runs in the application layer, and can use either TCP or UDP. Network access servers, which control access to a network, usually contain a RADIUS client component that communicates with the RADIUS server.[1] RADIUS is often the back-end of choice for 802.1X authentication.[2] A RADIUS server is usually a background process running on UNIX or Microsoft Windows.[1]
As more dial-up customers used the NSFNET a request for proposal was sent out by Merit Network in 1991 to consolidate their various proprietary authentication, authorization and accounting systems. Among the early respondents was Livingston Enterprises and an early version of the RADIUS was written after a meeting. The early RADIUS server was installed on a UNIX operating system. Livingston Enterprises was acquired by Lucent and together with Merit steps were taken to gain industry acceptance for RADIUS as a protocol. Both companies offered a RADIUS server at no charge.[11] In 1997 RADIUS was published as RFC 2058 and RFC 2059, current versions are RFC 2865 and RFC 2866.[12]

https://en.wikipedia.org/wiki/RADIUS

- **LDAP (1993)**

The Lightweight Directory Access Protocol (LDAP /ˈɛldæp/) is an open, vendor-neutral, industry standard application protocol for accessing and maintaining distributed directory information services over an Internet Protocol (IP) network.[1] Directory services play an important role in developing intranet and Internet applications by allowing the sharing of information about users, systems, networks, services, and applications throughout the network.[2] As examples, directory services may provide any organized set of records, often with a hierarchical structure, such as a corporate email directory. Similarly, a telephone directory is a list of subscribers with an address and a phone number.
LDAP is specified in a series of Internet Engineering Task Force (IETF) Standard Track publications called Request for Comments (RFCs), using the description language ASN.1. The latest specification is Version 3, published as RFC 4511[3] (a road map to the technical specifications is provided by RFC4510).
A common use of LDAP is to provide a central place to store usernames and passwords. This allows many different applications and services to connect to the LDAP server to validate users.[4]
LDAP is based on a simpler subset of the standards contained within the X.500 standard. Because of this relationship, LDAP is sometimes called X.500-lite.[5]
The protocol was originally created[7] by Tim Howes of the University of Michigan, Steve Kille of Isode Limited, Colin Robbins of Nexor and Wengyik Yeong of Performance Systems International, circa 1993, as a successor[8] to DIXIE and DAS. Mark Wahl of Critical Angle Inc., Tim Howes, and Steve Kille started work in 1996 on a new version of LDAP, LDAPv3, under the aegis of the Internet Engineering Task Force (IETF). LDAPv3, first published in 1997, superseded LDAPv2 and added support for extensibility, integrated the Simple Authentication and Security Layer, and better aligned the protocol to the 1993 edition of X.500. Further development of the LDAPv3 specifications themselves and of numerous extensions adding features to LDAPv3 has come through the IETF.

https://en.wikipedia.org/wiki/Lightweight_Directory_Access_Protocol

### Data transmission

#### Text

- **HTML (1990-1991) - HyperText Markup Language**

In 1980, physicist Tim Berners-Lee, a contractor at CERN, proposed and prototyped ENQUIRE, a system for CERN researchers to use and share documents. In 1989, Berners-Lee wrote a memo proposing an Internet-based hypertext system.[3] Berners-Lee specified HTML and wrote the browser and server software in late 1990. 
The first publicly available description of HTML was a document called "HTML Tags", first mentioned on the Internet by Tim Berners-Lee in late 1991.

https://en.wikipedia.org/wiki/HTML

- **URL (1992-1994) - Uniform Resource Locators**

Uniform Resource Locators were defined in RFC 1738 in 1994 by Tim Berners-Lee, the inventor of the World Wide Web, and the URI working group of the Internet Engineering Task Force (IETF),[7] as an outcome of collaboration started at the IETF Living Documents birds of a feather session in 1992.[7][8]

https://en.wikipedia.org/wiki/URL

#### Images

- **WMF - Windows Metafile, SVG ancestor (1991) - Microsoft**

Long before the invention of Scalable Vector Graphics, Microsoft Corporation recognized the value of recording images in a format that its applications and operating systems could easily render irrespective of the output device.  With the release of Windows 3.0, Microsoft released its Windows Metafile (WMF) format, which can contain vector and raster graphics in one package.  

https://datatracker.ietf.org/doc/html/rfc7903

- **JPEG (1992)**

Picture compression format.

https://en.wikipedia.org/wiki/JPEG

- **PNG (1997)** 

Portable Network Graphics (PNG, officially pronounced /pɪŋ/[2][3] PING, colloquially pronounced /ˌpiːɛnˈdʒiː/[4] PEE-en-JEE) is a raster-graphics file format that supports lossless data compression.
PNG was published as informational RFC 2083 in March 1997 and as an ISO/IEC 15948 standard in 2004

https://en.wikipedia.org/wiki/Portable_Network_Graphics

#### Music and Video

- **RIFF - Resource Interchange File Format (1991) - Microsoft and IBM**

The Resource Interchange File Format (RIFF) is a generic file container format for storing data in tagged chunks.[2] It is primarily used to store multimedia such as sound and video, though it may also be used to store any arbitrary data.[3]
The Microsoft implementation is mostly known through container formats like AVI, ANI and WAV, which use RIFF as their basis.[4]
RIFF was introduced in 1991 by Microsoft and IBM, and was presented by Microsoft as the default format for Windows 3.1 multimedia files. It is based on Electronic Arts' Interchange File Format, introduced in 1985 on the Commodore Amiga, the only difference being that multi-byte integers are in little-endian format, native to the 80x86 processor series used in IBM PCs, rather than the big-endian format native to the 68k processor series used in Amiga and Apple Macintosh computers, where IFF files were heavily used. A RIFX format, which is big-endian, was also introduced.

https://en.wikipedia.org/wiki/Resource_Interchange_File_Format

- **Quicktime (1991-2018) - Apple**

QuickTime is an extensible multimedia framework developed by Apple Inc., capable of handling various formats of digital video, picture, sound, panoramic images, and interactivity. Created in 1991, the latest Mac version, QuickTime X, is available for Mac OS X Snow Leopard up to macOS Mojave. Apple ceased support for the Windows version of QuickTime in 2016, and ceased support for QuickTime 7 on macOS in 2018

Apple released the first version of QuickTime on December 2, 1991 as a multimedia add-on for System 6 and later. The lead developer of QuickTime, Bruce Leak, ran the first public demonstration at the May 1991 Worldwide Developers Conference, where he played Apple's famous 1984 advertisement in a window at 320×240 pixels resolution.

QuickTime 1.x

The original video codecs included:
  * the Animation codec, which used run-length encoding and was better suited to cartoon-type images with large areas of flat color
  * the Apple Video codec (also known as "Road Pizza"), suited to normal live-action video.[32]
  * the Graphics codec, for 8-bit images, including ones that had undergone dithering
The first commercial project produced using QuickTime 1.0 was the CD-ROM From Alice to Ocean. The first publicly visible use of QuickTime was Ben & Jerry's interactive factory tour (dubbed The Rik & Joe Show after its in-house developers). The Rik and Joe Show was demonstrated onstage at MacWorld in San Francisco when John Sculley announced QuickTime.[33]

Apple released QuickTime 1.5 for Mac OS in the latter part of 1992. This added the SuperMac-developed Cinepak vector-quantization video codec (initially known as Compact Video). It could play video at 320×240 resolution at 30 frames per second on a 25 MHz Motorola 68040 CPU. It also added text tracks, which allowed for captioning, lyrics and other potential uses.

It was followed by several version (up to version 7 and X) in the 2000s and 2010s.

https://en.wikipedia.org/wiki/QuickTime


- **WAV - Uncompressed sound format (1991) - Microsoft and IBM**

Waveform Audio File Format[3] (WAVE,[3] or WAV due to its filename extension;[3][6][7] pronounced "wave"[8]) is an audio file format standard, developed by IBM and Microsoft, for storing an audio bitstream on PCs. It is the main format used on Microsoft Windows systems for uncompressed audio. The usual bitstream encoding is the linear pulse-code modulation (LPCM) format.
WAV is an application of the Resource Interchange File Format (RIFF) bitstream format method for storing data in chunks, and thus is similar to the 8SVX and the AIFF format used on Amiga and Macintosh computers, respectively.
August 1991

https://en.wikipedia.org/wiki/WAV

- **AVI - Audio Video Interleave (1992) - Microsoft**

Audio Video Interleave (also Audio Video Interleaved and known by its initials and filename extension AVI, usually pronounced /ˌeɪ.viːˈaɪ/[3]), is a proprietary multimedia container format and Windows standard[4] introduced by Microsoft in November 1992 as part of its Video for Windows software. AVI files can contain both audio and video data in a file container that allows synchronous audio-with-video playback. Like the DVD video format, AVI files support multiple streaming audio and video, although these features are seldom used.

https://en.wikipedia.org/wiki/Audio_Video_Interleave

- **MP3 (1993)**

MP3 (formally MPEG-1 Audio Layer III or MPEG-2 Audio Layer III)[4] is a coding format for digital audio developed largely by the Fraunhofer Society in Germany, with support from other digital scientists in the United States and elsewhere. Originally defined as the third audio format of the MPEG-1 standard, it was retained and further extended — defining additional bit-rates and support for more audio channels — as the third audio format of the subsequent MPEG-2 standard. A third version, known as MPEG 2.5 — extended to better support lower bit rates — is commonly implemented, but is not a recognized standard.
On 7 July 1994, the Fraunhofer Society released the first software MP3 encoder, called l3enc.[59] The filename extension .mp3 was chosen by the Fraunhofer team on 14 July 1995 (previously, the files had been named .bit).[1] With the first real-time software MP3 player WinPlay3 (released 9 September 1995) many people were able to encode and play back MP3 files on their PCs. Because of the relatively small hard drives of the era (≈500–1000 MB) lossy compression was essential to store multiple albums' worth of music on a home computer as full recordings (as opposed to MIDI notation, or tracker files which combined notation with short recordings of instruments playing single notes).

https://en.wikipedia.org/wiki/MP3

#### File compression

- **DEFLATE (1993-1996)**

In computing, Deflate (stylized as DEFLATE) is a lossless data compression file format that uses a combination of LZ77 and Huffman coding. It was designed by Phil Katz, for version 2 of his PKZIP archiving tool. Deflate was later specified in RFC 1951 (1996).[1]

Katz also designed the original algorithm used to construct Deflate streams. This algorithm was patented as U.S. Patent 5,051,745, and assigned to PKWARE, Inc.[2][3] As stated in the RFC document, an algorithm producing Deflate files was widely thought to be implementable in a manner not covered by patents.[1] This led to its widespread use – for example, in gzip compressed files and PNG image files, in addition to the ZIP file format for which Katz originally designed it. The patent has since expired.

https://en.wikipedia.org/wiki/Deflate

- **Winzip - Popular ZIP Gui on Windows (1991)** 

WinZip is a trialware file archiver and compressor for Windows, macOS, iOS and Android. It is developed by WinZip Computing (formerly Nico Mak Computing), which is owned by Corel Corporation. The program can create archives in Zip file format, unpack some other archive file formats and it also has various tools for system integration.

WinZip 1.0 was released in April 1991 as a Graphical User Interface (GUI) front-end for PKZI.

https://en.wikipedia.org/wiki/WinZip

- **ARJ (1993)**

ARJ (Archived by Robert Jung) is a software tool designed by Robert K. Jung for creating high-efficiency compressed file archives. ARJ is currently on version 2.86 for MS-DOS and 3.20 for Microsoft Windows and supports 16-bit, 32-bit and 64-bit Intel architectures.[1]

ARJ was one of many file compression utilities for MS-DOS and Microsoft Windows during the early and mid-1990s. Parts of ARJ were covered by U.S. Patent 5,140,321 (expired). ARJ is well-documented and includes over 150 command line switches.

https://en.wikipedia.org/wiki/ARJ

- **RAR (1993)**

RAR is a proprietary archive file format that supports data compression, error correction and file spanning.[3] It was developed in 1993 by Russian software engineer Eugene Roshal and the software is licensed by win.rar GmbH.[3] The name RAR stands for Roshal Archive.

https://en.wikipedia.org/wiki/RAR_(file_format)

- **7ZIP - 7z file format, ZIP improvement (1999) - Igor Pavlov**

7-Zip is a free and open-source file archiver, a utility used to place groups of files within compressed containers known as "archives". It is developed by Igor Pavlov and was first released in 1999.[2] 7-Zip has its own archive format called 7z, but can read and write several others.

By default, 7-Zip creates 7z-format archives with a .7z file extension.

In 2011, TopTenReviews found that the 7z compression was at least 17% better than ZIP,[17] and 7-Zip's own site has since 2002 reported that while compression ratio results are very dependent upon the data used for the tests, "Usually, 7-Zip compresses to 7z format 30–70% better than to zip format, and 7-Zip compresses to zip format 2–10% better than most other zip-compatible programs."[18

https://en.wikipedia.org/wiki/7-Zip

#### Voice over IP

- **SIP - Session Initiation Protocol (1996)** 

Signaling protocol used for initiating, maintaining, and terminating communication sessions that include voice, video and messaging applications. 
SIP is used in **Internet telephony**, in private IP telephone systems, as well as mobile phone calling over LTE (**VoLTE**). Essential to telecommunications.

https://en.wikipedia.org/wiki/Session_Initiation_Protocol

#### Video over IP

- **MPEG-1 (1993)**

Since 1972, International Telecommunication Union's radio telecommunications sector (ITU-R) had been working on creating a global recommendation for Analog HDTV. These recommendations, however, did not fit in the broadcasting bands which could reach home users. The standardization of MPEG-1 in 1993 led to the acceptance of recommendations ITU-R BT.709

https://en.wikipedia.org/wiki/High-definition_television

- **MPEG-2 (1995)**

MPEG-2 evolved out of the shortcomings of MPEG-1.
MPEG-1's known weaknesses:
An audio compression system limited to two channels (stereo).
No standardized support for interlaced video with poor compression when used for interlaced video
Only one standardized "profile" (Constrained Parameters Bitstream), which was unsuited for higher resolution video. MPEG-1 could support 4k video but there was no easy way to encode video for higher resolutions, and identify hardware capable of supporting it, as the limitations of such hardware were not defined.
Support for only one chroma subsampling, 4:2:0.
Sakae Okubo of NTT was the ITU-T coordinator for developing the H.262/MPEG-2 Part 2 video coding standard and the requirements chairman in MPEG for the MPEG-2 set of standards.[27] The majority of patents underlying MPEG-2 technology are owned by three companies: Sony (311 patents), Thomson (198 patents) and Mitsubishi Electric (119 patents).[28] Hyundai Electronics (now SK Hynix) developed the first MPEG-2 SAVI (System/Audio/Video) decoder in 1995.[29]

https://en.wikipedia.org/wiki/MPEG-2

- **H.323 (1996)** 

Recommendation from the ITU Telecommunication Standardization Sector (ITU-T) that defines the protocols to provide audio-visual communication sessions on any packet network.[1] The H.323 standard addresses call signaling and control, multimedia transport and control, and bandwidth control for point-to-point and multi-point conferences.[2]
It is widely implemented[3] by **voice and videoconferencing equipment manufacturers**, is used within various **Internet real-time applications** such as GnuGK and NetMeeting and is widely deployed worldwide by service providers and enterprises for both voice and video services over IP networks.

https://en.wikipedia.org/wiki/H.323

- **H.263 (1996)**

The H.263 standard was first designed to be utilized in H.324 based systems (PSTN and other **circuit-switched network videoconferencing and videotelephony)**, but it also found use in H.323 (RTP/IP-based videoconferencing), H.320 (ISDN-based videoconferencing, where it became the most widely used video compression standard),[4] RTSP (streaming media) and SIP (IP-based videoconferencing) solutions.

https://en.wikipedia.org/wiki/H.263

#### Instant messaging

- **XMPP (1999)**
 
Open communication protocol designed for **instant messaging (IM), presence information, and contact list maintenance**. enables the near-real-time exchange of structured data between two or more network entities. Widely in use.

https://en.wikipedia.org/wiki/XMPP

#### Machine-to-machine

- **CORBA - Common Object Request Broker Architecture (1991)**

The Common Object Request Broker Architecture (CORBA) is a standard defined by the Object Management Group (OMG) designed to facilitate the communication of systems that are deployed on diverse platforms. CORBA enables collaboration between systems on different operating systems, programming languages, and computing hardware. CORBA uses an object-oriented model although the systems that use the CORBA do not have to be object-oriented. CORBA is an example of the distributed object paradigm.

CORBA enables communication between software written in different languages and running on different computers. Implementation details from specific operating systems, programming languages, and hardware platforms are all removed from the responsibility of developers who use CORBA. CORBA normalizes the method-call semantics between application objects residing either in the same address-space (application) or in remote address-spaces (same host, or remote host on a network). Version 1.0 was released in October 1991.

https://en.wikipedia.org/wiki/Common_Object_Request_Broker_Architecture

- **XML (1998) - Extensible Markup Language**

A markup language and file format for storing, transmitting, and reconstructing arbitrary data. It defines a set of rules for encoding documents in a format that is both human-readable and machine-readable. The World Wide Web Consortium's XML 1.0 Specification[2] of 1998[3] and several other related specifications[4]—all of them free open standards—define XML.[5]

https://en.wikipedia.org/wiki/XML

- **MQTT (1999)**

Lightweight, **publish-subscribe, machine to machine network protocol**. It is designed for connections with remote locations that have devices with resource constraints or limited network bandwidth. Widely in use.

https://en.wikipedia.org/wiki/MQTT

#### Files

- **CIFS (1996) - Microsoft**

In 1996, Microsoft published a version of SMB 1.0[4] with minor modifications under the Common Internet File System (CIFS /sɪfs/) moniker. CIFS was compatible with even the earliest incarnation of SMB, including LAN Manager's.[4] It supports symbolic links, hard links, and larger file size, but none of the features of SMB 2.0 and later.[4][5] Microsoft's proposal, however, remained an Internet Draft and never achieved standard status.[6] Microsoft has since discontinued use of the CIFS moniker but continues developing SMB and making subsequent specifications publicly available.

https://en.wikipedia.org/wiki/Server_Message_Block

- **Rsync (1999)**

Utility for **efficiently transferring and synchronizing files** between a computer and a storage drive and across networked computers by comparing the modification times and sizes of files. Detailed in the creator's PhD Thesis.

https://en.wikipedia.org/wiki/Rsync

#### Remote access
- **SSH - Secure Shell (1995): remote login and command-line execution**

The Secure Shell Protocol (SSH) is a cryptographic network protocol for operating network services securely over an unsecured network.[1] Its most notable applications are remote login and command-line execution.
SSH applications are based on a client–server architecture, connecting an SSH client instance with an SSH server
SSH was first designed in 1995 by Finnish computer scientist Tatu Ylönen. Subsequent development of the protocol suite proceeded in several developer groups, producing several variants of implementation. The protocol specification distinguishes two major versions, referred to as SSH-1 and SSH-2. The most commonly implemented software stack is **OpenSSH**, released in **1999** as open-source software by the OpenBSD developers. 

https://en.wikipedia.org/wiki/Secure_Shell

- **VNC - Virtual Network Computing (1999)** 

**Remote desktop protocol**. Still widely used. Insecure (not encrypted by default).

https://en.wikipedia.org/wiki/Virtual_Network_Computing

## Programming languages and frameworks
**[`^        back to top        ^`](#)**

- **Haskell - lazy evaluation functional language (1990)**

Haskell (/ˈhæskəl/[25]) is a general-purpose, statically-typed, purely functional programming language with type inference and lazy evaluation.[26][27] Designed for teaching, research and industrial applications, Haskell has pioneered a number of programming language features such as type classes, which enable type-safe operator overloading. Haskell's main implementation is the Glasgow Haskell Compiler (GHC). It is named after logician Haskell Curry.[1]

Haskell's semantics are historically based on those of the Miranda programming language, which served to focus the efforts of the initial Haskell working group.[28] The last formal specification of the language was made in July 2010, while the development of GHC continues to expand Haskell via language extensions.

Haskell is used in academia and industry.[29][30][31] As of May 2021, Haskell was the 28th most popular programming language by Google searches for tutorials,[32] and made up less than 1% of active users on the GitHub source code repository.[33]

Following the release of Miranda by Research Software Ltd. in 1985, interest in lazy functional languages grew. By 1987, more than a dozen non-strict, purely functional programming languages existed. Miranda was the most widely used, but it was proprietary software. At the conference on Functional Programming Languages and Computer Architecture (FPCA '87) in Portland, Oregon, there was a strong consensus that a committee be formed to define an open standard for such languages. The committee's purpose was to consolidate existing functional languages into a common one to serve as a basis for future research in functional-language design.[34]

Haskell 1.0 to 1.4
Type classes, which enable type-safe operator overloading, were first proposed by Philip Wadler and Stephen Blott for Standard ML but were first implemented in Haskell between 1987 and version 1.0.[35][36]

The first version of Haskell ("Haskell 1.0") was defined in 1990.[1] The committee's efforts resulted in a series of language definitions (1.0, 1.1, 1.2, 1.3, 1.4).

https://en.wikipedia.org/wiki/Haskell

- **OpenGL - rendering 2D and 3D vector graphics (1991) - Silicon Graphics**

OpenGL (Open Graphics Library[3]) is a cross-language, cross-platform application programming interface (API) for rendering 2D and 3D vector graphics. The API is typically used to interact with a graphics processing unit (GPU), to achieve hardware-accelerated rendering.

Silicon Graphics, Inc. (SGI) began developing OpenGL in 1991 and released it on June 30, 1992;[4][5] applications use it extensively in the fields of computer-aided design (CAD), virtual reality, scientific visualization, information visualization, flight simulation, and video games. Since 2006, OpenGL has been managed by the non-profit technology consortium Khronos Group.[6]

https://en.wikipedia.org/wiki/OpenGL

- **Visual Basic - event-driven programming language for Microsoft, VBA for MS Office (1991)**

The original Visual Basic (also referred to as Classic Visual Basic)[1] is a third-generation event-driven programming language from Microsoft known for its Component Object Model (COM) programming model first released in 1991 and declared legacy during 2008. Microsoft intended Visual Basic to be relatively easy to learn and use.[2][3] Visual Basic was derived from BASIC and enables the rapid application development (RAD) of graphical user interface (GUI) applications, access to databases using Data Access Objects, Remote Data Objects, or ActiveX Data Objects, and creation of ActiveX controls and objects.

A programmer can create an application using the components provided by the Visual Basic program itself. Over time the community of programmers developed third-party components.[4][5][6][7][8] Programs written in Visual Basic can also make use of the Windows API, which requires external functions declarations.

The final release was version 6 in 1998. On April 8, 2008, Microsoft stopped supporting Visual Basic 6.0 IDE. The Microsoft Visual Basic team still maintains compatibility for Visual Basic 6.0 applications through its "It Just Works" program on supported Windows operating systems.[9]

In 2014, some software developers still preferred Visual Basic 6.0 over its successor, Visual Basic .NET.[4] Visual Basic 6.0 was selected as the most dreaded programming language by respondents of Stack Overflow's annual developer survey in 2016, 2017, and 2018.[10][11][12]

A dialect of Visual Basic, Visual Basic for Applications (VBA), is used as a macro or scripting language within several Microsoft and ISV applications, including Microsoft Office.[13]

Visual Basic 1.0 was introduced in 1991. The drag and drop design for creating the user interface is derived from a prototype form generator developed by Alan Cooper and his company called Tripod.[21][22][23] Microsoft contracted with Cooper and his associates to develop Tripod into a programmable form system for Windows 3.0, under the code name Ruby (no relation to the later Ruby programming language). Tripod did not include a programming language at all. Microsoft decided to combine Ruby with the Basic language to create Visual Basic. The Ruby interface generator provided the "visual" part of Visual Basic, and this was combined with the "EB" Embedded BASIC engine designed for Microsoft's abandoned "Omega" database system. Ruby also provided the ability to load dynamic link libraries containing additional controls (then called "gizmos"), which later became the VBX interface.[24]

All versions of the Visual Basic development environment from 1.0 to 6.0 were retired by Microsoft by 2008, and are therefore **no longer supported**. 

https://en.wikipedia.org/wiki/Visual_Basic_(classic)

- **Python - interpreted multi-paradigm dynamically-typed high-level programming language (1991)**

Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.[32]

Python is dynamically-typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a "batteries included" language due to its comprehensive standard library.[33][34]

Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0.[35] Python 2.0 was released in 2000 and introduced new features such as list comprehensions, cycle-detecting garbage collection, reference counting, and Unicode support. Python 3.0, released in 2008, was a major revision that is not completely backward-compatible with earlier versions. Python 2 was discontinued with version 2.7.18 in 2020.[36]

Python consistently ranks as one of the most popular programming languages.[37][38][39][40]

Python was conceived in the late 1980s[41] by Guido van Rossum at Centrum Wiskunde & Informatica (CWI) in the Netherlands as a successor to the ABC programming language, which was inspired by SETL,[42] capable of exception handling and interfacing with the Amoeba operating system.[13] Its implementation began in December 1989.[43] Van Rossum shouldered sole responsibility for the project, as the lead developer, until 12 July 2018, when he announced his "permanent vacation" from his responsibilities as Python's "benevolent dictator for life", a title the Python community bestowed upon him to reflect his long-term commitment as the project's chief decision-maker.[44] In January 2019, active Python core developers elected a five-member Steering Council to lead the project.[45][46]

Python 2.0 was released on 16 October 2000, with many major new features.[47] Python 3.0, released on 3 December 2008, with many of its major features backported to Python 2.6.x[48] and 2.7.x. Releases of Python 3 include the 2to3 utility, which automates the translation of Python 2 code to Python 3.[49]

Python 2.7's end-of-life was initially set for 2015, then postponed to 2020 out of concern that a large body of existing code could not easily be forward-ported to Python 3.[50][51] No further security patches or other improvements will be released for it.[52][53] With Python 2's end-of-life, only Python 3.6.x[54] and later were supported. Later, support for 3.6 was also discontinued. In 2021, Python 3.9.2 and 3.8.8 were expedited[55] as all versions of Python (including 2.7[56]) had security issues leading to possible remote code execution[57] and web cache poisoning.[58]

In 2022, Python 3.10.4 and 3.9.12 were expedited[59] and so were older releases including 3.8.13, and 3.7.13 because of many security issues.[60] Python 3.9.13 is the latest 3.9 version, and from now on 3.9 (and older; 3.8 and 3.7) will only get security updates.[61]

Python is a multi-paradigm programming language. Object-oriented programming and structured programming are fully supported, and many of its features support functional programming and aspect-oriented programming (including metaprogramming[62] and metaobjects [magic methods] ).[63] Many other paradigms are supported via extensions, including design by contract[64][65] and logic programming.[66]

Python uses dynamic typing and a combination of reference counting and a cycle-detecting garbage collector for memory management.[67] It uses dynamic name resolution (late binding), which binds method and variable names during program execution.

Its design offers some support for functional programming in the Lisp tradition. It has filter,mapandreduce functions; list comprehensions, dictionaries, sets, and generator expressions.[68] The standard library has two modules (itertools and functools) that implement functional tools borrowed from Haskell and Standard ML.[69]

- **R - programming language for statistical computing and graphics (1993)**

R is a programming language for statistical computing and graphics supported by the R Core Team and the R Foundation for Statistical Computing. Created by statisticians Ross Ihaka and Robert Gentleman, R is used among data miners, bioinformaticians and statisticians for data analysis and developing statistical software.[6] Users have created packages to augment the functions of the R language.

According to user surveys and studies of scholarly literature databases, R is one of the most commonly used programming languages used in data mining.[7] As of March 2022, R ranks 11th in the TIOBE index, a measure of programming language popularity, in which the language peaked in 8th place in August 2020.[8][9]

The official R software environment is an open-source free software environment within the GNU package, available under the GNU General Public License. It is written primarily in C, Fortran, and R itself (partially self-hosting). Precompiled executables are provided for various operating systems. R has a command line interface.[10] Multiple third-party graphical user interfaces are also available, such as RStudio, an integrated development environment, and Jupyter, a notebook interface.

R is an open-source implementation of the S programming language combined with lexical scoping semantics from Scheme, which allow objects to be defined in predetermined blocks rather than the entirety of the code.[1] S was created by Rick Becker, John Chambers, Doug Dunn, Jean McRae, and Judy Schilling at Bell Labs around 1976. Designed for statistical analysis, the language is an interpreted language whose code could be directly run without a compiler.[11] Many programs written for S run unaltered in R.[10] As a dialect of the Lisp language, Scheme was created by Gerald J. Sussman and Guy L. Steele Jr. at MIT around 1975.[12]

In 1991, statisticians Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand, embarked on an S implementation.[13] It was named partly after the first names of the first two R authors and partly as a play on the name of S.[10] They began publicizing it on the data archive StatLib and the s-news mailing list in August 1993.[14] In 1995, statistician Martin Mächler convinced Ihaka and Gentleman to make R free and open-source software under the GNU General Public License.[14][15][16] The first official release came in June 1995.[14] The first official "stable beta" version (v1.0) was released on 29 February 2000.[17][18]

The Comprehensive R Archive Network (CRAN) was officially announced on 23 April 1997. CRAN stores R's executable files, source code, documentations, as well as packages contributed by users. CRAN originally had 3 mirrors and 12 contributed packages.[19] As of January 2022, it has 101 mirrors[20] and 18,728 contributed packages.[21] In addition to hosting packages CRAN hosts binaries for major distributions of Linux, MacOS and Windows.[22]

The R Core Team was formed in 1997 to further develop the language.[10][23] As of January 2022, it consists of Chambers, Gentleman, Ihaka, and Mächler, plus statisticians Douglas Bates, Peter Dalgaard, Kurt Hornik, Michael Lawrence, Friedrich Leisch, Uwe Ligges, Thomas Lumley, Sebastian Meyer, Paul Murrell, Martyn Plummer, Brian Ripley, Deepayan Sarkar, Duncan Temple Lang, Luke Tierney, and Simon Urbanek, as well as computer scientist Tomas Kalibera. Stefano Iacus, Guido Masarotto, Heiner Schwarte, Seth Falcon, Martin Morgan, and Duncan Murdoch were members.[14][24] In April 2003,[25] the R Foundation was founded as a non-profit organization to provide further support for the R project.[10]

https://en.wikipedia.org/wiki/R_(programming_language)

- **Lua - language for extending software applications (1993)**

Lua (/ˈluːə/ LOO-ə; from Portuguese: lua [ˈlu.(w)ɐ] meaning moon) is a lightweight, high-level, multi-paradigm programming language designed primarily for embedded use in applications.[3] Lua is cross-platform, since the interpreter of compiled bytecode is written in ANSI C,[4] and Lua has a relatively simple C API to embed it into applications.[5]

Lua originated in 1993 as a language for extending software applications to meet the increasing demand for customization at the time. It provided the basic facilities of most procedural programming languages, but more complicated or domain-specific features were not included; rather, it included mechanisms for extending the language, allowing programmers to implement such features. As Lua was intended to be a general embeddable extension language, the designers of Lua focused on improving its speed, portability, extensibility, and ease-of-use in development.

ua was created in 1993 by Roberto Ierusalimschy, Luiz Henrique de Figueiredo, and Waldemar Celes, members of the Computer Graphics Technology Group (Tecgraf) at the Pontifical Catholic University of Rio de Janeiro, in Brazil.

From 1977 until 1992, Brazil had a policy of strong trade barriers (called a market reserve) for computer hardware and software. In that atmosphere, Tecgraf's clients could not afford, either politically or financially, to buy customized software from abroad. Those reasons led Tecgraf to implement the basic tools it needed from scratch.[6]

Lua's predecessors were the data-description/configuration languages SOL (Simple Object Language) and DEL (data-entry language).[7] They had been independently developed at Tecgraf in 1992–1993 to add some flexibility into two different projects (both were interactive graphical programs for engineering applications at Petrobras company). There was a lack of any flow-control structures in SOL and DEL, and Petrobras felt a growing need to add full programming power to them.

In The Evolution of Lua, the language's authors wrote:[6]

In 1993, the only real contender was Tcl, which had been explicitly designed to be embedded into applications. However, Tcl had unfamiliar syntax, did not offer good support for data description, and ran only on Unix platforms. We did not consider LISP or Scheme because of their unfriendly syntax. Python was still in its infancy. In the free, do-it-yourself atmosphere that then reigned in Tecgraf, it was quite natural that we should try to develop our own scripting language ... Because many potential users of the language were not professional programmers, the language should avoid cryptic syntax and semantics. The implementation of the new language should be highly portable, because Tecgraf's clients had a very diverse collection of computer platforms. Finally, since we expected that other Tecgraf products would also need to embed a scripting language, the new language should follow the example of SOL and be provided as a library with a C API.

Lua 1.0 was designed in such a way that its object constructors, being then slightly different from the current light and flexible style, incorporated the data-description syntax of SOL (hence the name Lua: Sol meaning "Sun" in Portuguese, and Lua meaning "Moon"). Lua syntax for control structures was mostly borrowed from Modula (if, while, repeat/until), but also had taken influence from CLU (multiple assignments and multiple returns from function calls, as a simpler alternative to reference parameters or explicit pointers), C++ ("neat idea of allowing a local variable to be declared only where we need it"[6]), SNOBOL and AWK (associative arrays). In an article published in Dr. Dobb's Journal, Lua's creators also state that LISP and Scheme with their single, ubiquitous data-structure mechanism (the list) were a major influence on their decision to develop the table as the primary data structure of Lua.[8]

Lua semantics have been increasingly influenced by Scheme over time,[6] especially with the introduction of anonymous functions and full lexical scoping. Several features were added in new Lua versions.

Versions of Lua prior to version 5.0 were released under a license similar to the BSD license. From version 5.0 onwards, Lua has been licensed under the MIT License. Both are permissive free software licences and are almost identical.

Features

Lua is commonly described as a "multi-paradigm" language, providing a small set of general features that can be extended to fit different problem types. Lua does not contain explicit support for inheritance, but allows it to be implemented with metatables. Similarly, Lua allows programmers to implement namespaces, classes, and other related features using its single table implementation; first-class functions allow the employment of many techniques from functional programming; and full lexical scoping allows fine-grained information hiding to enforce the principle of least privilege.

In general, Lua strives to provide simple, flexible meta-features that can be extended as needed, rather than supply a feature-set specific to one programming paradigm. As a result, the base language is light—the full reference interpreter is only about 247 kB compiled[4]—and easily adaptable to a broad range of applications.

A dynamically typed language intended for use as an extension language or scripting language, Lua is compact enough to fit on a variety of host platforms. It supports only a small number of atomic data structures such as boolean values, numbers (double-precision floating point and 64-bit integers by default), and strings. Typical data structures such as arrays, sets, lists, and records can be represented using Lua's single native data structure, the table, which is essentially a heterogeneous associative array.

Lua implements a small set of advanced features such as first-class functions, garbage collection, closures, proper tail calls, coercion (automatic conversion between string and number values at run time), coroutines (cooperative multitasking) and dynamic module loading.

https://en.wikipedia.org/wiki/Lua_(programming_language)

- **Java - client-server applications, class-based object-oriented language, use of virtual machine (1995)**

Java is a high-level, class-based, object-oriented programming language that is designed to have as few implementation dependencies as possible. It is a general-purpose programming language intended to let programmers write once, run anywhere (WORA),[17] meaning that compiled Java code can run on all platforms that support Java without the need to recompile.[18] Java applications are typically compiled to bytecode that can run on any Java virtual machine (JVM) regardless of the underlying computer architecture. The syntax of Java is similar to C and C++, but has fewer low-level facilities than either of them. The Java runtime provides dynamic capabilities (such as reflection and runtime code modification) that are typically not available in traditional compiled languages. As of 2019, Java was one of the most popular programming languages in use according to GitHub,[19][20] particularly for client–server web applications, with a reported 9 million developers.[21]

Java was originally developed by James Gosling at Sun Microsystems. It was released in May 1995 as a core component of Sun Microsystems' Java platform. The original and reference implementation Java compilers, virtual machines, and class libraries were originally released by Sun under proprietary licenses. As of May 2007, in compliance with the specifications of the Java Community Process, Sun had relicensed most of its Java technologies under the GPL-2.0-only license. Oracle offers its own HotSpot Java Virtual Machine, however the official reference implementation is the OpenJDK JVM which is free open-source software and used by most developers and is the default JVM for almost all Linux distributions.

As of March 2022, Java 18 is the latest version, while Java 17, 11 and 8 are the current long-term support (LTS) versions. Oracle released the last zero-cost public update for the legacy version Java 8 LTS in January 2019 for commercial use, although it will otherwise still support Java 8 with public updates for personal use indefinitely. Other vendors have begun to offer zero-cost builds of OpenJDK 18 and 8, 11 and 17 that are still receiving security and other upgrades.

Oracle (and others) highly recommend uninstalling outdated and unsupported versions of Java, due to unresolved security issues in older versions.[22] Oracle advises its users to immediately transition to a supported version, such as one of the LTS versions (8, 11, 17).

James Gosling, Mike Sheridan, and Patrick Naughton initiated the Java language project in June 1991.[23] Java was originally designed for interactive television, but it was too advanced for the digital cable television industry at the time.[24] The language was initially called Oak after an oak tree that stood outside Gosling's office. Later the project went by the name Green and was finally renamed Java, from Java coffee, a type of coffee from Indonesia.[25] Gosling designed Java with a C/C++-style syntax that system and application programmers would find familiar.[26]

Sun Microsystems released the first public implementation as Java 1.0 in 1996.[27] It promised write once, run anywhere (WORA) functionality, providing no-cost run-times on popular platforms. Fairly secure and featuring configurable security, it allowed network- and file-access restrictions. Major web browsers soon incorporated the ability to run Java applets within web pages, and Java quickly became popular. The Java 1.0 compiler was re-written in Java by Arthur van Hoff to comply strictly with the Java 1.0 language specification.[28] With the advent of Java 2 (released initially as J2SE 1.2 in December 1998 – 1999), new versions had multiple configurations built for different types of platforms. J2EE included technologies and APIs for enterprise applications typically run in server environments, while J2ME featured APIs optimized for mobile applications. The desktop version was renamed J2SE. In 2006, for marketing purposes, Sun renamed new J2 versions as Java EE, Java ME, and Java SE, respectively.

In 1997, Sun Microsystems approached the ISO/IEC JTC 1 standards body and later the Ecma International to formalize Java, but it soon withdrew from the process.[29][30][31] Java remains a de facto standard, controlled through the Java Community Process.[32] At one time, Sun made most of its Java implementations available without charge, despite their proprietary software status. Sun generated revenue from Java through the selling of licenses for specialized products such as the Java Enterprise System.

On November 13, 2006, Sun released much of its Java virtual machine (JVM) as free and open-source software (FOSS), under the terms of the GPL-2.0-only license. On May 8, 2007, Sun finished the process, making all of its JVM's core code available under free software/open-source distribution terms, aside from a small portion of code to which Sun did not hold the copyright.[33]

Sun's vice-president Rich Green said that Sun's ideal role with regard to Java was as an evangelist.[34] Following Oracle Corporation's acquisition of Sun Microsystems in 2009–10, Oracle has described itself as the steward of Java technology with a relentless commitment to fostering a community of participation and transparency.[35] This did not prevent Oracle from filing a lawsuit against Google shortly after that for using Java inside the Android SDK (see the Android section).

On April 2, 2010, James Gosling resigned from Oracle.[36]

In January 2016, Oracle announced that Java run-time environments based on JDK 9 will discontinue the browser plugin.[37]

Java software runs on everything from laptops to data centers, game consoles to scientific supercomputers.[38]

Criticisms directed at Java include the implementation of generics,[66] speed,[67] the handling of unsigned numbers,[68] the implementation of floating-point arithmetic,[69] and a history of security vulnerabilities in the primary Java VM implementation HotSpot.[70]

The use of Java-related technology in Android led to a legal dispute between Oracle and Google. On May 7, 2012, a San Francisco jury found that if APIs could be copyrighted, then Google had infringed Oracle's copyrights by the use of Java in Android devices.[79] District Judge William Alsup ruled on May 31, 2012, that APIs cannot be copyrighted,[80] but this was reversed by the United States Court of Appeals for the Federal Circuit in May 2014.[81] On May 26, 2016, the district court decided in favor of Google, ruling the copyright infringement of the Java API in Android constitutes fair use.[82] In March 2018, this ruling was overturned by the Appeals Court, which sent down the case of determining the damages to federal court in San Francisco.[83] Google filed a petition for writ of certiorari with the Supreme Court of the United States in January 2019 to challenge the two rulings that were made by the Appeals Court in Oracle's favor.[84] On April 5, 2021, the Court ruled 6-2 in Google's favor, that its use of Java APIs should be considered fair use. However, the court refused to rule on the copyrightability of APIs, choosing instead to determine their ruling by considering Java's API copyrightable "purely for argument’s sake."[85]

https://en.wikipedia.org/wiki/Java_(programming_language)

- **PHP - scripting language powering the web (1995)**

PHP is a general-purpose scripting language geared toward web development.[5] It was originally created by Danish-Canadian programmer Rasmus Lerdorf in 1994.[6] The PHP reference implementation is now produced by The PHP Group.[7] PHP originally stood for Personal Home Page,[6] but it now stands for the recursive initialism PHP: Hypertext Preprocessor.[8]

PHP code is usually processed on a web server by a PHP interpreter implemented as a module, a daemon or as a Common Gateway Interface (CGI) executable. On a web server, the result of the interpreted and executed PHP code – which may be any type of data, such as generated HTML or binary image data – would form the whole or part of an HTTP response. Various web template systems, web content management systems, and web frameworks exist which can be employed to orchestrate or facilitate the generation of that response. Additionally, PHP can be used for many programming tasks outside the web context, such as standalone graphical applications[9] and robotic drone control.[10] PHP code can also be directly executed from the command line.

The standard PHP interpreter, powered by the Zend Engine, is free software released under the PHP License. PHP has been widely ported and can be deployed on most web servers on a variety of operating systems and platforms.[11]

The PHP language evolved without a written formal specification or standard until 2014, with the original implementation acting as the de facto standard which other implementations aimed to follow. Since 2014, work has gone on to create a formal PHP specification.[12]

W3Techs reports that, as of January 2022, "PHP is used by 78.1% of all the websites whose server-side programming language we know."[13] PHP version 7.4 is the most used version. Support for version 7.3 was dropped on 6 December 2021.

PHP development began in 1994 when Rasmus Lerdorf wrote several Common Gateway Interface (CGI) programs in C,[14][15] which he used to maintain his personal homepage. He extended them to work with web forms and to communicate with databases, and called this implementation "Personal Home Page/Forms Interpreter" or PHP/FI.

PHP/FI could be used to build simple, dynamic web applications. To accelerate bug reporting and improve the code, Lerdorf initially announced the release of PHP/FI as "Personal Home Page Tools (PHP Tools) version 1.0" on the Usenet discussion group comp.infosystems.www.authoring.cgi on June 8, 1995.[1][16] This release already had the basic functionality that PHP has today. This included Perl-like variables, form handling, and the ability to embed HTML. The syntax resembled that of Perl, but was simpler, more limited and less consistent.[7]

Early PHP was not intended to be a new programming language, and grew organically, with Lerdorf noting in retrospect: "I don't know how to stop it, there was never any intent to write a programming language [...] I have absolutely no idea how to write a programming language, I just kept adding the next logical step on the way."[18] A development team began to form and, after months of work and beta testing, officially released PHP/FI 2 in November 1997.

The fact that PHP was not originally designed, but instead was developed organically has led to inconsistent naming of functions and inconsistent ordering of their parameters.[19] In some cases, the function names were chosen to match the lower-level libraries which PHP was "wrapping",[20] while in some very early versions of PHP the length of the function names was used internally as a hash function, so names were chosen to improve the distribution of hash values.[21]

https://en.wikipedia.org/wiki/PHP



- **Ruby - everything is an object (1995)**

Ruby is an interpreted, high-level, general-purpose programming language which supports multiple programming paradigms. It was designed with an emphasis on programming productivity and simplicity. In Ruby, everything is an object, including primitive data types. It was developed in the mid-1990s by Yukihiro "Matz" Matsumoto in Japan.

Ruby is dynamically typed and uses garbage collection and just-in-time compilation. It supports multiple programming paradigms, including procedural, object-oriented, and functional programming. According to the creator, Ruby was influenced by Perl, Smalltalk, Eiffel, Ada, BASIC, and Lisp.[11][3]

Matsumoto has said that Ruby was conceived in 1993. In a 1999 post to the ruby-talk mailing list, he describes some of his early ideas about the language:[12]

I was talking with my colleague about the possibility of an object-oriented scripting language. I knew Perl (Perl4, not Perl5), but I didn't like it really, because it had the smell of a toy language (it still has). The object-oriented language seemed very promising. I knew Python then. But I didn't like it, because I didn't think it was a true object-oriented language – OO features appeared to be add-on to the language. As a language maniac and OO fan for 15 years, I really wanted a genuine object-oriented, easy-to-use scripting language. I looked for but couldn't find one. So I decided to make it.

Matsumoto describes the design of Ruby as being like a simple Lisp language at its core, with an object system like that of Smalltalk, blocks inspired by higher-order functions, and practical utility like that of Perl.[13]

The name "Ruby" originated during an online chat session between Matsumoto and Keiju Ishitsuka on February 24, 1993, before any code had been written for the language.[14] Initially two names were proposed: "Coral" and "Ruby". Matsumoto chose the latter in a later e-mail to Ishitsuka.[15] Matsumoto later noted a factor in choosing the name "Ruby" – it was the birthstone of one of his colleagues.[16][17]

Early releases
The first public release of Ruby 0.95 was announced on Japanese domestic newsgroups on December 21, 1995.[18][19] Subsequently, three more versions of Ruby were released in two days.[14] The release coincided with the launch of the Japanese-language ruby-list mailing list, which was the first mailing list for the new language.

Already present at this stage of development were many of the features familiar in later releases of Ruby, including object-oriented design, classes with inheritance, mixins, iterators, closures, exception handling and garbage collection.[20]

https://en.wikipedia.org/wiki/Ruby_(programming_language)

- **Javascript - from web-browser scripting language to full stack (1995)**

JavaScript (/ˈdʒɑːvəskrɪpt/),[10] often abbreviated JS, is a programming language that is one of the core technologies of the World Wide Web, alongside HTML and CSS.[11] As of 2022, 98% of websites use JavaScript on the client side for webpage behavior,[12] often incorporating third-party libraries.[13] All major web browsers have a dedicated JavaScript engine to execute the code on users' devices.

JavaScript is a high-level, often just-in-time compiled language that conforms to the ECMAScript standard.[14] It has dynamic typing, prototype-based object-orientation, and first-class functions. It is multi-paradigm, supporting event-driven, functional, and imperative programming styles. It has application programming interfaces (APIs) for working with text, dates, regular expressions, standard data structures, and the Document Object Model (DOM).

The ECMAScript standard does not include any input/output (I/O), such as networking, storage, or graphics facilities. In practice, the web browser or other runtime system provides JavaScript APIs for I/O.

JavaScript engines were originally used only in web browsers, but are now core components of some servers and a variety of applications. The most popular runtime system for this usage is Node.js.

Although Java and JavaScript are similar in name, syntax, and respective standard libraries, the two languages are distinct and differ greatly in design.

Creation at Netscape
The first web browser with a graphical user interface, Mosaic, was released in 1993. Accessible to non-technical people, it played a prominent role in the rapid growth of the nascent World Wide Web.[15] The lead developers of Mosaic then founded the Netscape corporation, which released a more polished browser, Netscape Navigator, in 1994. This quickly became the most-used.[16][17]

During these formative years of the Web, web pages could only be static, lacking the capability for dynamic behavior after the page was loaded in the browser. There was a desire in the flourishing web development scene to remove this limitation, so in 1995, Netscape decided to add a scripting language to Navigator. They pursued two routes to achieve this: collaborating with Sun Microsystems to embed the Java programming language, while also hiring Brendan Eich to embed the Scheme language.[6]

Netscape management soon decided that the best option was for Eich to devise a new language, with syntax similar to Java and less like Scheme or other extant scripting languages.[5][6] Although the new language and its interpreter implementation were called LiveScript when first shipped as part of a Navigator beta in September 1995, the name was changed to JavaScript for the official release in December.[6][1][18]

The choice of the JavaScript name has caused confusion, implying that it is directly related to Java. At the time, the dot-com boom had begun and Java was the hot new language, so Eich considered the JavaScript name a marketing ploy by Netscape.[19]

Adoption by Microsoft
Microsoft debuted Internet Explorer in 1995, leading to a browser war with Netscape. On the JavaScript front, Microsoft reverse-engineered the Navigator interpreter to create its own, called JScript.[20]

JScript was first released in 1996, alongside initial support for CSS and extensions to HTML. Each of these implementations was noticeably different from their counterparts in Navigator.[21][22] These differences made it difficult for developers to make their websites work well in both browsers, leading to widespread use of "best viewed in Netscape" and "best viewed in Internet Explorer" logos for several years.[21][23]

The rise of JScript
In November 1996, Netscape submitted JavaScript to Ecma International, as the starting point for a standard specification that all browser vendors could conform to. This led to the official release of the first ECMAScript language specification in June 1997.

The standards process continued for a few years, with the release of ECMAScript 2 in June 1998 and ECMAScript 3 in December 1999. Work on ECMAScript 4 began in 2000.[20]

Meanwhile, Microsoft gained an increasingly dominant position in the browser market. By the early 2000s, Internet Explorer's market share reached 95%.[24] This meant that JScript became the de facto standard for client-side scripting on the Web.

Microsoft initially participated in the standards process and implemented some proposals in its JScript language, but eventually it stopped collaborating on Ecma work. Thus ECMAScript 4 was mothballed.

Growth and standardization
During the period of Internet Explorer dominance in the early 2000s, client-side scripting was stagnant. This started to change in 2004, when the successor of Netscape, Mozilla, released the Firefox browser. Firefox was well received by many, taking significant market share from Internet Explorer.[25]

In 2005, Mozilla joined ECMA International, and work started on the ECMAScript for XML (E4X) standard. This led to Mozilla working jointly with Macromedia (later acquired by Adobe Systems), who were implementing E4X in their ActionScript 3 language, which was based on an ECMAScript 4 draft. The goal became standardizing ActionScript 3 as the new ECMAScript 4. To this end, Adobe Systems released the Tamarin implementation as an open source project. However, Tamarin and ActionScript 3 were too different from established client-side scripting, and without cooperation from Microsoft, ECMAScript 4 never reached fruition.

Meanwhile, very important developments were occurring in open-source communities not affiliated with ECMA work. In 2005, Jesse James Garrett released a white paper in which he coined the term Ajax and described a set of technologies, of which JavaScript was the backbone, to create web applications where data can be loaded in the background, avoiding the need for full page reloads. This sparked a renaissance period of JavaScript, spearheaded by open-source libraries and the communities that formed around them. Many new libraries were created, including jQuery, Prototype, Dojo Toolkit, and MooTools.

Google debuted its Chrome browser in 2008, with the V8 JavaScript engine that was faster than its competition.[26][27] The key innovation was just-in-time compilation (JIT),[28] so other browser vendors needed to overhaul their engines for JIT.[29]

In July 2008, these disparate parties came together for a conference in Oslo. This led to the eventual agreement in early 2009 to combine all relevant work and drive the language forward. The result was the ECMAScript 5 standard, released in December 2009.

Reaching maturity
Ambitious work on the language continued for several years, culminating in an extensive collection of additions and refinements being formalized with the publication of ECMAScript 6 in 2015.[30]

The creation of Node.js in 2009 by Ryan Dahl sparked a significant increase in the usage of JavaScript outside of web browsers. Node combines the V8 engine, an event loop, and I/O APIs, thereby providing a stand-alone JavaScript runtime system.[31][32] As of 2018, Node had been used by millions of developers,[33] and npm had the most modules of any package manager in the world.[34]

The ECMAScript draft specification is currently maintained openly on GitHub, and editions are produced via regular annual snapshots.[35] Potential revisions to the language are vetted through a comprehensive proposal process.[36][37] Now, instead of edition numbers, developers check the status of upcoming features individually.[35]

The current JavaScript ecosystem has many libraries and frameworks, established programming practices, and substantial usage of JavaScript outside of web browsers. Plus, with the rise of single-page applications and other JavaScript-heavy websites, several transpilers have been created to aid the development process.[38]

https://en.wikipedia.org/wiki/JavaScript

- **DirectX (1995)**

Microsoft DirectX is a collection of application programming interfaces (APIs) for handling tasks related to multimedia, especially game programming and video, on Microsoft platforms. Originally, the names of these APIs all began with "Direct", such as Direct3D, DirectDraw, DirectMusic, DirectPlay, DirectSound, and so forth. The name DirectX was coined as a shorthand term for all of these APIs (the X standing in for the particular API names) and soon became the name of the collection. When Microsoft later set out to develop a gaming console, the X was used as the basis of the name Xbox to indicate that the console was based on DirectX technology.[3] The X initial has been carried forward in the naming of APIs designed for the Xbox such as XInput and the Cross-platform Audio Creation Tool (XACT), while the DirectX pattern has been continued for Windows APIs such as Direct2D and DirectWrite.

Direct3D (the 3D graphics API within DirectX) is widely used in the development of video games for Microsoft Windows and the Xbox line of consoles. Direct3D is also used by other software applications for visualization and graphics tasks such as CAD/CAM engineering. As Direct3D is the most widely publicized component of DirectX, it is common to see the names "DirectX" and "Direct3D" used interchangeably.

The DirectX software development kit (SDK) consists of runtime libraries in redistributable binary form, along with accompanying documentation and headers for use in coding. Originally, the runtimes were only installed by games or explicitly by the user. Windows 95 did not launch with DirectX, but DirectX was included with Windows 95 OEM Service Release 2.[4] Windows 98 and Windows NT 4.0 both shipped with DirectX, as has every version of Windows released since. The SDK is available as a free download. While the runtimes are proprietary, closed-source software, source code is provided for most of the SDK samples. Starting with the release of Windows 8 Developer Preview, DirectX SDK has been integrated into Windows SDK.[5]

https://en.wikipedia.org/wiki/DirectX

- **Direct3D (1996) - Microsoft**

Direct3D is a graphics application programming interface (API) for Microsoft Windows. Part of DirectX, Direct3D is used to render three-dimensional graphics in applications where performance is important, such as games. Direct3D uses hardware acceleration if it is available on the graphics card, allowing for hardware acceleration of the entire 3D rendering pipeline or even only partial acceleration. Direct3D exposes the advanced graphics capabilities of 3D graphics hardware, including Z-buffering,[1] W-buffering,[2] stencil buffering, spatial anti-aliasing, alpha blending, color blending, mipmapping, texture blending,[3][4] clipping, culling, atmospheric effects, perspective-correct texture mapping, programmable HLSL shaders[5] and effects.[6] Integration with other DirectX technologies enables Direct3D to deliver such features as video mapping, hardware 3D rendering in 2D overlay planes, and even sprites, providing the use of 2D and 3D graphics in interactive media ties.

https://en.wikipedia.org/wiki/Direct3D

- **Unreal Engine (1998) - Unreal**

Unreal Engine (UE) is a 3D computer graphics game engine developed by Epic Games, first showcased in the 1998 first-person shooter game Unreal. Initially developed for PC first-person shooters, it has since been used in a variety of genres of games and has seen adoption by other industries, most notably the film and television industry. Written in C++, the Unreal Engine features a high degree of portability, supporting a wide range of desktop, mobile, console and virtual reality platforms.

https://en.wikipedia.org/wiki/Unreal_Engine

### Automation
- **LonWork (1999)** 

LonWorks or Local Operating Network is an open standard (ISO/IEC 14908) for networking platforms specifically created to address the needs of control applications. The platform is built on a protocol created by Echelon Corporation for networking devices over media such as twisted pair, powerlines, fibre optics , and RF. It is used for the automation of various functions within buildings such as lighting and HVAC; see building automation.
The technology has its origins with chip designs, power line and twisted pair, signaling technology, routers, network management software, and other products from Echelon Corporation. In 1999 the communications protocol (then known as LonTalk) was submitted to ANSI and accepted as a standard for control networking (ANSI/CEA-709.1-B). Echelon's power line and twisted pair signaling technology was also submitted to ANSI for standardization and accepted. Since then, ANSI/CEA-709.1 has been accepted as the basis for IEEE 1473-L (in-train controls), AAR electro-pneumatic braking systems for freight trains, IFSF (European petrol station control), SEMI (semiconductor equipment manufacturing), and in 2005 as EN 14908 (European building automation standard). The protocol is also one of several data link/physical layers of the BACnet ASHRAE/ANSI standard for building automation.
China ratified the technology as a national controls standard, GB/Z 20177.1-2006 and as a building and intelligent community standard, GB/T 20299.4-2006; and in 2007 CECED, the European Committee of Domestic Equipment Manufacturers, adopted the protocol as part of its Household Appliances Control and Monitoring – Application Interworking Specification (AIS) standards.
During 2008 ISO and IEC have granted the communications protocol, twisted pair signaling technology, power line signaling technology, and Internet Protocol (IP) compatibility standard numbers ISO/IEC 14908-1, -2, -3, and -4.[1]

https://en.wikipedia.org/wiki/LonWorks

## Navigation
**[`^        back to top        ^`](#)**

**GPS is operational, military use, miniaturization and shift to dual-use**

The Gulf War from 1990 to 1991 was the first conflict in which the military widely used GPS.[58]

In 1991, a project to create a miniature GPS receiver successfully ended, replacing the previous 16 kg (35 lb) military receivers with a 1.25 kg (2.8 lb) handheld receiver.[26]

In 1992, the 2nd Space Wing, which originally managed the system, was inactivated and replaced by the 50th Space Wing.

By December 1993, GPS achieved initial operational capability (IOC), with a full constellation (24 satellites) available and providing the Standard Positioning Service (SPS).[59]

Full Operational Capability (FOC) was declared by Air Force Space Command (AFSPC) in April 1995, signifying full availability of the military's secure Precise Positioning Service (PPS).[59]

In 1996, recognizing the importance of GPS to civilian users as well as military users, U.S. President Bill Clinton issued a policy directive[60] declaring GPS a dual-use system and establishing an Interagency GPS Executive Board to manage it as a national asset.

In 1998, United States Vice President Al Gore announced plans to upgrade GPS with two new civilian signals for enhanced user accuracy and reliability, particularly with respect to aviation safety, and in 2000 the United States Congress authorized the effort, referring to it as GPS III.

https://en.wikipedia.org/wiki/Global_Positioning_System
