# Tech evolutions in the 1980s

**In short:**
- [Inventions](#inventions) : 
  * Color inkjet printer
  * STN passive matrix LCD screens invention
  * First RFID patent
  * Passive Optical Networks (PON) proposal
  * First practical organic LED (OLED) device
  * FinFET
  * Larger (14") TFT screens
- [Electronics](#electronics) : 
  * From published schematic diagrams to single ICs
  * Intel i286
  * Intel i386
  * IA-32 architecture (i386 - x86)
  * UART serial communication
  * first commercial flash memory
  * first commercial blue LED based on the indirect bandgap semiconductor
  * Intel i486
- [Energy](#energy) : 
- [Telecommunications](#telecommunications) : 
  * T3 circuits
  * Breakup of the Bell system
  * Birth of ILECs (1984)
  * Artificial division between long distance and local calls in the U.S.
  * Single-mode optical fibre and cable
  * ISDN
  * ISPs
- [Networking](#networking) : 
  * OSI Model vs. TCP/IP War
  * TCP/IP Emergence (IPv4)
  * Remote Procedure Call (RPC)
  * Ethernet (IEEE 802.3)
  * Multiprotocol router (Cisco IOS)
  * Original standard for Ethernet over fiber (FOIRL)
  * BBS
  * NSFNET
  * First internet access
- [Cryptography](#cryptography) : 
  * Age of RSA - RSA patent granted
  * X.509
- [Computers](#computers) :
  * [Form factor](#form-factor) : 
    - Minicomputers replace low-end mainframes
    - Rise of superminicomputers
    - GUI + Mouse (Xerox Star 8010)
    - First PC (IBM PC)
    - First PC with a GUI (Apple LISA)
    - 640x400i (Commodore Amiga) 
  * [OS](#os) : 
    
    - *Clients* : 
      * IBM DOS (Microsft MS DOS)
      * Apple MacOS
      * Microsoft Windows 2.0, 
  
  * [Peripherals](#peripherals) : 
    - Laser printing to mass markets
    - Parallel Port, COM port, AT/ISA bus, Apple Desktop Bux, PS/2, EISA bus
  * [Storage](#storage) : 
    - Internal HDD (10 MB)
    - 3"1/2 Floppy Disk
    - external drive
    - SCSI
    - ATA/IDE
  * [Uses](#uses) :
    
    - *Servers*: 
      * Mainframes : graphic display terminals : X Window
      * Electronic data exchange : EDI
      * Filesharing : SMB
      * DNS : Bind
      * Forums : Bulletin Board System (BBS)
    
    - *Clients*: 
      * Desktop publishing : PageMager
      * Unix shell: Bash    
    
    - *CGI*: 
      * Pixar early history
      * First Academy Award for a computer-generated film (Tin Toy)
- [Consumer Electronics](#consumer-electronics) :
  * [Gadgets](#gadgets) : 
    - First personal digital assistant (PDA) / handheld PC
    - First graphing calculator
    - First symbolic computing calculator
  * [Multimedia](#multimedia) : 
    - Infrared LED remote controls (Starcom Cable TV Converter)
    - Compact Disc (CD)
    - Pocket LCV TVs
    - Audio compact disk player
  * [Screens](#screens) : 
    - Orange-on-black plasma screen for laptops
    - Flat screen CRT TV
    - SVGA Graphics Cards
  * [Broadcast](#broadcast) : 
    - TVRO/C-band satellite era
    - HD-MAC - last analog HDTV system 
  * [Video games](#video-games) : 
    - Golden age - third generation of video games consoles  : Nintendo NES, Sega Master
    - Adventure games with a GUI : Mystery House
    - Crash of 1983  : ET Game
    - Fourth generation of video games consoles generations : NEC Home Electronics, Nintendo Gameboy
    - Recovery and decline of arcade games
- [Standards and protocols](#standards-and-protocols) : 
  * Compact Disc Digital Audio (CD)
  * WWW
  * BMP, GIF
  * email protocols
  * IRC
  * Zip (PKZIP)
  * ISO 9660 file system
  * EDI (EFIFACT, ODETTE/OFTP)
  * Netbios
  * SMB
  * Modems speeds (V22, V22bis, V32)
- [Programming languages and frameworks](#programming-languages-and-frameworks) : Smalltalk (object-oriented, dynamically typed reflective programming language), Miranda (first purely functional language to be commercially supported), Objective-C (adds Smalltalk-style messaging to the C, later used by Apple MacOS, iOS), C++ (from "C with Classes" to systems programming and embedded, resource-constrained software and large systems, with performance, efficiency, and flexibility of use, manual memory management, object oriented), Erlang (telecom language, distributed, fault-tolerant, soft real-time, code hot-swapping), Perl (dynamic text processing language, REGEX, early 90s CGI scripts)
- [Navigation](#navigation) : Decision to open GPS to civilian use and deployment of modern GPS satellites

## Inventions
**[`^        back to top        ^`](#)**

- **Color inkjet printer (1982) - Robert Howard**

In 1982, Robert Howard came up with the idea to produce a small color printing system that used piezos to spit drops of ink. He formed the company, R.H. (Robert Howard) Research (named Howtek, Inc. in Feb 1984), and developed the revolutionary technology that led to the Pixelmaster color printer with solid ink[6] using Thermojet technology. This technology consists of a tubular single nozzle acoustical wave drop generator invented originally by Steven Zoltan in 1972 with a glass nozzle and improved by the Howtek inkjet engineer in 1984 with a Tefzel molded nozzle to remove unwanted fluid frequencies.

https://en.wikipedia.org/wiki/Inkjet_printing

- **STN passive matrix LCD screens invention (1983) - Brown, Boveri & Cie (BBC) Research Center**

In 1983, researchers at Brown, Boveri & Cie (BBC) Research Center, Switzerland, invented the super-twisted nematic (STN) structure for passive matrix-addressed LCDs. H. Amstutz et al. were listed as inventors in the corresponding patent applications filed in Switzerland on July 7, 1983, and October 28, 1983. Patents were granted in Switzerland CH 665491, Europe EP 0131216,[68] U.S. Patent 4,634,229 and many more countries. In 1980, Brown Boveri started a 50/50 joint venture with the Dutch Philips company, called Videlec.[69] Philips had the required know-how to design and build integrated circuits for the control of large LCD panels. In addition, Philips had better access to markets for electronic components and intended to use LCDs in new product generations of hi-fi, video equipment and telephones. In 1984, Philips researchers Theodorus Welzen and Adrianus de Vaan invented a video speed-drive scheme that solved the slow response time of STN-LCDs, enabling high-resolution, high-quality, and smooth-moving video images on STN-LCDs.[70] In 1985, Philips inventors Theodorus Welzen and Adrianus de Vaan solved the problem of driving high-resolution STN-LCDs using low-voltage (CMOS-based) drive electronics, allowing the application of high-quality (high resolution and video speed) LCD panels in battery-operated portable products like notebook computers and mobile phones.[71] In 1985, Philips acquired 100% of the Videlec AG company based in Switzerland. Afterwards, Philips moved the Videlec production lines to the Netherlands. Years later, Philips successfully produced and marketed complete modules (consisting of the LCD screen, microphone, speakers etc.) in high-volume production for the booming mobile phone industry.

https://en.wikipedia.org/wiki/Liquid-crystal_display

- **First RFID patent (1983)**

May 17, 1983: The first patent to be associated with the abbreviation "RFID" was granted to Charles Walton.[16]

https://en.wikipedia.org/wiki/Near-field_communication

- **Passive Optical Networks (PON) proposed (1987) - British Telecommunication**

A passive optical network (PON) is a fiber-optic telecommunications technology for delivering broadband network access to end-customers. Its architecture implements a point-to-multipoint topology in which a single optical fiber serves multiple endpoints by using unpowered (passive) fiber optic splitters to divide the fiber bandwidth among the endpoints. Passive optical networks are often referred to as the last mile between an Internet service provider (ISP) and its customers.[1]

Passive optical networks were first proposed by British Telecommunications in 1987.

https://en.wikipedia.org/wiki/Passive_optical_network

- **first practical organic LED (OLED) device (1987) - Eastman Kodak**

Chemists Ching Wan Tang and Steven Van Slyke at Eastman Kodak built the first practical OLED device in 1987.[28] This device used a two-layer structure with separate hole transporting and electron transporting layers such that recombination and light emission occurred in the middle of the organic layer; this resulted in a reduction in operating voltage and improvements in efficiency.

https://en.wikipedia.org/wiki/OLED

- **FinFET - Hitachi (1988)** 

FinFET (fin field-effect transistor), a type of 3D multi-gate MOSFET, was developed by Digh Hisamoto and his team of researchers at Hitachi Central Research Laboratory in 1989.[18][19]

https://en.wikipedia.org/wiki/Semiconductor_device

- **Larger (14") TFT screens (1988) - Sharp**

In 1988, a Sharp research team led by engineer T. Nagayasu demonstrated a 14-inch full-color LCD display,[10][18] which convinced the electronics industry that LCD would eventually replace CRTs as the standard television display technology.[10]

https://en.wikipedia.org/wiki/Flat-panel_display

## Electronics
**[`^        back to top        ^`](#)**

- **From published schematic diagrams to single Integrated Circuits (1980s)**
 
The most popular computers of the 1980s such as the Apple II and IBM PC had published schematic diagrams and other documentation which permitted rapid reverse-engineering and third-party replacement motherboards. Usually intended for building new computers compatible with the exemplars, many motherboards offered additional performance or other features and were used to upgrade the manufacturer's original equipment.

During the late 1980s and early 1990s, it became economical to move an increasing number of peripheral functions onto the motherboard. In the late 1980s, personal computer motherboards began to include single ICs (also called Super I/O chips) capable of supporting a set of low-speed peripherals: PS/2 keyboard and mouse, floppy disk drive, serial ports, and parallel ports.

https://en.wikipedia.org/wiki/Motherboard

- **Intel 286 (1982) - Intel**

The Intel 80286[3] (also marketed as the iAPX 286[4] and often called Intel 286) is a 16-bit microprocessor that was introduced on February 1, 1982. It was the first 8086-based CPU with separate, non-multiplexed address and data buses and also the first with memory management and wide protection abilities. The 80286 used approximately 134,000 transistors in its original nMOS (HMOS) incarnation and, just like the contemporary 80186,[5] it could correctly execute most software written for the earlier Intel 8086 and 8088 processors.[6]

The 80286 was employed for the IBM PC/AT, introduced in 1984, and then widely used in most PC/AT compatible computers until the early 1990s.

https://en.wikipedia.org/wiki/Intel_80286

- **Intel i386 (1985) - Intel **

The Intel 386, originally released as 80386 and later renamed i386, is a 32-bit microprocessor introduced in 1985.[2] The first versions had 275,000 transistors[3] and were the CPU of many workstations and high-end personal computers of the time. As the original implementation of the 32-bit extension of the 80286 architecture,[4] the i386 instruction set, programming model, and binary encodings are still the common denominator for all 32-bit x86 processors, which is termed the i386-architecture, x86, or IA-32, depending on context.

The 32-bit i386 can correctly execute most code intended for the earlier 16-bit processors such as 8086 and 80286 that were ubiquitous in early PCs. (Following the same tradition, modern 64-bit x86 processors are able to run most programs written for older x86 CPUs, all the way back to the original 16-bit 8086 of 1978.) Over the years, successively newer implementations of the same architecture have become several hundreds of times faster than the original 80386 (and thousands of times faster than the 8086).[5] A 33 MHz 80386 was reportedly measured to operate at about 11.4 MIPS.[6]

Development of i386 technology began in 1982 under the internal name of P3.[7] The tape-out of the 80386 development was finalized on July 1985.[8] The 80386 was introduced as pre-production samples for software development workstations in October 1985.[9] Manufacturing of the chips in significant quantities commenced in June 1986,[10][11] along with the first plug-in device that allowed existing 80286-based computers to be upgraded to the 386, the Translator 386 by American Computer and Peripheral.[12][13] Mainboards for 80386-based computer systems were cumbersome and expensive at first, but manufacturing was justified upon the 80386's mainstream adoption. The first personal computer to make use of the 80386 was designed and manufactured by Compaq[14] and marked the first time a fundamental component in the IBM PC compatible de facto standard was updated by a company other than IBM.

https://en.wikipedia.org/wiki/I386

- **IA-32 architecture (1985) - Intel**

IA-32 (short for "Intel Architecture, 32-bit", sometimes also called i386[1][2])[3] is the 32-bit version of the x86 instruction set architecture, designed by Intel and first implemented in the 80386 microprocessor in 1985. IA-32 is the first incarnation of x86 that supports 32-bit computing;[4] as a result, the "IA-32" term may be used as a metonym to refer to all x86 versions that support 32-bit computing.[5][6]

Within various programming language directives, IA-32 is still sometimes referred to as the "i386" architecture. In some other contexts, certain iterations of the IA-32 ISA are sometimes labelled i486, i586 and i686, referring to the instruction supersets offered by the 80486, the P5 and the P6 microarchitectures respectively. These updates offered numerous additions alongside the base IA-32 set including floating-point capabilities and the MMX extensions.

Intel was historically the largest manufacturer of IA-32 processors, with the second biggest supplier having been AMD. During the 1990s, VIA, Transmeta and other chip manufacturers also produced IA-32 compatible processors (e.g. WinChip). In the modern era, Intel still produced IA-32 processors under the Intel Quark microcontroller platform until 2019; however, since the 2000s, the majority of manufacturers (Intel included) moved almost exclusively to implementing CPUs based on the 64-bit variant of x86, x86-64. x86-64, by specification, offers legacy operating modes that operate on the IA-32 ISA for backwards compatibility. Even given the contemporary prevalence of x86-64, as of 2018, IA-32 protected mode versions of many modern operating systems are still maintained, e.g. Microsoft Windows (until Windows 10; Windows 11 requires x86-64-compatible processor for x86 versions)[7] and the Debian Linux distribution.[8] In spite of IA-32's name (and causing some potential confusion), the 64-bit evolution of x86 that originated out of AMD would not be known as "IA-64", that name instead belonging to Intel's Itanium architecture.

https://en.wikipedia.org/wiki/IA-32

- **UART serial communication (1987) - National Semiconductor**

The 16550 UART (universal asynchronous receiver/transmitter) is an integrated circuit designed for implementing the interface for serial communications. The corrected -A version was released in 1987 by National Semiconductor.[1] 

https://en.wikipedia.org/wiki/16550_UART

- **First commercial flash memory - (1987, Toshiba), (1988, Intel)**

Toshiba commercially launched NAND flash memory in 1987.[1][13] Intel Corporation introduced the first commercial NOR type flash chip in 1988.[22

https://en.wikipedia.org/wiki/Flash_memory

In 1989, Intel employed the FGMOS as an analog nonvolatile memory element** in its **electrically trainable artificial neural network (ETANN) chip**,[3] demonstrating the potential of using FGMOS devices for applications other than digital memory.

https://en.wikipedia.org/wiki/Floating-gate_MOSFET

- **First commercial blue LED based on the indirect bandgap semiconductor, silicon carbide (SiC) (1989) - Cree**

In August 1989, Cree introduced the first commercially available blue LED based on the indirect bandgap semiconductor, silicon carbide (SiC).[43] SiC LEDs had very low efficiency, no more than about 0.03%, but did emit in the blue portion of the visible light spectrum.[44][45]

https://en.wikipedia.org/wiki/Light-emitting_diode

- **Intel i486 (1989) - Intel**

The Intel 486, officially named i486 and also known as 80486, is a microprocessor. It is a higher-performance follow-up to the Intel 386. The i486 was introduced in 1989. It represents the fourth generation of binary compatible CPUs following the 8086 of 1978, the Intel 80286 of 1982, and 1985's i386.

It was the first tightly-pipelined[c] x86 design as well as the first x86 chip to include more than one million transistors. It offered a large on-chip cache and an integrated floating-point unit.

A typical 50 MHz i486 executes around 40 million instructions per second (MIPS), reaching 50 MIPS peak performance. It is approximately twice as fast as the i386 or i286 per clock cycle. The i486's improved performance is thanks to its five-stage pipeline with all stages bound to a single cycle. The enhanced FPU unit on the chip was significantly faster than the i387 FPU per cycle. The intel 80387 FPU ("i387") was a separate, optional math coprocessor that was installed in a motherboard socket alongside the i386.

The i486 was succeeded by the original Pentium.

https://en.wikipedia.org/wiki/I486

## Energy
**[`^        back to top        ^`](#)**


## Telecommunications
**[`^        back to top        ^`](#)**

- **Leased lines - T3 circuits**

With the extension of digital services in the 1980s, **leased lines were used to connect customer premises to frame relay or ATM networks**. Access data rates increased from the original T1 option with maximum transmission speed of 1.544 Mbit/s up to **T3 circuits**.

https://en.wikipedia.org/wiki/Leased_line

- **Breakup of the Bell System (1984) - Birth of ILECs, inter-exchange carrier, local exchange carrier**

During the 1970s, a growing conviction that the scope of AT&T's monopoly had become more of a hindrance than a stimulus to innovation led to a new antitrust case against the company. The resulting settlement produced the Bell breakup in 1984, the artificial division of the industry between local and long-distance calling, and the opening of the long-distance market to full competition.

https://www.princeton.edu/~starr/articles/articles02/Starr-TelecomImplosion-9-02.htm

The breakup of the Bell System was mandated on January 8, 1982, by an agreed consent decree providing that AT&T Corporation would, as had been initially proposed by AT&T, relinquish control of the Bell Operating Companies, which had provided local telephone service in the United States.[1] This effectively took the monopoly that was the Bell System and split it into entirely separate companies that would continue to provide telephone service. AT&T would continue to be a provider of long-distance service, while the now-independent Regional Bell Operating Companies (RBOCs), nicknamed the "Baby Bells", would provide local service, and would no longer be directly supplied with equipment from AT&T subsidiary Western Electric.

This divestiture was initiated by the filing in 1974 by the United States Department of Justice of an antitrust lawsuit against AT&T.[2] AT&T was, at the time, the sole provider of telephone service throughout most of the United States. Furthermore, most telephonic equipment in the United States was produced by its subsidiary Western Electric. This vertical integration led AT&T to have almost total control over communication technology in the country, which led to the antitrust case United States v. AT&T. The plaintiff in the court complaint asked the court to order AT&T to divest ownership of Western Electric.[3]

Feeling that it was about to lose the suit, AT&T proposed an alternative: its breakup. It proposed that it retain control of Western Electric, Yellow Pages, the Bell trademark, Bell Labs, and AT&T Long Distance. It also proposed that it be freed from a 1956 antitrust consent decree, then administered by Judge Vincent P. Biunno in the United States District Court for the District of New Jersey, that barred it from participating in the general sale of computers (retreat from international markets, relinquish ownership in Bell Canada, and Northern Electric a Western Electric subsidiary).[4] In return, it proposed to give up ownership of the local operating companies. This last concession, it argued, would achieve the government's goal of creating competition in supplying telephone equipment and supplies to the operative companies. The settlement was finalized on January 8, 1982, with some changes ordered by the decree court: the regional holding companies got the Bell trademark, Yellow Pages, and about half of Bell Labs.

Effective January 1, 1984, the Bell System's many member companies were variously merged into seven independent "Regional Holding Companies", also known as Regional Bell Operating Companies (RBOCs), or "Baby Bells". This divestiture reduced the book value of AT&T by approximately 70%.

https://en.wikipedia.org/wiki/Breakup_of_the_Bell_System

An **incumbent local exchange carrier (ILEC)** is a local telephone company which held the regional monopoly on landline service before the market was opened to competitive local exchange carriers, or the corporate successor of such a firm.

In the United States, ILECs were companies in existence at the time of the breakup of AT&T into the Regional Bell Operating Companies (RBOCs), also known as the "Baby Bells"

https://en.wikipedia.org/wiki/Incumbent_local_exchange_carrier

- **Artificial division between long distance and local calls in the U.S. (1985)**

Even after the breakup of the Bell system in 1984, AT&T and the regional Bell operating companies (the "Baby Bells") had remained bulwarks of the economy.

https://www.princeton.edu/~starr/articles/articles02/Starr-TelecomImplosion-9-02.htm

Three types of carriers could be distinguished:
- **Inter-Exchange Carrier (IEC)**, a company allowed to handle long-distance calls following the break-up of the Bell system in the US by anti-trust regulators.
https://foldoc.org/IntereXchange+Carrier
- **Local Exchange Carriers (LEC)**, a company allowed to handle local calls following the break-up of the Bell system in the US by anti-trust regulators. These vary from Regional Bell Operating Companies (RBOC) through to small independents such as Farmers Cooperative.
https://foldoc.org/Local+Exchange+Carriers
- **Competitive Access Providers (CAP) or "Bypass Carrier"**, a company which provided network links between the customer and the IntereXchange Carrier or even directly to the InternetService Provider. CAPs operate private networks independentof Local Exchange Carriers.
https://foldoc.org/Competitive+Access+Provider

LEC were not allowed to handle long-distance traffic and Inter Exchange carriers were not allowed to handle local calls.

- **Integrated Services Digital Network (ISDN) (1988)** 

ISDN is a set of communication standards for simultaneous digital transmission of voice, video, data, and other network services over the digitalised circuits of the public switched telephone network.[1] Work on the standard began in 1980 at Bell Labs and was formally standardized in 1988 in the CCITT "Red Book".
Prior to ISDN, the telephone system consisted of digital links like T1/E1 on the long-distance lines between telephone company offices and analog signals on copper telephone wires to the customers, the "last mile". 

https://en.wikipedia.org/wiki/Integrated_Services_Digital_Network

- **ISPs - Internet Service Providers (1980s)**

An Internet service provider (ISP) is an organization that provides services for accessing, using, or participating in the Internet. ISPs can be organized in various forms, such as commercial, community-owned, non-profit, or otherwise privately owned.

Internet services typically provided by ISPs can include Internet access, Internet transit, domain name registration, web hosting, Usenet service, and colocation.

An ISP typically serves as the access point or the gateway that provides a user access to everything available on the Internet.[1] Such a network can also be called as an eyeball network.

During the 1980s, online service providers such as CompuServe and America On Line (AOL) began to offer limited capabilities to access the Internet, such as e-mail interchange, but full access to the Internet was not readily available to the general public.

In 1989, the first Internet service providers, companies offering the public direct access to the Internet for a monthly fee, were established in Australia[4] and the United States. In Brookline, Massachusetts, **The World** became the first commercial ISP in the US. Its first customer was served in November 1989.[5] These companies generally offered dial-up connections, using the public telephone network to provide last-mile connections to their customers. The barriers to entry for dial-up ISPs were low and many providers emerged.

However, cable television companies and the telephone carriers already had wired connections to their customers and could offer Internet connections at much higher speeds than dial-up using broadband technology such as cable modems and digital subscriber line (DSL). As a result, these companies often became the dominant ISPs in their service areas, and what was once a highly competitive ISP market became effectively a monopoly or duopoly in countries with a commercial telecommunications market, such as the United States.

In 1995, NSFNET was decommissioned removing the last restrictions on the use of the Internet to carry commercial traffic and network access points were created to allow peering arrangements between commercial ISPs.

https://en.wikipedia.org/wiki/Internet_service_provider

## Networking
**[`^        back to top        ^`](#)**

- **OSI Model vs. TCP/IP War - TCP/IP Emergence (IPv4)**

Hubert Zimmermann, and Charles Bachman as chairman, played a key role in the development of the Open Systems Interconnections reference model. Beginning in 1978, this international work led to a draft proposal in 1980 and the final OSI model was published in 1984.

The early research and development of standards for data networks and protocols culminated in the Internet–OSI Standards War in the late 1980s and early 1990s.
Beginning in the early 1980s, ARPA pursued commercial partnerships with the telecommunication and computer industry which enabled the adoption of TCP/IP. In Europe, CERN purchased UNIX machines with TCP/IP for their intranet between 1984 and 1988

At the beginning of the 1990s, academic institutions and organizations in some European countries had adopted TCP/IP.[nb 6] In February 1990 RARE stated "without putting into question its OSI policy, recognizes the TCP/IP family of protocols as an open multivendor suite, well adapted to scientific and technical applications." In the same month, CERN established a transatlantic TCP/IP link with Cornell University in the United States.[53][67] Conversely, starting in August 1990, the NSFNET backbone supported the OSI Connectionless Network Protocol (CLNP) in addition to TCP/IP. 

https://en.wikipedia.org/wiki/Protocol_Wars

- **RPC - Remote Procedure Call (1981)**

In distributed computing, a remote procedure call (RPC) is when a computer program causes a procedure (subroutine) to execute in a different address space (commonly on another computer on a shared network), which is coded as if it were a normal (local) procedure call, without the programmer explicitly coding the details for the remote interaction. That is, the programmer writes essentially the same code whether the subroutine is local to the executing program, or remote. This is a form of client–server interaction (caller is client, executor is server), typically implemented via a request–response message-passing system. In the object-oriented programming paradigm, RPCs are represented by remote method invocation (RMI). The RPC model implies a level of location transparency, namely that calling procedures are largely the same whether they are local or remote, but usually, they are not identical, so local calls can be distinguished from remote calls. Remote calls are usually orders of magnitude slower and less reliable than local calls, so distinguishing them is important.

RPCs are a form of inter-process communication (IPC), in that different processes have different address spaces: if on the same host machine, they have distinct virtual address spaces, even though the physical address space is the same; while if they are on different hosts, the physical address space is different. Many different (often incompatible) technologies have been used to implement the concept.

Request–response protocols date to early distributed computing in the late 1960s, theoretical proposals of remote procedure calls as the model of network operations date to the 1970s, and practical implementations date to the early 1980s. Bruce Jay Nelson is generally credited with coining the term "remote procedure call" in 1981.[1]

Remote procedure calls used in modern operating systems trace their roots back to the RC 4000 multiprogramming system,[2] which used a request-response communication protocol for process synchronization.[3] The idea of treating network operations as remote procedure calls goes back at least to the 1970s in early ARPANET documents.[4] In 1978, Per Brinch Hansen proposed Distributed Processes, a language for distributed computing based on "external requests" consisting of procedure calls between processes.[5]

One of the earliest practical implementations was in 1982 by Brian Randell and colleagues for their Newcastle Connection between UNIX machines.[6] This was soon followed by "Lupine" by Andrew Birrell and Bruce Nelson in the Cedar environment at Xerox PARC.[7][8][9] Lupine automatically generated stubs, providing type-safe bindings, and used an efficient protocol for communication.[8] One of the first business uses of RPC was by Xerox under the name "Courier" in 1981. The first popular implementation of RPC on Unix was Sun's RPC (now called ONC RPC), used as the basis for Network File System (NFS).

In the 1990s, with the popularity of object-oriented programming, an alternative model of remote method invocation (RMI) was widely implemented, such as in Common Object Request Broker Architecture (CORBA, 1991) and Java remote method invocation. RMIs, in turn, fell in popularity with the rise of the internet, particularly in the 2000s.

https://en.wikipedia.org/wiki/Remote_procedure_call

- **Ethernet (1980) - IEEE 802.3 (1983)**
Ethernet (/ˈiːθərnɛt/) is a family of wired computer networking technologies commonly used in local area networks (LAN), metropolitan area networks (MAN) and wide area networks (WAN).[1] It was commercially introduced in 1980 and first standardized in 1983 as IEEE 802.3. Ethernet has since been refined to support higher bit rates, a greater number of nodes, and longer link distances, but retains much backward compatibility. Over time, Ethernet has largely replaced competing wired LAN technologies such as Token Ring, FDDI and ARCNET.

The original 10BASE5 Ethernet uses coaxial cable as a shared medium, while the newer Ethernet variants use twisted pair and fiber optic links in conjunction with switches. Over the course of its history, Ethernet data transfer rates have been increased from the original 2.94 Mbit/s[2] to the latest 400 Gbit/s, with rates up to 1.6 Tbit/s under development. The Ethernet standards include several wiring and signaling variants of the OSI physical layer.

https://en.wikipedia.org/wiki/Ethernet

- **BBS - Bulletin Board System (1980s)**

With the advent of the personal computer (PCs) in the early 1980s, one could use a modem to dial into a remote mainframe computer. In this case, the PC was used like a dumb terminal. But now files could be transferred and one PC could connect to another via modems.

The 1980s saw the rise of the **Bulletin Board System (BBS)**. A BBS was just a computer with a modem listening for incoming calls. The public could dial up a BBS with a modem and then download free software, participate in discussions on various topics, play on-line games, etc. Dialing in to a BBS was something like going to an Internet site. Except that to go to another BBS site, you would need to dial another number (and possible pay long distance telephone charges). Many BBSs would have a monthly charge but some were run by volunteers and were free. Many companies established BBSs for customers that contained support information, catalogs, etc. In the early 1990s, BBSs were booming. By the mid 1990s some even offered Internet connections. For some history of BBSs see Sysops' Corner: History of BBSing

https://tldp.org/HOWTO/Modem-HOWTO-29.html

- **Multiprotocol router - Cisco IOS (1985)**

Cisco Systems was founded in December 1984 by Sandy Lerner along with her husband Leonard Bosack. Lerner was the director of computer facilities for the Stanford University Graduate School of Business. Bosack was in charge of the Stanford University computer science department's computers.[15]

Cisco's initial product has roots in Stanford University's campus technology. In the early 1980s students and staff at Stanford, including Bosack, used technology on the campus to link all of the school's computer systems to talk to one another, creating a box that functioned as a **multiprotocol router** called the "Blue Box".[16] The Blue Box used circuitry made by Andy Bechtolsheim, and software that was originally written at Stanford by research engineer William Yeager.[16] Due to the underlying architecture, and its ability to scale well, Yeager's well-designed invention became a key to Cisco's early success.[17]

In 1985, Bosack and Stanford employee Kirk Lougheed began a project to formally network Stanford's campus.[16] They adapted Yeager's software into what became the foundation for **Cisco IOS**, despite Yeager's claims that he had been denied permission to sell the Blue Box commercially. On July 11, 1986, Bosack and Lougheed were forced to resign from Stanford and the university contemplated filing criminal complaints against Cisco and its founders for the theft of its software, hardware designs, and other intellectual properties.[16] In 1987, Stanford licensed the router software and two computer boards to Cisco.[16] In addition to Bosack, Lerner, Lougheed, Greg Satz (a programmer), and Richard Troiano (who handled sales), completed the early Cisco team.[16] The company's first CEO was Bill Graves, who held the position from 1987 to 1988.[18] In 1988, John Morgridge was appointed CEO.[1

- **NSFNET (1986)**

In the early 1980s, the National Science Foundation (NSF) funded national supercomputing centers at several universities in the United States, and provided interconnectivity in 1986 with the NSFNET project, thus creating network access to these supercomputer sites for research and academic organizations in the United States. International connections to NSFNET, the emergence of architecture such as the Domain Name System, and the adoption of TCP/IP internationally on existing networks marked the beginnings of the Internet.[6][7][8] Commercial Internet service providers (ISPs) emerged in 1989 in the United States and Australia.[9] The ARPANET was decommissioned in 1990.[10] Limited private connections to parts of the Internet by officially commercial entities emerged in several American cities by late 1989 and 1990.

https://en.wikipedia.org/wiki/History_of_the_Internet

- **Internet access (1989)**

Dial-up Internet has been around since the 1980s via public providers such as NSFNET-linked universities. The BBC established Internet access via Brunel University in the United Kingdom in 1989.

https://en.wikipedia.org/wiki/Dial-up_Internet_access

## Cryptography
**[`^        back to top        ^`](#)**

- **Age of RSA - RSA Patent granted (1983)**

https://en.wikipedia.org/wiki/RSA_(cryptosystem)

- **X.509 (1988)** 

In cryptography, X.509 is an International Telecommunication Union (ITU) standard defining the format of public key certificates.[1] X.509 certificates are used in many Internet protocols, including TLS/SSL, which is the basis for HTTPS,[2] the secure protocol for browsing the web. They are also used in offline applications, like electronic signatures.

https://en.wikipedia.org/wiki/X.509
## Computers
**[`^        back to top        ^`](#)**
### Form factor
**[`^        back to top        ^`](#)**

- **Minicomputers replace low-end mainframes (late 1980s)**
 
Shrinking demand and tough competition started a shakeout in the market in the early 1970s—RCA sold out to UNIVAC and GE sold its business to Honeywell; between 1986 and 1990 Honeywell was bought out by Bull; UNIVAC became a division of Sperry, which later merged with Burroughs to form Unisys Corporation in 1986.
In 1984 estimated sales of desktop computers ($11.6 billion) exceeded mainframe computers ($11.4 billion) for the first time. IBM received the vast majority of mainframe revenue.[19] During the 1980s, minicomputer-based systems grew more sophisticated and were able to displace the lower end of the mainframes. These computers, sometimes called **departmental computers**, were typified by the Digital Equipment Corporation VAX series.

https://en.wikipedia.org/wiki/Mainframe_computer

- **Rise of superminicomputers**

Notable manufacturers of superminicomputers in 1980 included: Digital Equipment Corporation, Perkin-Elmer, and Prime Computer.[14][15] Other makers of systems included SEL/Gould and Data General.[16] Four years later there were about a dozen companies producing a significant number of superminicomputers

https://en.wikipedia.org/wiki/Superminicomputer

- **Xerox Star 8010 workstation - First commercial GUI with a mouse (1981) - Xerox**.

The Xerox PARC GUI consisted of graphical elements such as windows, menus, radio buttons, and check boxes. The concept of icons was later introduced by David Canfield Smith, who had written a thesis on the subject under the guidance of Kay.[19][20][21] The PARC GUI employs a pointing device along with a keyboard. These aspects can be emphasized by using the alternative term and acronym for windows, icons, menus, pointing device (WIMP). This effort culminated in the 1973 Xerox Alto, the first computer with a GUI, though the system never reached commercial production. 
In 1981, Xerox eventually commercialized the Alto in the form of a new and enhanced system – the Xerox 8010 Information System – more commonly known as the Xerox Star.[22][23] These early systems spurred many other GUI efforts, including Lisp machines by Symbolics and other manufacturers, the Apple Lisa (which presented the concept of menu bar and window controls) in 1983, the Apple Macintosh 128K in 1984, and the Atari ST with Digital Research's GEM, and Commodore Amiga in 1985. Visi On was released in 1983 for the IBM PC compatible computers, but was never popular due to its high hardware demands.[24] Nevertheless, it was a crucial influence on the contemporary development of Microsoft Windows.[25]

https://en.wikipedia.org/wiki/Graphical_user_interface

- **IBM PC (1981) powered by Microsoft IBM PC DOS**

The IBM Personal Computer (model 5150, commonly known as the IBM PC) is the first microcomputer released in the IBM PC model line and the basis for the IBM PC compatible de facto standard. Released on August 12, 1981, it was created by a team of engineers and designers directed by Don Estridge in Boca Raton, Florida.
The machine was based on open architecture and third-party peripherals. Over time, expansion cards and software technology increased to support it.
The PC had a substantial influence on the personal computer market. The specifications of the IBM PC became one of the most popular computer design standards in the world. The only significant competition it faced from a non-compatible platform throughout the 1980s was from the Apple Macintosh product line. The majority of modern personal computers are distant descendants of the IBM PC.

https://en.wikipedia.org/wiki/IBM_Personal_Computer

The third marketed version of an integrated mouse shipped as a part of a computer and intended for personal computer navigation came with the Xerox 8010 Star in 1981.
By 1982, the Xerox 8010 was probably the best-known computer with a mouse. The Sun-1 also came with a mouse, and the forthcoming Apple Lisa was rumored to use one, but the peripheral remained obscure; Jack Hawley of The Mouse House reported that one buyer for a large organization believed at first that his company sold lab mice. Hawley, who manufactured mice for Xerox, stated that "Practically, I have the market all to myself right now"; a Hawley mouse cost $415.[49] In 1982, Logitech introduced the P4 Mouse at the Comdex trade show in Las Vegas, its first hardware mouse.[50] That same year Microsoft made the decision to make the MS-DOS program Microsoft Word mouse-compatible, and developed the first PC-compatible mouse. Microsoft's mouse shipped in 1983, thus beginning the Microsoft Hardware division of the company.[51] However, the mouse remained relatively obscure until the appearance of the Macintosh 128K (which included an updated version of the single-button[52] Lisa Mouse) in 1984,[53] and of the Amiga 1000 and the Atari ST in 1985.

https://en.wikipedia.org/wiki/Computer_mouse

- **Apple LISA - First PC with a GUI, OS with protected memory, document-oriented workflow (1983) - Apple**

Lisa is a desktop computer developed by Apple, released on January 19, 1983. It is one of the first personal computers to present a graphical user interface (GUI) in a machine aimed at individual business users. Its development began in 1978.[2] It underwent many changes before shipping at US$9,995 (equivalent to $27,190 in 2021) with a five-megabyte hard drive. It was affected by its high price, insufficient software, unreliable Apple FileWare floppy disks, and the immediate release of the cheaper and faster Macintosh. Only 10,000 were sold in two years.

Considered a commercial failure (albeit one with technical acclaim), Lisa introduced a number of advanced features that reappeared on the Macintosh and eventually IBM PC compatibles. Among them is an operating system with protected memory[3] and a document-oriented workflow. The hardware was more advanced overall than the forthcoming Macintosh 128K; the Lisa included hard disk drive support, capacity for up to 2 megabytes (MB) of random-access memory (RAM), expansion slots, and a larger, higher-resolution display.

https://en.wikipedia.org/wiki/Apple_Lisa

- **1984 Apple Commercial**

By 1984 computer dealers saw Apple as the only clear alternative to IBM's influence;[93] some even promoted its products to reduce dependence on the PC.[78] The company announced the Macintosh 128k to the press in October 1983, followed by an 18-page brochure included with magazines in December.[94] Its debut, however, was announced by a single national broadcast of a US$1.5 million television commercial, "1984" (equivalent to $3,900,000 in 2021). It was directed by Ridley Scott, aired during the third quarter of Super Bowl XVIII on January 22, 1984,[95] and is now considered a "watershed event"[96] and a "masterpiece.

https://en.wikipedia.org/wiki/History_of_personal_computers

- **Comodore Amiga - Home computer with 640x400i interlaced resolution (1985) - Comodore**

The 640 × 400i resolution (720 × 480i with borders disabled) was first introduced by home computers such as the Commodore Amiga and, later, Atari Falcon. These computers used interlace to boost the maximum vertical resolution. These modes were only suited to graphics or gaming, as the flickering interlace made reading text in word processor, database, or spreadsheet software difficult.

### OS
**[`^        back to top        ^`](#)**

#### Clients

- **IBM DOS operating system (1981) - IBM / Microsoft**

IBM initially announced intent to support multiple operating systems: CP/M-86, UCSD p-System,[62] and an in-house product called IBM PC DOS, developed by Microsoft.[63][8] In practice, IBM's expectation and intent was for the market to primarily use PC DOS,[64] CP/M-86 was not available for six months after the PC's release[65] and received extremely few orders once it was,[66] and p-System was also not available at release. PC DOS rapidly established itself as the standard OS for the PC and remained the standard for over a decade, with a variant being sold by Microsoft themselves as MS-DOS.

https://en.wikipedia.org/wiki/IBM_Personal_Computer

https://en.wikipedia.org/wiki/IBM_PC_DOS

- **Classic MacOS (1984)**

The "classic" Mac OS is the original Macintosh operating system that was introduced in 1984 alongside the first Macintosh and remained in primary use on Macs until the introduction of Mac OS X in 2001.

https://en.wikipedia.org/wiki/Macintosh_operating_systems

- **Windows 2.0 (1987) - Microsoft**

Windows 2.0 is a major release of Microsoft Windows, a family of graphical operating systems for personal computers developed by Microsoft. It was released to manufacturing on December 9, 1987, as a successor to Windows 1.0.

The product includes two different variants, a base edition for 8086 real mode, and Windows/386, an enhanced edition for i386 protected mode. Windows 2.0 differs from its predecessor by allowing users to overlap and resize application windows, while the operating environment also introduced desktop icons, keyboard shortcuts, and support for 16-color VGA graphics. It also **introduced Microsoft Word and Excel**, and integrated the Control Panel, while the developer support increased substantially.

Noted as an improvement of its predecessor, Microsoft Windows gained more sales and popularity after the release of the operating environment, although it is also considered to be the incarnation that remained a work in progress. Due to the introduction of overlapping windows, Apple Inc. had filed a lawsuit against Microsoft in March 1988 after accusing them of violating copyrights Apple held, although in the end, the judge ruled in favor of Microsoft. The operating environment was succeeded by Windows 2.1 in May 1988, while Microsoft ended its support on December 31, 2001.

https://en.wikipedia.org/wiki/Windows_2.0x

**Dynamic Data Exchange** was first introduced in 1987 with the release of Windows 2.0 as a method of interprocess communication so that one program could communicate with or control another program, somewhat like Sun's RPC (Remote Procedure Call).[1] At the time, the only method for communication between the operating system and client applications was the "Windows Messaging Layer." DDE extended this protocol to allow peer-to-peer communication among client applications, via message broadcasts.

https://en.wikipedia.org/wiki/Dynamic_Data_Exchange

### Peripherals
**[`^        back to top        ^`](#)**

- **Laser printers to mass markets based on Canon CX engine (1980s) - HP LaserJet, Apple LaserWriter**

*1981: The first small personal computer designed for office use, the Xerox Star 8010, reached market. The system used a desktop metaphor that was unsurpassed in commercial sales, until the Apple Macintosh. Although it was innovative, the Star workstation was a prohibitively-expensive (US$17,000) system, affordable only to a fraction of the businesses and institutions at which it was targeted.[9]

**1984: The first laser printer intended for mass-market sales, the HP LaserJet**, was released; it used the Canon CX engine, controlled by HP software. The LaserJet was quickly followed by printers from Brother Industries, IBM, and others. First-generation machines had large photosensitive drums, of circumference greater than the loaded paper's length. Once faster-recovery coatings were developed, the drums could touch the paper multiple times in a pass, and therefore be smaller in diameter.

**1985: Apple introduced the LaserWriter (also based on the Canon CX engine)**,[10] but used the newly released PostScript page-description language (up until this point, each manufacturer used its own proprietary page-description language, making the supporting software complex and expensive). PostScript allowed the use of text, fonts, graphics, images, and color largely independent of the printer's brand or resolution.
PageMaker, developed by Aldus for the Macintosh and LaserWriter, was also released in 1985 and the combination became very popular for desktop publishing.[5][6]

https://en.wikipedia.org/wiki/Laser_printing

-**Parallel cable and port (1981) - IBM**
IBM released the IBM Personal Computer in 1981 and included a variant of the Centronics interface— only IBM logo printers (rebranded from Epson) could be used with the IBM PC.[5] IBM standardized the **parallel cable with a DB25F connector on the PC side and the 36-pin Centronics connector on the printer side**. Vendors soon released printers compatible with both standard Centronics and the IBM implementation.

https://en.wikipedia.org/wiki/Parallel_port

- **COM port (1981) - IBM**
COM (communication port)[1][2] is the original, yet still common, name of the serial port interface on PC-compatible computers. It can refer not only to physical ports, but also to emulated ports, such as ports created by Bluetooth or USB adapters.

The name for the COM port started with the original IBM PC. IBM had called the four well-defined communication RS-232 ports the "COM" ports, starting from COM1 through COM4. In BASICA and PC DOS you can open these ports as "COM1:" through "COM4:", and all PC compatibles using MSDOS used the same denotation.[citation needed] Most PC-compatible computers in the 1980s and 1990s had one or two COM ports.

By 2007, most computers shipped with only one or no physical COM ports. Today few consumer-grade PC-compatible computers include COM ports,[3] though some of them do still include a COM header on the motherboard.[4]

After the RS-232 COM port was removed from most consumer-grade computers, an external USB-to-UART serial adapter cable was used to compensate for the loss. A major supplier of these chips is FTDI.[citation needed]

https://en.wikipedia.org/wiki/COM_(hardware_interface)

- **AT/ISA bus - IBM PC AT (1984) - IBM**

IBM PC AT introduces the AT bus, later known as the ISA bus, a 16-bit bus with backwards compatibility with 8-bit PC-compatible expansion cards. The bus also offered fifteen IRQs and seven DMA channels, expanded from eight IRQs and four DMA channels for the PC, achieved by adding another 8259A IRQ controller and another 8237A DMA controller.[5][6] Some IRQ and DMA channels are used by the motherboard and not exposed on the expansion bus. Both dual IRQ and DMA chipsets are cascading which shares the primary pair. In addition to these chipsets, Intel 82284 Clock Driver and Ready Interface and Intel 82288 Bus Controller are to support the microprocessor.

https://en.wikipedia.org/wiki/IBM_Personal_Computer/AT

Industry Standard Architecture (ISA) is the 16-bit internal bus of IBM PC/AT and similar computers based on the Intel 80286 and its immediate successors during the 1980s. The bus was (largely) backward compatible with the 8-bit bus of the 8088-based IBM PC, including the IBM PC/XT as well as IBM PC compatibles.
Originally referred to as the PC bus (8-bit) or AT bus (16-bit), it was also termed I/O Channel by IBM. The ISA term was coined as a retronym by competing PC-clone manufacturers in the late 1980s or early 1990s as a reaction to IBM attempts to replace the AT-bus with its new and incompatible Micro Channel architecture.

https://en.wikipedia.org/wiki/Industry_Standard_Architecture

- **Apple Desktop Bus - chaining of 16 devices (1986) - Apple**

In 1986 Apple first implemented the Apple Desktop Bus allowing the daisy chaining of up to 16 devices, including mice and other devices on the same bus with no configuration whatsoever. Featuring only a single data pin, the bus used a purely polled approach to device communications and survived as the standard on mainstream models (including a number of non-Apple workstations) until 1998 when Apple's iMac line of computers joined the industry-wide switch to using USB. Beginning with the Bronze Keyboard PowerBook G3 in May 1999, Apple dropped the external ADB port in favor of USB, but retained an internal ADB connection in the PowerBook G4 for communication with its built-in keyboard and trackpad until early 2005.

https://en.wikipedia.org/wiki/Computer_mouse

- **IBM PS/2 - PS/2 for mouse and keyboard, UART serial port, VGA (1987) - IBM***

The Personal System/2 or PS/2 is IBM's second generation[2][3] of personal computers. Released in 1987, it officially replaced the IBM PC, XT, AT, and PC Convertible in IBM's lineup. Many of the PS/2's innovations, such as the 16550 UART (serial port), 1440 KB 3.5-inch floppy disk format, 72-pin SIMMs, the PS/2 port, and the VGA video standard, went on to become standards in the broader PC market.[4][5]

https://en.wikipedia.org/wiki/IBM_PS/2

Micro Channel architecture, or the Micro Channel bus, is a proprietary 16- or 32-bit parallel computer bus introduced by IBM in 1987 which was used on PS/2 and other computers until the mid-1990s. Its name is commonly abbreviated as "MCA", although not by IBM. In IBM products, it superseded the ISA bus and was itself subsequently superseded by the PCI bus architecture.

https://en.wikipedia.org/wiki/Extended_Industry_Standard_Architecture

With the arrival of the IBM PS/2 personal-computer series in 1987, IBM introduced the eponymous PS/2 port for mice and keyboards, which other manufacturers rapidly adopted. The most visible change was the use of a round 6-pin mini-DIN, in lieu of the former 5-pin MIDI style full sized DIN 41524 connector. In default mode (called stream mode) a PS/2 mouse communicates motion, and the state of each button, by means of 3-byte packets.[98] For any motion, button press or button release event, a PS/2 mouse sends, over a bi-directional serial port, a sequence of three bytes, with the following format:

https://en.wikipedia.org/wiki/Computer_mouse

- **EISA - Extended Industry Standard Architecture (1988) - IBM PC clones**

(in practice almost always shortened to EISA and frequently pronounced "eee-suh") is a bus standard for IBM PC compatible computers. It was announced in September 1988 by a consortium of PC clone vendors (the Gang of Nine) as an alternative to IBM's proprietary Micro Channel architecture (MCA) in its PS/2 series.[2]
In comparison with the AT bus, which the Gang of Nine retroactively renamed to the ISA bus to avoid infringing IBM's trademark on its PC/AT computer, EISA is extended to 32 bits and allows more than one CPU to share the bus. The bus mastering support is also enhanced to provide access to 4 GB of memory. Unlike MCA, EISA can accept older XT and ISA boards — the lines and slots for EISA are a superset of ISA.
EISA was much favoured by manufacturers due to the proprietary nature of MCA, and even IBM produced some machines supporting it
Gang of 9:
The Gang of Nine was the informal name given to the consortium of personal computer manufacturing companies that together created the EISA bus. Rival members generally acknowledged Compaq's leadership, with one stating in 1989 that within the Gang of Nine "when you have 10 people sit down before a table to write a letter to the president, someone has to write the letter. Compaq is sitting down at the typewriter".[6] The members were:[2]
  * AST Research, Inc.
  * Compaq Computer Corporation
  * Seiko Epson Corporation
  * Hewlett-Packard Company
  * NEC Corporation
  * Olivetti
  * Tandy Corporation
  * WYSE
  * Zenith Data Systems

https://en.wikipedia.org/wiki/Extended_Industry_Standard_Architecture

### Storage
**[`^        back to top        ^`](#)**

- **IBM PC/XT - Internal HDD - 10 MB - (1983) - IBM**

Most HDDs in the early 1980s were sold to PC end users as an external, add-on subsystem. The subsystem was not sold under the drive manufacturer's name but under the subsystem manufacturer's name such as Corvus Systems and Tallgrass Technologies, or under the PC system manufacturer's name such as the Apple ProFile. The IBM PC/XT in 1983 included an internal 10 MB HDD, and soon thereafter internal HDDs proliferated on personal computers.

External HDDs remained popular for much longer on the Apple Macintosh. Many Macintosh computers made between 1986 and 1998 featured a SCSI port on the back, making external expansion simple. Older compact Macintosh computers did not have user-accessible hard drive bays (indeed, the Macintosh 128K, Macintosh 512K, and Macintosh Plus did not feature a hard drive bay at all), so on those models external SCSI disks were the only reasonable option for expanding upon any internal storage.

https://en.wikipedia.org/wiki/Hard_disk_drive

- **Apple UniDisk 3.5 - 3"1/2 Floppy Disk (1985) - Apple**

In September 1985, Apple released its first 3+1⁄2-inch drive (A2M2053) for the Apple II series utilizing Sony's new 800-kilobyte double-sided drive mechanism, which would not be released for the Macintosh until four months later. The Apple UniDisk 3.5 drive contained additional circuitry making it an "intelligent" or "smart" drive; this made it incompatible with the Macintosh, despite having the identical mechanism that was to be later used in the Macintosh drive. 

https://en.wikipedia.org/wiki/Macintosh_External_Disk_Drive

- **Apple 800K External Drive - External Drive (1986) - Apple**

In January 1986, Apple introduced the Macintosh Plus which had a Sony double-sided 800-kilobyte capacity disk drive, and used the new HFS disk format providing directories and sub-directories. This drive was fitted into an external case as the Macintosh 800K External Drive (M0131), which was slimmer than the earlier 400-kilobyte drive.

https://en.wikipedia.org/wiki/Macintosh_External_Disk_Drive

- **Small Computer System Interface (SCSI) - (1986)**

Small Computer System Interface (SCSI, /ˈskʌzi/ SKUZ-ee)[1] is a set of standards for physically connecting and transferring data between computers and peripheral devices. The SCSI standards define commands, protocols, electrical, optical and logical interfaces. The SCSI standard defines command sets for specific peripheral device types; the presence of "unknown" as one of these types means that in theory it can be used as an interface to almost any device, but the standard is highly pragmatic and addressed toward commercial requirements. The initial Parallel SCSI was most commonly used for hard disk drives and tape drives, but it can connect a wide range of other devices, including scanners and CD drives, although not all controllers can handle all devices.
The ancestral SCSI standard, X3.131-1986, generally referred to as SCSI-1, was published by the X3T9 technical committee of the American National Standards Institute (ANSI) in 1986. SCSI-2 was published in August 1990 as X3.T9.2/86-109, with further revisions in 1994 and subsequent adoption of a multitude of interfaces. Further refinements have resulted in improvements in performance and support for ever-increasing data storage capacity.[2]

https://en.wikipedia.org/wiki/SCSI#Interfaces

- **Parallel ATA (PATA), originally AT Attachment, also known as ATA or IDE (1986) - Western Digital and Compaq**

a standard interface designed for IBM PC-compatible computers. It was first developed by Western Digital and Compaq in 1986 for compatible hard drives and CD or DVD drives

https://en.wikipedia.org/wiki/Parallel_ATA

### Uses
**[`^        back to top        ^`](#)**

#### Servers

- **Mainframes : graphic display terminals, terminal emulation (1980s)**
By the 1980s, many mainframes supported **graphic display terminals**, and **terminal emulation**, but not graphical user interfaces. 

https://en.wikipedia.org/wiki/Mainframe_computer


- **Bind  (1984) : First DNS server**. Still widely in use (BIND9) 

https://en.wikipedia.org/wiki/Domain_Name_System

- **BBS - Bulletin Board System (1973-1995)**

A bulletin board system or BBS (also called Computer Bulletin Board Service, CBBS[1]) is a computer server running software that allows users to connect to the system using a terminal program. Once logged in, the user can perform functions such as uploading and downloading software and data, reading news and bulletins, and exchanging messages with other users through public message boards and sometimes via direct chatting. In the early 1980s, message networks such as FidoNet were developed to provide services such as NetMail, which is similar to internet-based email.[2]

Many BBSes also offer online games in which users can compete with each other. BBSes with multiple phone lines often provide chat rooms, allowing users to interact with each other. Bulletin board systems were in many ways a precursor to the modern form of the World Wide Web, social networks, and other aspects of the Internet. Low-cost, high-performance asynchronous modems drove the use of online services and BBSes through the early 1990s. InfoWorld estimated that there were 60,000 BBSes serving 17 million users in the United States alone in 1994, a collective market much larger than major online services such as CompuServe.

The introduction of inexpensive dial-up internet service and the Mosaic web browser offered ease of use and global access that BBS and online systems did not provide, and led to a rapid crash in the market starting in late 1994-early 1995. Over the next year, many of the leading BBS software providers went bankrupt and tens of thousands of BBSes disappeared. Today, BBSing survives largely as a nostalgic hobby in most parts of the world, but it is still an extremely popular form of communication for Taiwanese youth (see PTT Bulletin Board System).[3] Most surviving BBSes are accessible over Telnet and typically offer free email accounts, FTP services, IRC and all the protocols commonly used on the Internet. Some offer access through packet switched networks or packet radio connections.[1]

https://en.wikipedia.org/wiki/Bulletin_board_system

#### Clients

- **PageMaker - Desktop publishing (1985) - Aldus**

PageMaker, developed by Aldus for the Macintosh and LaserWriter, was also released in 1985 and the combination became very popular for desktop publishing

https://en.wikipedia.org/wiki/Laser_printing

- **Bash - Unix Shell (1989)**

Bash is a Unix shell and command language written by Brian Fox for the GNU Project as a free software replacement for the Bourne shell.[13][14] First released in 1989,[15] it has been used as the default login shell for most Linux distributions.[16] Bash was one of the first programs Linus Torvalds ported to Linux, alongside GCC.[17] A version is also available for Windows 10 via the Windows Subsystem for Linux.[18] It is also the default user shell in Solaris 11.[19] Bash was also the default shell in versions of Apple macOS from 10.3 (originally default shell was tcsh) to the 2019 release of macOS Catalina, which changed the default shell to zsh, although Bash remains available as an alternative shell.[20]

Bash is a command processor that typically runs in a text window where the user types commands that cause actions. Bash can also read and execute commands from a file, called a shell script. Like most Unix shells, it supports filename globbing (wildcard matching), piping, here documents, command substitution, variables, and control structures for condition-testing and iteration. The keywords, syntax, dynamically scoped variables and other basic features of the language are all copied from sh. Other features, e.g., history, are copied from csh and ksh. Bash is a POSIX-compliant shell, but with a number of extensions.

The shell's name is an acronym for Bourne Again Shell, a pun on the name of the Bourne shell that it replaces[21] and the notion of being "born again".[22][23]

A security hole in Bash dating from version 1.03 (August 1989),[24] dubbed Shellshock, was discovered in early September 2014 and quickly led to a range of attacks across the Internet.[25][26][27] Patches to fix the bugs were made available soon after the bugs were identified.

Brian Fox began coding Bash on January 10, 1988,[28] after Richard Stallman became dissatisfied with the lack of progress being made by a prior developer.[13] Stallman and the Free Software Foundation (FSF) considered a free shell that could run existing shell scripts so strategic to a completely free system built from BSD and GNU code that this was one of the few projects they funded themselves, with Fox undertaking the work as an employee of FSF.[13][29] Fox released Bash as a beta, version .99, on June 8, 1989,[15] and remained the primary maintainer until sometime between mid-1992[30] and mid-1994,[31] when he was laid off from FSF[32] and his responsibility was transitioned to another early contributor, Chet Ramey.[33][34][35]

Since then, Bash has become by far the most popular shell among users of Linux, becoming the default interactive shell on that operating system's various distributions[36][37] (although Almquist shell may be the default scripting shell) and on Apple's macOS releases before Catalina in October 2019.[38][39][16] Bash has also been ported to Microsoft Windows and distributed with Cygwin and MinGW, to DOS by the DJGPP project, to Novell NetWare, to OpenVMS by the GNV project,[40] to ArcaOS,[41] and to Android via various terminal emulation applications.

In September 2014, Stéphane Chazelas, a Unix/Linux specialist,[42] discovered a security bug in the program. The bug, first disclosed on September 24, was named Shellshock and assigned the numbers CVE-2014-6271, CVE-2014-6277 and CVE-2014-7169. The bug was regarded as severe, since CGI scripts using Bash could be vulnerable, enabling arbitrary code execution. The bug was related to how Bash passes function definitions to subshells through environment variables.[43]

https://en.wikipedia.org/wiki/Bash_(Unix_shell)


#### CGI

- **Pixar early history**

In 1982, the Pixar team began working on special-effects film sequences with Industrial Light & Magic. After years of research, and key milestones such as the Genesis Effect in Star Trek II: The Wrath of Khan and the Stained Glass Knight in Young Sherlock Holmes,[14] the group, which then numbered 40 individuals, was spun out as a corporation in February 1986 by Catmull and Smith. Among the 38 remaining employees, there were also Malcolm Blanchard, David DiFrancesco, Ralph Guggenheim, and Bill Reeves, who had been part of the team since the days of NYIT. Tom Duff, also an NYIT member, would later join Pixar after its formation.[2] With Lucas's 1983 divorce, which coincided with the sudden dropoff in revenues from Star Wars licenses following the release of Return of the Jedi, they knew he would most likely sell the whole Graphics Group. Worried that the employees would be lost to them if that happened, which would prevent the creation of the first computer-animated movie, they concluded that the best way to keep the team together was to turn the group into an independent company. But Moore's Law also suggested that sufficient computing power for the first film was still some years away, and they needed to focus on a proper product until then. Eventually, they decided they should be a hardware company in the meantime, with their Pixar Image Computer as the core product, a system primarily sold to governmental, scientific, and medical markets.[2][10][19] They also used SGI computers.[20]

In 1983, Nolan Bushnell founded a new computer-guided animation studio called Kadabrascope as a subsidiary of his Chuck E. Cheese's Pizza Time Theatres company (PTT), which was founded in 1977. Only one major project was made out of the new studio, an animated Christmas special for NBC starring Chuck E. Cheese and other PTT mascots; known as "Chuck E. Cheese: The Christmas That Almost Wasn't". The animation movement would be made using tweening instead of traditional cel animation. After the video game crash of 1983, Bushnell started selling some subsidiaries of PTT to keep the business afloat. Sente Technologies (another division, was founded to have games distributed in PTT stores) was sold to Bally Games and Kadabrascope was sold to Lucasfilm. The Kadabrascope assets were combined with the Computer Division of Lucasfilm.[21] Coincidentally, one of Steve Jobs's first jobs was under Bushnell in 1973 as a technician at his other company Atari, which Bushnell sold to Warner Communications in 1976 to focus on PTT.[22] PTT would later go bankrupt in 1984 and be acquired by ShowBiz Pizza Place.[23]

In 1986, the newly independent Pixar was headed by President Edwin Catmull and Executive Vice President Alvy Ray Smith. Lucas's search for investors led to an offer from Steve Jobs, which Lucas initially found too low. He eventually accepted after determining it impossible to find other investors. At that point, Smith and Catmull had been declined 45 times, and 35 venture capitalists and ten large corporations had declined.[24] Jobs, who had been edged out of Apple in 1985,[2] was now founder and CEO of the new computer company NeXT. On February 3, 1986, he paid $5 million of his own money to George Lucas for technology rights and invested $5 million cash as capital into the company, joining the board of directors as chairman.[2][25]

In 1985, while still at Lucasfilm, they had made a deal with the Japanese publisher Shogakukan to make a computer-animated movie called Monkey, based on the Monkey King. The project continued sometime after they became a separate company in 1986, but it became clear that the technology was not sufficiently advanced. The computers were not powerful enough and the budget would be too high. So they focused on the computer hardware business for years until a computer-animated feature became feasible according to Moore's law.[26][27]

At the time, Walt Disney Studios was interested and eventually bought and used the Pixar Image Computer and custom software written by Pixar as part of its Computer Animation Production System (CAPS) project, to migrate the laborious ink and paint part of the 2D animation process to a more automated method. The company's first feature film to be released using this new animation method was The Rescuers Down Under (1990).[28][29]

In a bid to drive sales of the system and increase the company's capital, Jobs suggested releasing the product to the mainstream market. Pixar employee John Lasseter, who had long been working on not-for-profit short demonstration animations, such as Luxo Jr. (1986) to show off the device's capabilities, premiered his creations to great fanfare at SIGGRAPH, the computer graphics industry's largest convention.[30]

However, the Image Computer had inadequate sales[30] which threatened to end the company as financial losses grew. Jobs increased investment in exchange for an increased stake, reducing the proportion of management and employee ownership until eventually, his total investment of $50 million gave him control of the entire company. In 1989, Lasseter's growing animation department, originally composed of just four people (Lasseter, Bill Reeves, Eben Ostby, and Sam Leffler), was turned into a division that produced computer-animated commercials for outside companies.

https://en.wikipedia.org/wiki/Pixar#Early_history

- **Tin toy, 1989 Academy Award for Best Animated Short Film, first for a computer-generated film (1988) - Pixar**

At Pixar, Lasseter created short, computer-animated films to show off the Pixar Image Computer's capabilities. In 1988, Lasseter produced the short film **Tin Toy* told from the perspective of a toy, referencing Lasseter's love of classic toys. It **won the 1989 Academy Award for Best Animated Short Film, the first computer-generated film to do so.** [9]

Tin Toy gained Disney's attention, and the new team at The Walt Disney Company, CEO Michael Eisner and chairman Jeffrey Katzenberg in the film division, sought to get Lasseter to come back.[9] Lasseter, grateful for Jobs' faith in him, felt compelled to stay with Pixar, telling co-founder Ed Catmull, "I can go to Disney and be a director, or I can stay here and make history."[9] Katzenberg realized he could not lure Lasseter back to Disney and therefore set plans into motion to ink a production deal with Pixar to produce a film. [9] Disney had always made all their movies in-house and refused to change this. But when Tim Burton, who used to work at Disney, wanted to buy back the rights to The Nightmare Before Christmas, Disney struck a deal allowing him to make it as a Disney film outside the studio. This allowed Pixar to make their movies outside Disney.[10]

https://en.wikipedia.org/wiki/Toy_Story

## Consumer Electronics
**[`^        back to top        ^`](#)**
### Gadgets
**[`^        back to top        ^`](#)**

- **Psion - First personal digital assistant (PDA) / handheld PC (1984) - Psion**

A personal digital assistant (PDA), also known as a handheld PC,[1][2] is a variety mobile device which functions as a personal information manager. PDAs have been mostly displaced by the widespread adoption of highly capable smartphones, in particular those based on iOS and Android.[3]

A PDA has an electronic visual display. Most models also have audio capabilities, allowing usage as a portable media player, and also enabling many of them to be used as telephones. Nearly all modern PDAs can access the Internet, intranets or extranets via Wi-Fi or Wireless WANs, letting them include a web browser. Sometimes, instead of buttons, PDAs employ touchscreen technology.

The first PDA, the Organiser, was released in 1984 by Psion, followed by Psion's Series 3,

https://en.wikipedia.org/wiki/Personal_digital_assistant

- **First graphing calculator (1985) - Casio**

The first graphing calculator was the Casio fx-7000G released in 1985.

https://en.wikipedia.org/wiki/Calculator

- **First symbolic computing calculator (1987) - HP**

The first calculator capable of symbolic computing was the HP-28C, released in 1987. It could, for example, solve quadratic equations symbolically. 

https://en.wikipedia.org/wiki/Calculator

### Multimedia
**[`^        back to top        ^`](#)**

- **Starcom Cable TV Converter - infrared LED remote controls (1980) - Jerrold Electronics / General Instrument**

In 1980, the most popular remote control was the Starcom Cable TV Converter[25] (from Jerrold Electronics, a division of General Instrument)[13] which used 40-kHz sound to change channels. Then, a Canadian company, Viewstar, Inc., was formed by engineer Paul Hrivnak and started producing a cable TV converter with an infrared remote control. The product was sold through Philips for approximately $190 CAD. The Viewstar converter was an immediate success, the millionth converter being sold on March 21, 1985, with 1.6 million sold by 1989.[26][27]

https://en.wikipedia.org/wiki/Remote_control#Infrared,_line_of_sight_and_operating_angle

- **CD - Compact Disc (1982) - Philips and Sony**

The compact disc (CD) is a digital optical disc data storage format that was co-developed by Philips and Sony to store and play digital audio recordings. In August 1982, the first compact disc was manufactured. It was then released in October 1982 and branded as Digital Audio Compact Disc.

The format was later adapted for storage of data (CD-ROM). Several other formats were further derived from these, including write-once audio and data storage (CD-R), rewritable media (CD-RW), Video CD (VCD), Super Video CD (SVCD), Photo CD, Picture CD, Compact Disc-Interactive (CD-i) and Enhanced Music CD.

Standard CDs have a diameter of 120 millimetres (4.7 in) and are designed to hold up to 74 minutes of uncompressed stereo digital audio or about 650 MiB of data. Capacity is routinely extended to 80 minutes and 700 MiB by arranging data more closely on the same sized disc. The Mini CD has various diameters ranging from 60 to 80 millimetres (2.4 to 3.1 in); they are sometimes used for CD singles, storing up to 24 minutes of audio, or delivering device drivers.

At the time of the technology's introduction in 1982, a CD could store much more data than a personal computer hard disk drive, which would typically hold 10 MiB. By 2010, hard drives commonly offered as much storage space as a thousand CDs, while their prices had plummeted to commodity level. In 2004, worldwide sales of audio CDs, CD-ROMs, and CD-Rs reached about 30 billion discs. By 2007, 200 billion CDs had been sold worldwide.[2]

https://en.wikipedia.org/wiki/Compact_disc

- **Pocket LCD TVs (1982) - Seiko Epson**

The first color LCD televisions were developed as handheld televisions in Japan. In 1980, Hattori Seiko's R&D group began development on color LCD pocket televisions.[72] In 1982, Seiko Epson released the first LCD television, the Epson TV Watch, a wristwatch equipped with a small active-matrix LCD television.[73][74] Sharp Corporation introduced dot matrix TN-LCD in 1983.[62] In 1984, Epson released the ET-10, the first full-color, pocket LCD television.[75] The same year, Citizen Watch,[76] introduced the Citizen Pocket TV,[72] a 2.7-inch color LCD TV,[76] with the first commercial TFT LCD.[72] In 1988, Sharp demonstrated a 14-inch, active-matrix, full-color, full-motion TFT-LCD. This led to Japan launching an LCD industry, which developed large-size LCDs, including TFT computer monitors and LCD televisions.[77] Epson developed the 3LCD projection technology in the 1980s, and licensed it for use in projectors in 1988.[78] Epson's VPJ-700, released in January 1989, was the world's first compact, full-color LCD projector.[74]

https://en.wikipedia.org/wiki/Liquid-crystal_display

By 1982, pocket LCD TVs based on LCD technology were developed in Japan.[15] The 2.1-inch Epson ET-10[16] Epson Elf was the first color LCD pocket TV, released in 1984.[17] 

https://en.wikipedia.org/wiki/Flat-panel_display

- **Sony CDP-101 - Audio compact disk player (1982) - Sony**

Compact Disc Digital Audio (CDDA or CD-DA), also known as Digital Audio Compact Disc or simply as Audio CD, is the standard format for audio compact discs. The standard is defined in the Red Book, one of a series of Rainbow Books (named for their binding colors) that contain the technical specifications for all CD formats.

The first commercially available audio CD player, the Sony CDP-101, was released October 1982 in Japan. The format gained worldwide acceptance in 1983–84, selling more than a million CD players in those two years, to play 22.5 million discs.[1]

Beginning in the 2000s, CDs were increasingly being replaced by other forms of digital storage and distribution, with the result that by 2010 the number of audio CDs being sold in the U.S. had dropped about 50% from their peak; however, they remained one of the primary distribution methods for the music industry.

https://en.wikipedia.org/wiki/Compact_Disc_Digital_Audio

### Screens
**[`^        back to top        ^`](#)**

- **Orange monochrome plasma displays for laptops (1983) - IBM**

In 1983, IBM introduced a 19-inch (48 cm) orange-on-black monochrome display (Model 3290 Information Panel) which was able to show up to four simultaneous IBM 3270 terminal sessions. By the end of the decade, orange monochrome plasma displays were used in a number of high-end AC-powered portable computers, such as the Compaq Portable 386 (1987) and the IBM P75 (1990). Plasma displays had a better contrast ratio, viewability angle, and less motion blur than the LCDs that were available at the time, and were used until the introduction of active-matrix color LCD displays in 1992.[59]

Due to heavy competition from monochrome LCDs used in laptops and the high costs of plasma display technology, in 1987 IBM planned to shut down its factory in Kingston, New York, the largest plasma plant in the world, in favor of manufacturing mainframe computers, which would have left development to Japanese companies.[60] Dr. Larry F. Weber, a University of Illinois ECE PhD (in plasma display research) and staff scientist working at CERL (home of the PLATO System), co-founded Plasmaco with Stephen Globus and IBM plant manager James Kehoe, and bought the plant from IBM for US$50,000. Weber stayed in Urbana as CTO until 1990, then moved to upstate New York to work at Plasmaco.

https://en.wikipedia.org/wiki/Plasma_display

- **Flat screen CRT TV (1987) - Zenith**

In 1987, flat-screen CRTs were developed by Zenith for computer monitors, reducing reflections and helping increase image contrast and brightness.[63][64] Such CRTs were expensive, which limited their use to computer monitors.[65] Attempts were made to produce flat-screen CRTs using inexpensive and widely available float glass.[66]

https://en.wikipedia.org/wiki/Cathode-ray_tube

- **SVGA Graphics Cards (1988)**

In the late 1980s, after the release of IBM's VGA, third-party manufacturers began making graphics cards based on its specifications with extended capabilities. As these cards grew in popularity they began to be referred to as "Super VGA."
This term was not an official standard, but a shorthand for enhanced VGA cards which had become common by 1988.[1] One card that explicitly used the term was **Genoa's SuperVGA HiRes.**[3]
Super VGA cards broke compatibility with the IBM VGA standard, requiring software developers to provide specific display drivers and implementations for each card their software could operate on.

https://en.wikipedia.org/wiki/Super_VGA


### Broadcast
**[`^        back to top        ^`](#)**

- **TVRO/C-band satellite era (1980–1986)**

On 26 April 1982, the first satellite channel in the UK, Satellite Television Ltd. (later Sky One), was launched.[68] Its signals were transmitted from the ESA's Orbital Test Satellites
Originally, all channels were broadcast in the clear (ITC) because the equipment necessary to receive the programming was too expensive for consumers. With the growing number of TVRO systems, the program providers and broadcasters had to scramble their signal and develop subscription systems.
In January 1986, HBO began using the now-obsolete VideoCipher II system to encrypt their channels.
On 11 December 1988 Luxembourg launched Astra 1A, the first satellite to provide medium power satellite coverage to Western Europe.[75] This was one of the first medium-powered satellites, transmitting signals in Ku band and allowing reception with small(90 cm) dishes for the first time ever.

https://en.wikipedia.org/wiki/Satellite_television

- **HD-MAC - last analog HDTV system (1986)**

In 1986, the European Community proposed HD-MAC, an analog HDTV system with 1,152 lines. A public demonstration took place for the 1992 Summer Olympics in Barcelona. However HD-MAC was scrapped in 1993 and the Digital Video Broadcasting (DVB) project was formed, which would foresee development of a digital HDTV standard.[7]

https://en.wikipedia.org/wiki/High-definition_television

### Video games
**[`^        back to top        ^`](#)**

- **Golden age : Move of arcade games to microprocessors (1978-1983)**

As technology moved from transistor-transistor logic (TTL) integrated circuits to microprocessors, a new wave of arcade video games arose, starting with Taito's **Space Invaders** in 1978 and leading to a golden age of arcade video games that included **Pac-Man** (Namco, 1980), **Missile Command** (Atari, 1980), and **Donkey Kong (Nintendo, 1981)**. The golden age waned in 1983 due to an excess number of arcade games, the growing draw of home video game consoles and computers, and a moral panic on the impact of arcade video games on youth.[22][57] The arcade industry was also partially impacted by the video game crash of 1983.

https://en.wikipedia.org/wiki/Arcade_game#History

Space Invaders led off what is considered to be the golden age of arcade games which lasted from 1978 to 1982. Several influential and best-selling arcade games were released during this period from Atari, Namco, Taito, Williams, and Nintendo, including Asteroids (1979), Galaxian (1979), Defender (1980), Missile Command (1980), Tempest (1981), and Galaga (1981). Pac-Man, released in 1980, became a popular culture icon, and a new wave of games appeared that focused on identifiable characters and alternate mechanics such as navigating a maze or traversing a series of platforms. Aside from Pac-Man and its sequel, Ms. Pac-Man (1982), the most popular games in this vein during the golden age were Donkey Kong (1981) and Q*bert (1982).[14] Games like Pac-Man, Donkey Kong and Q*bert also introduced the **concept of narratives and characters to video games**, which led companies to adopt these later as mascots for marketing purposes.[17][18]

https://en.wikipedia.org/wiki/History_of_video_games

- **Second generation consoles, shift to Japan and new gaming industry model**

At the start of the second generation, all games were developed and produced in-house. Four former Atari programmers, having left from conflicts in management style after Atari was purchased by Warner Communications in 1976, established Activision in 1979 to develop their own VCS games, which included Kaboom! and Pitfall!. Atari sued Activision on the basis of theft of trade secrets and violation of their non-disclosure agreements, but the two companies settled out of court in 1982, with Activision agreeing to pay a small license fee to Atari for every game of theirs their sold. This established Activision as the first third-party developer for a console. This also established a working model for licensing other third-party developers, which several companies followed in Activision's wake, partially contributing to the video game crash of 1983 due to oversaturation.[19]

As the second generation of consoles coincided with the golden age of arcade video games, a common trend that emerged during the generation was licensing arcade video games for consoles. Many of them were increasingly licensed from Japanese video game companies by 1980, which led to Jonathan Greenberg of Forbes predicting in early 1981 that Japanese companies would eventually dominate the North American video game industry later in the decade.[20]

At this stage, both consoles and game cartridges were intended to be sold for profit by manufacturers. However, by segregating games from the console, this approach established the use of the razorblade business model in future console generations, where consoles would be sold at or below cost while licensing fees from third-party games would bring in profits.[21][22]

https://en.wikipedia.org/wiki/Second_generation_of_video_game_consoles

- **Mystery House - First adventure game with a GUI, horror game**

Mystery House is an adventure game released by On-Line Systems in 1980. It was designed, written and illustrated by Roberta Williams, and programmed by Ken Williams for the Apple II.[1] Mystery House is the first graphical adventure game and the first game produced by On-Line Systems, the company which would evolve into Sierra On-Line.[2] It is one of the earliest horror video games.[3]

https://en.wikipedia.org/wiki/Mystery_House

- **8 bits game consoles - Third generation video games consoles (1983)**

In the history of video games, the third generation of game consoles, commonly referred to as the 8-bit era, began on July 15, 1983 with the Japanese release of two systems: Nintendo's Family Computer (commonly abbreviated to **Famicom**) and **Sega's SG-1000**.[1][2] When the Famicom was released outside of Japan it was remodelled and marketed as the **Nintendo Entertainment System (NES)**. This generation marked the end of the video game crash of 1983, and a shift in the dominance of home video game manufacturers from the United States to Japan.[3] Handheld consoles were not a major part of this generation, although the Game & Watch line from Nintendo (which started in 1980) and the Milton Bradley Microvision (which came out in 1979) were sold at the time. However, both are considered second generation hardware.
The best-selling console of this generation was the **NES/Famicom** from Nintendo, followed by the **Sega Master System** (the improved successor to the SG-1000), and the **Atari 7800**. Although the previous generation of consoles had also used 8-bit processors, it was at the end of the third generation that home consoles were first labeled and marketed by their "bits". 

https://en.wikipedia.org/wiki/Third_generation_of_video_game_consoles

- **16 bits game consoles - Fourth generation video games consoles (1987)**

In the history of video games, the fourth generation of game consoles, more commonly referred to as the 16-bit era, began on October 30, 1987, with the Japanese release of **NEC Home Electronics**' PC Engine (known as the TurboGrafx-16 in North America).
Although NEC released the first console of this era, sales were mostly dominated by the rivalry between Sega and Nintendo across most markets: the Sega Mega Drive (named the Sega Genesis in North America) and the **Super Nintendo Entertainment System **(SNES; the Super Famicom in Japan). Cartridge-based handheld consoles became prominent during this time, dominated by the **Nintendo Game Boy** (1989). Color handhelds were also released, including the Atari Lynx (1989) and Sega Game Gear (1990).

The first handheld game console released in the fourth generation was the Game Boy, on April 21, 1989. It went on to dominate handheld sales by an extremely large margin, despite featuring a 8-bit microprocessor and a low-contrast, unlit monochrome screen while all three of its leading competitors had color. Three major franchises made their debut on the Game Boy: Tetris, the Game Boy's killer application; Pokémon; and Kirby. With some design (Game Boy Pocket, Game Boy Light) and hardware (Game Boy Color) changes, it continued in production in some form until 2008, enjoying a better than 18-year run.
https://en.wikipedia.org/wiki/Fourth_generation_of_video_game_consoles

- **Nintendo Gameboy (1989) - Nintendo**
Cartridge-based handheld consoles became prominent during this time, dominated by the Nintendo Game Boy (1989).
Color handhelds were also released, including the Atari Lynx (1989) and Sega Game Gear (1990).
https://en.wikipedia.org/wiki/Fourth_generation_of_video_game_consoles

- **Recovery and decline of the arcade market (1986-1989)**

The arcade market had recovered by 1986, with the help of software conversion kits, the arrival of popular **beat 'em up games** (such as Kung-Fu Master and Renegade), and advanced motion simulator games (such as Sega's "taikan" games including Hang-On, Space Harrier and Out Run). However, the growth of home video game systems such as the Nintendo Entertainment System led to another brief arcade decline towards the end of the 1980s

https://en.wikipedia.org/wiki/Arcade_game#History

## Standards and protocols
**[`^        back to top        ^`](#)**

### Physical layer
- **G.652 (1984)** : 

international standard that describes the geometrical, mechanical, and transmission attributes of a **single-mode optical fibre and cable**, developed by the Standardization Sector of the International Telecommunication Union (ITU-T) that specifies the most popular type of single-mode optical fiber (SMF) cable

https://en.wikipedia.org/wiki/G.652

- **IEEE 802.3 (1983) - 10BASE5**
IEEE 802.3 is a working group and a collection of Institute of Electrical and Electronics Engineers (IEEE) standards produced by the working group defining the physical layer and data link layer's media access control (MAC) of wired Ethernet. This is generally a local area network (LAN) technology with some wide area network (WAN) applications. Physical connections are made between nodes and/or infrastructure devices (hubs, switches, routers) by various types of copper or fiber cable.

802.3 is a technology that supports the IEEE 802.1 network architecture.

802.3 also defines LAN access method using CSMA/CD.

IEEE 802.3 standard	1983	10BASE5 10 Mbit/s (1.25 MB/s) over thick coax. Same as Ethernet II (above) except Type field is replaced by Length, and an 802.2 LLC header follows the 802.3 header. Based on the CSMA/CD Process.

https://en.wikipedia.org/wiki/IEEE_802.3

### Link layer

- **802.3d	-	Fiber-optic inter-repeater link, replaced by 10BASE-FL in 1993 (1987)**

10BASE-F, or sometimes 10BASE-FX, is a generic term for the family of 10 Mbit/s Ethernet standards using fiber optic cable. In 10BASE-F, the 10 represents a maximum throughput of 10 Mbit/s, BASE indicates its use of baseband transmission, and F indicates that it relies on medium of fiber-optic cable. The technical standard requires two strands of 62.5/125 µm multimode fiber. 

Fiber-optic inter-repeater link (FOIRL) is a specification of Ethernet over optical fiber. It was especially designed as a back-to-back transport between repeater hubs as to decrease latency and collision detection time, thus increasing the possible network radius. It was replaced by 10BASE-FL.[1]

https://en.wikipedia.org/wiki/Classic_Ethernet#FOIRL

- **ARP - Address Resolution Protocol (1982)**

communication protocol used for discovering the link layer address, such as a MAC address, associated with a given internet layer address, typically an IPv4 address.  Essential to the internet.

https://en.wikipedia.org/wiki/Address_Resolution_Protocol

### Network layer
- **IPv4 - Internet Protocol version 4 (1981)**

the fourth version of the Internet Protocol (IP). Described in IETF publication RFC 791 (September 1981). First version deployed for production on SATNET in 1982 and on the ARPANET in January 1983. Essential to networking and the internet. 

https://en.wikipedia.org/wiki/IPv4

- **BGP - Border Gateway Protocol (1989)** :  exterior gateway protocol designed to exchange routing and reachability information among autonomous systems (AS) on the Internet. Essential to the internet. https://en.wikipedia.org/wiki/Border_Gateway_Protocol

- **IGMP - Internet Group Management Protocol (1989)** : a communications protocol used by hosts and adjacent routers on IPv4 networks to establish multicast group memberships. IGMP is an integral part of IP multicast and allows the network to direct multicast transmissions only to hosts that have requested them.
IGMP can be used for one-to-many networking applications such as online streaming video and gaming, and allows more efficient use of resources when supporting these types of applications.
https://en.wikipedia.org/wiki/Internet_Group_Management_Protocol

### Transport layer
- **UDP - User_Datagram_Protocol (1980)** : Simple message-oriented transport layer protocol (reception is not certain).  Essential to the internet. 
https://en.wikipedia.org/wiki/User_Datagram_Protocol 

### Session layer

- **NetBIOS (1983-1986)**

NetBIOS (/ˈnɛtbaɪɒs/) is an acronym for Network Basic Input/Output System. It provides services related to the session layer of the OSI model allowing applications on separate computers to communicate over a local area network. As strictly an API, NetBIOS is not a networking protocol. Older operating systems[clarification needed] ran NetBIOS over IEEE 802.2 and IPX/SPX using the NetBIOS Frames (NBF) and NetBIOS over IPX/SPX (NBX) protocols, respectively. In modern networks, NetBIOS normally runs over TCP/IP via the NetBIOS over TCP/IP (NBT) protocol. This results in each computer in the network having both an IP address and a NetBIOS name corresponding to a (possibly different) host name. NetBIOS is also used for identifying system names in TCP/IP (Windows). Simply saying, it is a protocol that allows communication of files and printers through the Session Layer of the OSI Model in a LAN.[clarification needed]

NetBIOS is a non-routable OSI Session Layer 5 Protocol and a service that allows applications on computers to communicate with one another over a local area network (LAN). NetBIOS was developed in 1983 by Sytek Inc. as an API for software communication over IBM PC Network LAN technology.[1] On IBM PC Network, as an API alone, NetBIOS relied on proprietary Sytek networking protocols for communication over the wire.[citation needed] Despite supporting a maximum of 80 PCs in a LAN, NetBIOS became an industry standard.[1]

In 1985, IBM went forward with the Token Ring network scheme and a NetBIOS emulator was produced to allow NetBIOS-aware applications from the PC-Network era to work over this new design. This emulator, named NetBIOS Extended User Interface (NetBEUI), expanded the base NetBIOS API with, among other things, the ability to deal with the greater node capacity of Token Ring. A new networking protocol, NBF, was simultaneously produced to allow NetBEUI (NetBIOS) to provide its services over Token Ring – specifically, at the IEEE 802.2 Logical Link Control layer.

In 1985, Microsoft created a NetBIOS implementation for its MS-Net networking technology. As in the case of IBM's Token Ring, the services of Microsoft's NetBIOS implementation were provided over the IEEE 802.2 Logical Link Control layer by the NBF protocol.[citation needed] Until Microsoft adopted Domain Name System (DNS) resolution of hostnames, Microsoft operating systems used NetBIOS to resolve names in Windows client-server networks.[1]

In 1986, Novell released Advanced Novell NetWare 2.0 featuring the company's own NetBIOS emulator. Its services were encapsulated within NetWare's IPX/SPX protocol using the NetBIOS over IPX/SPX (NBX) protocol.

In 1987, a method of encapsulating NetBIOS in TCP and UDP packets, NetBIOS over TCP/IP (NBT), was published. It was described in RFC 1001 ("Protocol Standard for a NetBIOS Service on a TCP/UDP Transport: Concepts and Methods") and RFC 1002 ("Protocol Standard for a NetBIOS Service on a TCP/UDP Transport: Detailed Specifications"). The NBT protocol was developed in order to "allow an implementation [of NetBIOS applications] to be built on virtually any type of system where the TCP/IP protocol suite is available," and to "allow NetBIOS interoperation in the Internet."

After the PS/2 computer hit the market in 1987, IBM released the PC LAN Support Program, which included a driver for NetBIOS.

Since its original publishing in a technical reference book from IBM, the NetBIOS API specification has become a de facto standard.

NetBIOS provides three distinct services:
- Name service (NetBIOS-NS) for name registration and resolution.
- Datagram distribution service (NetBIOS-DGM) for connectionless communication.
- Session service (NetBIOS-SSN) for connection-oriented communication.

https://en.wikipedia.org/wiki/NetBIOS

### Application layer
- **TFTP - Trivial File Transfer Protoco (1981)**

a simple lockstep File Transfer Protocol which allows a client to get a file from or put a file onto a remote host. One of its primary uses is in the early stages of nodes booting from a local area network. 
Insecure (no login or access control mechanism) but still in use. was mostly superseded by FTP. 

https://en.wikipedia.org/wiki/Trivial_File_Transfer_Protocol

- **NICNAME/WHOIS - WHOIS (1982)** 
 
Who is responsible for the domain, IP. Essential to the internet. 
 
https://en.wikipedia.org/wiki/WHOIS
 
- **NTP - Network_Time_Protocol (1985)** 

used for network time coordination between computers. Essential to networking.

https://en.wikipedia.org/wiki/Network_Time_Protocol

- **DNS - domain name server (1987)**

hierarchical and decentralized naming system used to identify computers reachable through the Internet or other Internet Protocol (IP) networks. The resource records contained in the DNS associate domain names with other forms of information. These are most commonly used to map human-friendly domain names to the numerical IP addresses computers.
Essential to the internet and networking

https://en.wikipedia.org/wiki/Domain_Name_System

- **SNMP - Simple Network Management Protocol (1988)**

protocol for collecting and organizing information about managed devices on IP networks and for modifying that information to change device behaviour. Devices that typically support SNMP include cable modems, routers, switches, servers, workstations, printers, and more. Widely used in network management for network monitoring.

https://en.wikipedia.org/wiki/Simple_Network_Management_Protocol

### Data transmission

#### Text transmission
- **SMTP - Email sending protocol (1980)**.

Essential to Emails. 
https://en.wikipedia.org/wiki/Simple_Mail_Transfer_Protocol

- **ASN.1 - Abstract Syntax Notation One (1984)**

A standard interface description language for defining data structures that can be serialized and deserialized in a cross-platform way. It is broadly used in telecommunications and computer networking, and especially in cryptography.[1]

https://en.wikipedia.org/wiki/ASN.1

- **POP1 - Post Office Protocol (1984)**

Email reception protocol. Essential to Emails (POP3). 

https://en.wikipedia.org/wiki/Post_Office_Protocol

- **RARP - Reverse Address Resolution Protocol (1984)**
 
for the configuration of simple devices, replaced by BOOTP 

https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol

- **X.400 (1984)**

a suite of ITU-T Recommendations that defines the ITU-T **Message Handling System (MHS)**. At one time, the designers of X.400 were expecting it to be the predominant form of email, but this role has been taken by the SMTP-based Internet e-mail.

https://en.wikipedia.org/wiki/X.400

- **IMAP2 : Internet Message Access Protocol (1988)** 

email synchronization protocol. does not delete automatically emails on the server. Essential to emails (IMAP4) 
https://en.wikipedia.org/wiki/Internet_Message_Access_Protocol

- **WWW (1989)**

Research at CERN in Switzerland by the British computer scientist Tim Berners-Lee in 1989–90 resulted in the World Wide Web, linking hypertext documents into an information system, accessible from any node on the network.

https://en.wikipedia.org/wiki/History_of_the_Internet

#### Images

- **BMP - Bitmap image file, GIF ancestor (1981) - IBM and Microsoft**

The BMP file format, also known as bitmap image file, device independent bitmap (DIB) file format and bitmap, is a raster graphics image file format used to store bitmap digital images, independently of the display device (such as a graphics adapter), especially on Microsoft Windows[2] and OS/2[3] operating systems.
The BMP file format is capable of storing two-dimensional digital images both monochrome and color, in various color depths, and optionally with data compression, alpha channels, and color profiles. The Windows Metafile (WMF) specification covers the BMP file format.[4]

https://en.wikipedia.org/wiki/BMP_file_format

Microsoft Corporation and IBM Corporation needed to record images in a format that their applications and operating systems could easily  render on low-end machines (Intel 80286).  The resulting "BMP" format contains a single raster graphic with basic header fields that can be
easily mapped (or "blitted") to locations in memory.  As computing moved from 16-bit to 32-bit, BMP evolved to contain 32-bit structures. 

https://datatracker.ietf.org/doc/html/rfc7903

- **ILBM - Interleaved Bitmap (1985) - Electronic Arts**

Interleaved Bitmap (ILBM) is an image file format conforming to the Interchange File Format (IFF) standard. The format originated on the Amiga platform, and on IBM-compatible systems, files in this format or the related PBM (Planar Bitmap) format are typically encountered in games from late 1980s and early 1990s that were either Amiga ports or had their graphical assets designed on Amiga machines.[citation needed]
A characteristic feature of the format is that it stores bitmaps in the form of interleaved bit planes, which gives the format its name; this reflects the way the Amiga graphics hardware natively reads graphics data from memory. A simple form of compression is supported to make ILBM files more compact.[4]
On the Amiga, these files are not associated with a particular file extension, though as they started being used on PC systems where extensions are systematically used, they employed a .lbm or occasionally a .bbm extension.

https://en.wikipedia.org/wiki/ILBM

- **GIF - Bitmap image file, PNG ancestor (1987)**

Graphics Interchange Format (GIF; /ɡɪf/ GHIF or /dʒɪf/ JIF , see pronunciation) is a bitmap image format that was developed by a team at the online services provider CompuServe led by American computer scientist Steve Wilhite and released on 15 June 1987

https://en.wikipedia.org/wiki/GIF

#### Audio and Video

- **IFF - Interchange File Format (1985) - Electronic Arts**

Interchange File Format (IFF), is a generic container file format originally introduced by the Electronic Arts company in 1985 (in cooperation with Commodore) in order to facilitate transfer of data between software produced by different companies.
IFF files do not have any standard extension. On many systems that generate IFF files, file extensions are not important (the OS stores file format metadata separately from the file name). An .iff extension is commonly used for ILBM format files, which use the IFF container format.

https://en.wikipedia.org/wiki/Interchange_File_Format

- **AIFF - Audio Interchange File Format (1988) - Apple**

Audio Interchange File Format (AIFF) is an audio file format standard used for storing sound data for personal computers and other electronic audio devices. The format was developed by Apple Inc. in 1988 based on Electronic Arts' Interchange File Format (IFF, widely used on Amiga systems) and is most commonly used on Apple Macintosh computer systems.

The audio data in most AIFF files is uncompressed pulse-code modulation (PCM). This type of AIFF file uses much more disk space than lossy formats like MP3—about 10 MB for one minute of stereo audio at a sample rate of 44.1 kHz and a bit depth of 16 bits. There is also a compressed variant of AIFF known as AIFF-C or AIFC, with various defined compression codecs.

https://en.wikipedia.org/wiki/Audio_Interchange_File_Format

#### Video transmission
- **H.120 (1984)** - **first digital video compression standard**. 

It was developed by COST 211 and published by the CCITT (now the ITU-T) in 1984, with a revision in 1988.

https://en.wikipedia.org/wiki/H.120

the first digital video coding standard. v1 (1984) featured conditional replenishment, differential PCM (DPCM), scalar quantization, variable-length coding and a switch for quincunx sampling. v2 (1988) added motion compensation and background prediction. This standard was little-used and no codecs exist.
https://en.wikipedia.org/wiki/Video_Coding_Experts_Group

- **H.261(1988) - first digital video compression standard of practical use**

H.261 is an ITU-T video compression standard, first ratified in November 1988.[1][2] It is the first member of the H.26x family of video coding standards in the domain of the ITU-T Study Group 16 Video Coding Experts Group (VCEG, then Specialists Group on Coding for Visual Telephony). It was the first video coding standard that was useful in practical terms.

H.261 was originally designed for transmission over ISDN lines on which data rates are multiples of 64 kbit/s. The coding algorithm was designed to be able to operate at video bit rates between 40 kbit/s and 2 Mbit/s. The standard supports two video frame sizes: CIF (352×288 luma with 176×144 chroma) and QCIF (176×144 with 88×72 chroma) using a 4:2:0 sampling scheme. It also has a backward-compatible trick for sending still images with 704×576 luma resolution and 352×288 chroma resolution (which was added in a later revision in 1993).

https://en.wikipedia.org/wiki/H.261

#### File storage

- **ISO 9660 - CD-ROM file system (1988)**

ISO 9660 (also known as ECMA-119) is a file system for optical disc media. Being sold by the International Organization for Standardization (ISO) the file system is considered an international technical standard. Since the specification is available for anybody to purchase,[1] implementations have been written for many operating systems.

https://en.wikipedia.org/wiki/ISO_9660

#### File compression

- **LZW Algorithm (1984)**

Lempel–Ziv–Welch (LZW) is a universal lossless data compression algorithm created by Abraham Lempel, Jacob Ziv, and Terry Welch. It was published by Welch in 1984 as an improved implementation of the LZ78 algorithm published by Lempel and Ziv in 1978. The algorithm is simple to implement and has the potential for very high throughput in hardware implementations.[1] It is the algorithm of the widely used Unix file compression utility compress and is used in the GIF image format.

https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Welch

- **ARC (1986)**

ARC is a lossless data compression and archival format by System Enhancement Associates (SEA). The file format and the program were both called ARC. The format is known as the subject of controversy in the 1980s, part of important debates over what would later be known as open formats.

ARC was extremely popular during the early days of the dial-up BBS. ARC was convenient as it combined the functions of the SQ program to compress files and the LU program to create .LBR archives of multiple files. The format was later replaced by the ZIP format, which offered better compression ratios and the ability to retain directory structures through the compression/decompression process.

In 1985, Thom Henderson of System Enhancement Associates wrote a program called ARC,[3] based on earlier programs such as ar, that not only grouped files into a single archive file but also compressed them to save disk space, a feature of great importance on early personal computers, where space was very limited and modem transmission speeds were very slow. The archive files produced by ARC had file names ending in ".ARC" and were thus sometimes called "arc files".

The source code for ARC was released by SEA in 1986 and subsequently ported to Unix and Atari ST in 1987 by Howard Chu. This more portable codebase was subsequently ported to other platforms, including VAX/VMS and IBM System/370 mainframes. Howard's work was also the first to disprove the prevalent belief that Lempel-Ziv encoded files could not be further compressed. Additional compression could be achieved by using Huffman coding on the LZW data, and Howard's version of ARC was the first program to demonstrate this property. This hybrid technique was later used in several other compression schemes by Phil Katz and others.

https://en.wikipedia.org/wiki/ARC_(file_format)

- **PKZIP (1986) - PKWARE**

PKZIP is a file archiving computer program, notable for introducing the popular ZIP file format. PKZIP was first introduced for MS-DOS on the IBM-PC compatible platform in 1989. Since then versions have been released for a number of other architectures and operating systems. PKZIP was originally written by Phil Katz and marketed by his company PKWARE, Inc starting in 1986. The company bears his initials: 'PK'.

https://en.wikipedia.org/wiki/PKZIP

- **ZIP (1989)**

ZIP is an archive file format that supports lossless data compression. A ZIP file may contain one or more files or directories that may have been compressed. The ZIP file format permits a number of compression algorithms, though DEFLATE is the most common. This format was originally created in 1989 and was first implemented in PKWARE, Inc.'s PKZIP utility,[2] as a replacement for the previous ARC compression format by Thom Henderson.

https://en.wikipedia.org/wiki/ZIP_(file_format)

#### Remote access
- **X Window System (X11, or simply X) (1984)**

A windowing system for bitmap displays, common on Unix-like operating systems. Originated as part of Project Athena at Massachusetts Institute of Technology (MIT). At version 11 since 1987.

https://en.wikipedia.org/wiki/X_Window_System

#### Instant messaging
- **IRC : Internet Relay Chat (1988)**

text-based chat system for instant messaging.

https://en.wikipedia.org/wiki/Internet_Relay_Chat
https://en.wikipedia.org/wiki/Instant_messaging

#### Machine to Machine

- **Modem speed protocols**

  * V.22 1200 bps; fallback to 600 bps ; QDPSK = DPSK (1980)
  * V.22bis 2400 bps; QAM (1984)
  * V.32 9600 bps; QAM (1984 but not widely used until years later)

https://tldp.org/HOWTO/Modem-HOWTO-29.html

- **SMB - Server Message Block - files and printers access across the network (1983)**

Server Message Block (SMB) is a communication protocol[1] originally developed in 1983 by Barry A. Feigenbaum at IBM[2] and intended to provide shared access to files and printers across nodes on a network of systems running IBM's OS/2. It also provides an authenticated inter-process communication (IPC) mechanism. In 1987, Microsoft and 3Com implemented SMB in LAN Manager for OS/2, at which time SMB used the NetBIOS service atop the NetBIOS Frames protocol as its underlying transport. Later, Microsoft implemented SMB in Windows NT 3.1 and has been updating it ever since, adapting it to work with newer underlying transports: TCP/IP and NetBT. SMB implementation consists of two vaguely named Windows services: "Server" (ID: LanmanServer) and "Workstation" (ID: LanmanWorkstation).[3] It uses NTLM or Kerberos protocols for user authentication.

https://en.wikipedia.org/wiki/Server_Message_Block

- **BOOTP - Bootstrap Protocol (1985)** 

introduced the concept of a relay agent, which allowed the forwarding of BOOTP packets across networks, allowing one central BOOTP server to serve hosts on many IP subnets, replaced by DHCP

https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol

#### Electronic Data Exchange

- **ODETTE/OFTP (1986+)**

The Odette File Transfer Protocol (OFTP) is a protocol created in 1986, used for EDI (Electronic Data Interchange) between two communications business partners. Its name comes from the Odette Organisation (the Organization for data exchange by teletransmission in Europe).

The ODETTE File Transfer Protocol (ODETTE-FTP) was defined in 1986 by working group four of the Organisation for Data Exchange by Tele-Transmission in Europe (ODETTE) to address the electronic data interchange (EDI) requirements of the European automotive industry. It was designed in the spirit of the Open System Interconnection (OSI) model utilising the Network Service provided by the CCITT X.25 recommendation.

OFTP 2 was written in 2007 by Data Interchange, as a specification for the secure transfer of business documents over the Internet, ISDN and X.25 networks. A description of OFTP 1.3 can be found in RFC 2204, whilst OFTP 2 is defined in RFC 5024.

OFTP 2 can work point-to-point or indirectly via a VAN (Value Added Network). A single OFTP 2 entity can make and receive calls, exchanging files in both directions.[1] This means that OFTP 2 can work in a push or pull mode, as opposed to AS2, which can only work in a push mode.[2]

OFTP 2 can encrypt and digitally sign message data, request signed receipts and also offers high levels of data compression. All of these services are available when using OFTP 2 over TCP/IP, X.25/ISDN or native X.25. When used over a TCP/IP network such as the Internet, additional session level security is available by using OFTP 2 over Transport Layer Security (TLS).

https://en.wikipedia.org/wiki/OFTP

- **EDIFACT (1987+)**

United Nations/Electronic Data Interchange for Administration, Commerce and Transport (UN/EDIFACT) is an international standard for electronic data interchange (EDI) developed for the United Nations and approved and published by UNECE, the UN Economic Commission for Europe.[1]

In 1987, following the convergence of the UN and US/ANSI syntax proposals, the UN/EDIFACT Syntax Rules were approved as the ISO standard ISO 9735 by the International Organization for Standardization.[2]

The EDIFACT standard provides:

a set of syntax rules to structure data
an interactive exchange protocol (I-EDI)
standard messages which allow multi-country and multi-industry exchange
The work of maintenance and further development of this standard is done through the United Nations Centre for Trade Facilitation and Electronic Business (UN/CEFACT) under the UN Economic Commission for Europe, in the Finance Domain working group UN CEFACT TBG5.

https://en.wikipedia.org/wiki/EDIFACT

### Automation

- **CEBus (1984-1992)**

CEBus(r), short for Consumer Electronics Bus, also known as EIA-600, is a set of electrical standards and communication protocols for electronic devices to transmit commands and data. It is suitable for devices in households and offices to use, and might be useful for utility interface and light industrial applications.
In 1984, members of the Electronic Industries Alliance (EIA) identified a need for standards that included more capability than the de facto home automation standard X10. X10 provided blind transmission of the commands ON, OFF, DIM, BRIGHT, ALL LIGHTS ON, and ALL UNITS OFF over powerline carrier, and later infrared and short range radio mediums. Over a six-year period, engineers representing international companies met on a regular basis and developed a proposed standard. They called this standard CEBus (pronounced "see bus"). The CEBus standard was released in September 1992.
CEBus is an open architecture set of specification documents which define protocols for products to communicate through power line wire, low voltage twisted pair wire, coaxial cable, infrared, RF, and fiber optics.
The CEBus Standard was developed on the foundation of an IR (infrared) protocol developed by GE (General Electric). This work was transferred to the EIA at the beginning of the EIA's involvement, under the plan that it would be expanded then maintained by the EIA.

https://en.wikipedia.org/wiki/CEBus

## Programming languages and frameworks
**[`^        back to top        ^`](#)**

- **Smalltalk - Smalltalk-80 - object-oriented, dynamically typed reflective programming language (1980)**

Smalltalk is an object-oriented, dynamically typed reflective programming language. It was designed and created in part for educational use, specifically for constructionist learning, at the Learning Research Group (LRG) of Xerox PARC by Alan Kay, Dan Ingalls, Adele Goldberg, Ted Kaehler, Diana Merry, Scott Wallace, and others during the 1970s.

The language was first generally released as Smalltalk-80. Smalltalk-like languages are in active development and have gathered loyal communities of users around them. ANSI Smalltalk was ratified in 1998 and represents the standard version of Smalltalk.[5]

Smalltalk took second place for "most loved programming language" in the Stack Overflow Developer Survey in 2017,[6] but it was not among the 26 most loved 
programming languages of the 2018 survey.[7]

There are a large number of Smalltalk variants.[8] The unqualified word Smalltalk is often used to indicate the Smalltalk-80 language, the first version to be made publicly available and created in 1980. The first hardware-environments which run the Smalltalk VMs were Xerox Alto computers.

Smalltalk was the product of research led by Alan Kay at Xerox Palo Alto Research Center (PARC); Alan Kay designed most of the early Smalltalk versions, Adele Goldberg wrote most of the documentation, and Dan Ingalls implemented most of the early versions. The first version, termed Smalltalk-71, was created by Kay in a few mornings on a bet that a programming language based on the idea of message passing inspired by Simula could be implemented in "a page of code".[4] A later variant used for research work is now termed Smalltalk-72 and influenced the development of the Actor model. Its syntax and execution model were very different from modern Smalltalk variants.

After significant revisions which froze some aspects of execution semantics to gain performance (by adopting a Simula-like class inheritance model of execution), Smalltalk-76 was created. This system had a development environment featuring most of the now familiar tools, including a class library code browser/editor. Smalltalk-80 added metaclasses, to help maintain the "everything is an object" (except variables) paradigm by associating properties and behavior with individual classes, and even primitives such as integer and boolean values (for example, to support different ways to create instances).

Smalltalk-80 was the first language variant made available outside of PARC, first as Smalltalk-80 Version 1, given to a small number of firms (Hewlett-Packard, Apple Computer, Tektronix, and Digital Equipment Corporation (DEC)) and universities (UC Berkeley) for peer review and implementing on their platforms. Later (in 1983) a general availability implementation, named Smalltalk-80 Version 2, was released as an image (platform-independent file with object definitions) and a virtual machine specification. ANSI Smalltalk has been the standard language reference since 1998.[9]

Two of the currently popular Smalltalk implementation variants are descendants of those original Smalltalk-80 images. Squeak is an open source implementation derived from Smalltalk-80 Version 1 by way of Apple Smalltalk. VisualWorks is derived from Smalltalk-80 version 2 by way of Smalltalk-80 2.5 and ObjectWorks (both products of ParcPlace Systems, a Xerox PARC spin-off company formed to bring Smalltalk to the market). As an interesting link between generations, in 2001 Vassili Bykov implemented Hobbes, a virtual machine running Smalltalk-80 inside VisualWorks.[10] (Dan Ingalls later ported Hobbes to Squeak.)

During the late 1980s to mid-1990s, Smalltalk environments—including support, training and add-ons—were sold by two competing organizations: ParcPlace Systems and Digitalk, both California based. ParcPlace Systems tended to focus on the Unix/Sun microsystems market, while Digitalk focused on Intel-based PCs running Microsoft Windows or IBM's OS/2. Both firms struggled to take Smalltalk mainstream due to Smalltalk's substantial memory needs, limited run-time performance, and initial lack of supported connectivity to SQL-based relational database servers. While the high price of ParcPlace Smalltalk limited its market penetration to mid-sized and large commercial organizations, the Digitalk products initially tried to reach a wider audience with a lower price. IBM initially supported the Digitalk product, but then entered the market with a Smalltalk product in 1995 called VisualAge/Smalltalk. Easel introduced Enfin at this time on Windows and OS/2. Enfin became far more popular in Europe, as IBM introduced it into IT shops before their development of IBM Smalltalk (later VisualAge). Enfin was later acquired by Cincom Systems, and is now sold under the name ObjectStudio, and is part of the Cincom Smalltalk product suite.

In 1995, ParcPlace and Digitalk merged into ParcPlace-Digitalk and then rebranded in 1997 as ObjectShare, located in Irvine, CA. ObjectShare (NASDAQ: OBJS) was traded publicly until 1999, when it was delisted and dissolved. The merged firm never managed to find an effective response to Java as to market positioning, and by 1997 its owners were looking to sell the business. In 1999, Seagull Software acquired the ObjectShare Java development lab (including the original Smalltalk/V and Visual Smalltalk development team), and still owns VisualSmalltalk, although worldwide distribution rights for the Smalltalk product remained with ObjectShare who then sold them to Cincom.[11] VisualWorks was sold to Cincom and is now part of Cincom Smalltalk. Cincom has backed Smalltalk strongly, releasing multiple new versions of VisualWorks and ObjectStudio each year since 1999.

Cincom, GemTalk, and Instantiations, continue to sell Smalltalk environments. IBM has 'end of life'd VisualAge Smalltalk having in the late 1990s decided to back Java instead and it is, as of 2005, supported by Instantiations, Inc.[12] who renamed the product VA Smalltalk (VAST Platform) and continue to release new versions yearly. The open Squeak implementation has an active community of developers, including many of the original Smalltalk community, and has recently been used to provide the Etoys environment on the OLPC project, a toolkit for developing collaborative applications Croquet Project, and the Open Cobalt virtual world application. GNU Smalltalk is a free software implementation of a derivative of Smalltalk-80 from the GNU project. Pharo Smalltalk is a fork of Squeak oriented toward research and use in commercial environments.

A significant development, that has spread across all Smalltalk environments as of 2016, is the increasing usage of two web frameworks, Seaside and AIDA/Web, to simplify the building of complex web applications. Seaside has seen considerable market interest with Cincom, Gemstone, and Instantiations incorporating and extending it.

Smalltalk was one of many object-oriented programming languages based on Simula.[13] Smalltalk is also one of the most influential programming languages. Virtually all of the object-oriented languages that came after—Flavors,[14] CLOS, Objective-C, Java, Python, Ruby,[15] and many others—were influenced by Smalltalk. Smalltalk was also one of the most popular languages for agile software development methods, rapid application development (RAD) or prototyping, and software design patterns.[16] The highly productive environment provided by Smalltalk platforms made them ideal for rapid, iterative development.

Smalltalk emerged from a larger program of Advanced Research Projects Agency (ARPA) funded research that in many ways defined the modern world of computing. In addition to Smalltalk, working prototypes of things such as hypertext, GUIs, multimedia, the mouse, telepresence, and the Internet were developed by ARPA researchers in the 1960s.[17][18] Alan Kay (one of the inventors of Smalltalk) also described a tablet computer he called the Dynabook which resembles modern tablet computers like the iPad.[4]

Smalltalk environments were often the first to develop what are now common object-oriented software design patterns. One of the most popular is the model–view–controller (MVC) pattern for user interface design. The MVC pattern enables developers to have multiple consistent views of the same underlying data. It's ideal for software development environments, where there are various views (e.g., entity-relation, dataflow, object model, etc.) of the same underlying specification. Also, for simulations or games where the underlying model may be viewed from various angles and levels of abstraction.[19]

In addition to the MVC pattern, the Smalltalk language and environment were highly influential in the history of the graphical user interface (GUI) and the what you see is what you get (WYSIWYG) user interface, font editors, and desktop metaphors for UI design. The powerful built-in debugging and object inspection tools that came with Smalltalk environments set the standard for all the integrated development environments, starting with Lisp Machine environments, that came after.[20]

As in other object-oriented languages, the central concept in Smalltalk-80 (but not in Smalltalk-72) is that of an object. An object is always an instance of a class. Classes are "blueprints" that describe the properties and behavior of their instances. For example, a GUI's window class might declare that windows have properties such as the label, the position and whether the window is visible or not. The class might also declare that instances support operations such as opening, closing, moving and hiding. Each particular window object would have its own values of those properties, and each of them would be able to perform operations defined by its class.

Reflection is a term that computer scientists apply to software programs that have the ability to inspect their own structure, for example their parse tree or data types of input and output parameters. Reflection is a feature of dynamic, interactive languages such as Smalltalk and Lisp. Interactive programs with reflection (either interpreted or compiled) maintain the state of all in-memory objects, including the code object itself, which are generated during parsing/compilation and are programmatically accessible and modifiable.

https://en.wikipedia.org/wiki/Smalltalk

- **Objective-C - adds Smalltalk-style messaging to the C, later used by Apple MacOS, iOS (1984)**

Objective-C is a general-purpose, object-oriented programming language that adds Smalltalk-style messaging to the C programming language. Originally developed by Brad Cox and Tom Love in the early 1980s, it was selected by NeXT for its NeXTSTEP operating system. Objective-C was the standard programming language supported by Apple for developing macOS (which descended from NeXTSTEP[3]) and iOS applications using their respective application programming interfaces (APIs), Cocoa and Cocoa Touch, until the introduction of Swift in 2014.[4]

Objective-C programs developed for non-Apple operating systems or that are not dependent on Apple's APIs may also be compiled for any platform supported by GNU GCC or LLVM/Clang.

Objective-C source code 'messaging/implementation' program files usually have .m filename extensions, while Objective-C 'header/interface' files have .h extensions, the same as C header files. Objective-C++ files are denoted with a .mm file extension.

Objective-C was created primarily by Brad Cox and Tom Love in the early 1980s at their company Productivity Products International (PPI).[5]

Leading up to the creation of their company, both had been introduced to Smalltalk while at ITT Corporation's Programming Technology Center in 1981. The earliest work on Objective-C traces back to around that time.[6] Cox was intrigued by problems of true reusability in software design and programming. He realized that a language like Smalltalk would be invaluable in building development environments for system developers at ITT. However, he and Tom Love also recognized that backward compatibility with C was critically important in ITT's telecom engineering milieu.[7]

Cox began writing a pre-processor for C to add some of the abilities of Smalltalk. He soon had a working implementation of an object-oriented extension to the C language, which he called "OOPC" for Object-Oriented Pre-Compiler.[8] Love was hired by Schlumberger Research in 1982 and had the opportunity to acquire the first commercial copy of Smalltalk-80, which further influenced the development of their brainchild. In order to demonstrate that real progress could be made, Cox showed that making interchangeable software components really needed only a few practical changes to existing tools. Specifically, they needed to support objects in a flexible manner, come supplied with a usable set of libraries, and allow for the code (and any resources needed by the code) to be bundled into one cross-platform format.

Love and Cox eventually formed PPI to commercialize their product, which coupled an Objective-C compiler with class libraries. In 1986, Cox published the main description of Objective-C in its original form in the book Object-Oriented Programming, An Evolutionary Approach. Although he was careful to point out that there is more to the problem of reusability than just what Objective-C provides, the language often found itself compared feature for feature with other languages.

Popularization through NeXT
In 1988, NeXT licensed Objective-C from StepStone (the new name of PPI, the owner of the Objective-C trademark) and extended the GCC compiler to support Objective-C. NeXT developed the AppKit and Foundation Kit libraries on which the NeXTSTEP user interface and Interface Builder were based. While the NeXT workstations failed to make a great impact in the marketplace, the tools were widely lauded in the industry. This led NeXT to drop hardware production and focus on software tools, selling NeXTSTEP (and OPENSTEP) as a platform for custom programming.

In order to circumvent the terms of the GPL, NeXT had originally intended to ship the Objective-C frontend separately, allowing the user to link it with GCC to produce the compiler executable. Though initially accepted by Richard M. Stallman, this plan was rejected after Stallman consulted with GNU's lawyers and NeXT agreed to make Objective-C part of GCC.[9]

The work to extend GCC was led by Steve Naroff, who joined NeXT from StepStone. The compiler changes were made available as per GPL license terms, but the runtime libraries were not, rendering the open source contribution unusable to the general public. This led to other parties developing such runtime libraries under open source license. Later, Steve Naroff was also principal contributor to work at Apple to build the Objective-C frontend to Clang.

The GNU project started work on its free software implementation of Cocoa, named GNUstep, based on the OpenStep standard.[10] Dennis Glatting wrote the first GNU Objective-C runtime in 1992. The GNU Objective-C runtime, which has been in use since 1993, is the one developed by Kresten Krab Thorup when he was a university student in Denmark.[citation needed] Thorup also worked at NeXT from 1993 to 1996.[11]

Apple development and Swift
After acquiring NeXT in 1996, Apple Computer used OpenStep in its then-new operating system, Mac OS X. This included Objective-C, NeXT's Objective-C-based developer tool, Project Builder, and its interface design tool, Interface Builder. Both were later merged into one application, Xcode. Most of Apple's current Cocoa API is based on OpenStep interface objects and is the most significant Objective-C environment being used for active development.

At WWDC 2014, Apple introduced a new language, Swift, which was characterized as "Objective-C without the C".

https://en.wikipedia.org/wiki/Objective-C

- **C++ - from "C with Classes" to systems programming and embedded, resource-constrained software and large systems, with performance, efficiency, and flexibility of use, manual memory management, object oriented (1985)**

C++ was designed with an orientation toward systems programming and embedded, resource-constrained software and large systems, with performance, efficiency, and flexibility of use as its design highlights.[11] C++ has also been found useful in many other contexts, with key strengths being software infrastructure and resource-constrained applications,[11] including desktop applications, video games, servers (e.g. e-commerce, web search, or databases), and performance-critical applications (e.g. telephone switches or space probes).[12]

C++ is standardized by the International Organization for Standardization (ISO), with the latest standard version ratified and published by ISO in December 2020 as ISO/IEC 14882:2020 (informally known as C++20).[13] The C++ programming language was initially standardized in 1998 as ISO/IEC 14882:1998, which was then amended by the C++03, C++11, C++14, and C++17 standards. The current C++20 standard supersedes these with new features and an enlarged standard library. Before the initial standardization in 1998, C++ was developed by Stroustrup at Bell Labs since 1979 as an extension of the C language; he wanted an efficient and flexible language similar to C that also provided high-level features for program organization.[14] Since 2012, C++ has been on a three-year release schedule[15] with C++23 as the next planned standard.[16] (1985)**

C++ (/ˌsiːˌplʌsˈplʌs/) is a general-purpose programming language created by Danish computer scientist Bjarne Stroustrup as an extension of the C programming language, or "C with Classes". The language has expanded significantly over time, and modern C++ now has object-oriented, generic, and functional features in addition to facilities for low-level memory manipulation. It is almost always implemented as a compiled language, and many vendors provide C++ compilers, including the Free Software Foundation, LLVM, Microsoft, Intel, Embarcadero, Oracle, and IBM, so it is available on many platforms.[10]

C++ was designed with an orientation toward systems programming and embedded, resource-constrained software and large systems, with performance, efficiency, and flexibility of use as its design highlights.[11] C++ has also been found useful in many other contexts, with key strengths being software infrastructure and resource-constrained applications,[11] including desktop applications, video games, servers (e.g. e-commerce, web search, or databases), and performance-critical applications (e.g. telephone switches or space probes).[12]

C++ is standardized by the International Organization for Standardization (ISO), with the latest standard version ratified and published by ISO in December 2020 as ISO/IEC 14882:2020 (informally known as C++20).[13] The C++ programming language was initially standardized in 1998 as ISO/IEC 14882:1998, which was then amended by the C++03, C++11, C++14, and C++17 standards. The current C++20 standard supersedes these with new features and an enlarged standard library. Before the initial standardization in 1998, C++ was developed by Stroustrup at Bell Labs since 1979 as an extension of the C language; he wanted an efficient and flexible language similar to C that also provided high-level features for program organization.[14] Since 2012, C++ has been on a three-year release schedule[15] with C++23 as the next planned standard.[16]

In 1979, Bjarne Stroustrup, a Danish computer scientist, began work on "C with Classes", the predecessor to C++.[17] The motivation for creating a new language originated from Stroustrup's experience in programming for his PhD thesis. Stroustrup found that Simula had features that were very helpful for large software development, but the language was too slow for practical use, while BCPL was fast but too low-level to be suitable for large software development. When Stroustrup started working in AT&T Bell Labs, he had the problem of analyzing the UNIX kernel with respect to distributed computing. Remembering his PhD experience, Stroustrup set out to enhance the C language with Simula-like features.[18] C was chosen because it was general-purpose, fast, portable and widely used. As well as C and Simula's influences, other languages also influenced this new language, including ALGOL 68, Ada, CLU and ML.

Initially, Stroustrup's "C with Classes" added features to the C compiler, Cpre, including classes, derived classes, strong typing, inlining and default arguments.[19]

In 1982, Stroustrup started to develop a successor to C with Classes, which he named "C++" (++ being the increment operator in C) after going through several other names. New features were added, including virtual functions, function name and operator overloading, references, constants, type-safe free-store memory allocation (new/delete), improved type checking, and BCPL style single-line comments with two forward slashes (//). Furthermore, Stroustrup developed a new, standalone compiler for C++, Cfront.

In 1984, Stroustrup implemented the first stream input/output library. The idea of providing an output operator rather than a named output function was suggested by Doug McIlroy[1] (who had previously suggested Unix pipes).

In 1985, the first edition of The C++ Programming Language was released, which became the definitive reference for the language, as there was not yet an official standard.[20] The first commercial implementation of C++ was released in October of the same year.[17]

In 1989, C++ 2.0 was released, followed by the updated second edition of The C++ Programming Language in 1991.[21] New features in 2.0 included multiple inheritance, abstract classes, static member functions, const member functions, and protected members. In 1990, The Annotated C++ Reference Manual was published. This work became the basis for the future standard. Later feature additions included templates, exceptions, namespaces, new casts, and a Boolean type.

In 1998, C++98 was released, standardizing the language, and a minor update (C++03) was released in 2003.

After C++98, C++ evolved relatively slowly until, in 2011, the C++11 standard was released, adding numerous new features, enlarging the standard library further, and providing more facilities to C++ programmers. After a minor C++14 update released in December 2014, various new additions were introduced in C++17.[22] After becoming finalized in February 2020,[23] a draft of the C++20 standard was approved on 4 September 2020 and officially published on 15 December 2020.[24][25]

On January 3, 2018, Stroustrup was announced as the 2018 winner of the Charles Stark Draper Prize for Engineering, "for conceptualizing and developing the C++ programming language".[26]

As of 2022 C++ ranked fourth on the TIOBE index, a measure of the popularity of programming languages, after Python, C and Java.[27]

https://en.wikipedia.org/wiki/C%2B%2B

- **Miranda - first purely functional language to be commercially supported (1985)**

Miranda is a lazy, purely functional programming language designed by David Turner as a successor to his earlier programming languages SASL and KRC, using some concepts from ML and Hope. It was produced by Research Software Ltd. of England (which holds a trademark on the name Miranda)[1] and was the first purely functional language to be commercially supported.[citation needed]

Miranda was first released in 1985 as a fast interpreter in C for Unix-flavour operating systems, with subsequent releases in 1987 and 1989. It had a strong influence on the later Haskell programming language.[2]

In 2020 a version of Miranda was released as open source under a BSD licence. The codebase has been updated to conform to modern C standards (C11/C18) and to generate 64-bit binaries. This has been tested on operating systems including Debian, Ubuntu, WSL/Ubuntu, and MacOS (Catalina).[3][4]

Miranda is a lazy, purely functional programming language. That is, it lacks side effects and imperative programming features. A Miranda program (called a script) is a set of equations that define various mathematical functions and algebraic data types. The word set is important here: the order of the equations is, in general, irrelevant, and there is no need to define an entity prior to its use.

Since the parsing algorithm makes intelligent use of layout (indentation), there is rarely a need for bracketing statements and no statement terminators are required. This feature, inspired by ISWIM, is also used in occam and Haskell and was later popularized by Python.

https://en.wikipedia.org/wiki/Miranda_(programming_language)

- **Erlang - Telecom language, distributed, fault-tolerant, soft real-time, code hot-swapping (1986)**

Erlang (/ˈɜːrlæŋ/ UR-lang) is a general-purpose, concurrent, functional programming language, and a garbage-collected runtime system. The term Erlang is used interchangeably with Erlang/OTP, or Open Telecom Platform (OTP), which consists of the Erlang runtime system, several ready-to-use components (OTP) mainly written in Erlang, and a set of design principles for Erlang programs.[4]

The Erlang runtime system is designed for systems with these traits:

Distributed
Fault-tolerant
Soft real-time
Highly available, non-stop applications
Hot swapping, where code can be changed without stopping a system.[5]
The Erlang programming language has immutable data, pattern matching, and functional programming.[6] The sequential subset of the Erlang language supports eager evaluation, single assignment, and dynamic typing.

A normal Erlang application is built out of hundreds of small Erlang processes.

It was originally proprietary software within Ericsson, developed by Joe Armstrong, Robert Virding, and Mike Williams in 1986,[7] but was released as free and open-source software in 1998.[8][9] Erlang/OTP is supported and maintained by the Open Telecom Platform (OTP) product unit at Ericsson.

The name Erlang, attributed to Bjarne Däcker, has been presumed by those working on the telephony switches (for whom the language was designed) to be a reference to Danish mathematician and engineer Agner Krarup Erlang and a syllabic abbreviation of "Ericsson Language".[7][10][11] Erlang was designed with the aim of improving the development of telephony applications.[12] The initial version of Erlang was implemented in Prolog and was influenced by the programming language PLEX used in earlier Ericsson exchanges. By 1988 Erlang had proven that it was suitable for prototyping telephone exchanges, but the Prolog interpreter was far too slow. One group within Ericsson estimated that it would need to be 40 times faster to be suitable for production use. In 1992, work began on the BEAM virtual machine (VM) which compiles Erlang to C using a mix of natively compiled code and threaded code to strike a balance between performance and disk space.[13] According to co-inventor Joe Armstrong, the language went from lab product to real applications following the collapse of the next-generation AXE telephone exchange named AXE-N in 1995. As a result, Erlang was chosen for the next asynchronous transfer mode (ATM) exchange AXD.[7]

In February 1998, Ericsson Radio Systems banned the in-house use of Erlang for new products, citing a preference for non-proprietary languages.[14] The ban caused Armstrong and others to make plans to leave Ericsson.[15] In March 1998 Ericsson announced the AXD301 switch,[7] containing over a million lines of Erlang and reported to achieve a high availability of nine "9"s.[16] In December 1998, the implementation of Erlang was open-sourced and most of the Erlang team resigned to form a new company Bluetail AB.[7] Ericsson eventually relaxed the ban and re-hired Armstrong in 2004.[15]

In 2006, native symmetric multiprocessing support was added to the runtime system and VM.[7]

In 2014, Ericsson reported Erlang was being used in its support nodes, and in GPRS, 3G and LTE mobile networks worldwide and also by Nortel and T-Mobile.[19]

Erlang is used in RabbitMQ. Erlang is the programming language used to code WhatsApp.[20]

- **Perl - dynamic text processing language, REGEX, early 90s CGI scripts (1987)**

Perl is a family of two high-level, general-purpose, interpreted, dynamic programming languages. "Perl" refers to Perl 5, but from 2000 to 2019 it also referred to its redesigned "sister language", Perl 6, before the latter's name was officially changed to Raku in October 2019.[9][10]

Though Perl is not officially an acronym,[11] there are various backronyms in use, including "Practical Extraction and Reporting Language".[12] Perl was developed by Larry Wall in 1987 as a general-purpose Unix scripting language to make report processing easier.[13] Since then, it has undergone many changes and revisions. Raku, which began as a redesign of Perl 5 in 2000, eventually evolved into a separate language. Both languages continue to be developed independently by different development teams and liberally borrow ideas from each other.

The Perl languages borrow features from other programming languages including C, sh, AWK, and sed;[1] They provide text processing facilities without the arbitrary data-length limits of many contemporary Unix command line tools.[14] Perl 5 gained widespread popularity in the late 1990s as a CGI scripting language, in part due to its powerful regular expression and string parsing abilities.[15][16][17][18]

In addition to CGI, Perl 5 is used for system administration, network programming, finance, bioinformatics, and other applications, such as for GUIs. It has been nicknamed "the Swiss Army chainsaw of scripting languages" because of its flexibility and power,[19] and also what some consider ugliness[20] due to its utilization of more special characters than many other languages. In 1998, it was also referred to as the "duct tape that holds the Internet together," in reference to both its ubiquitous use as a glue language and its perceived inelegance.[21]

Perl is a highly expressive programming language: source code for a given algorithm can be short and highly compressible.[22][23]

Larry Wall began work on Perl in 1987, while working as a programmer at Unisys,[14] and version 1.0 on December 18, 1987.[1] The language expanded rapidly over the next few years.

Perl 2, released in 1988, featured a better regular expression engine. Perl 3, released in 1989, added support for binary data streams.[citation needed]

Originally, the only documentation for Perl was a single lengthy man page. In 1991, Programming Perl, known to many Perl programmers as the "Camel Book" because of its cover, was published and became the de facto reference for the language. At the same time, the Perl version number was bumped to 4, not to mark a major change in the language but to identify the version that was well documented by the book.[citation needed]

Early Perl 5
Main article: Perl 5 version history
Perl 4 went through a series of maintenance releases, culminating in Perl 4.036 in 1993, whereupon Wall abandoned Perl 4 to begin work on Perl 5. Initial design of Perl 5 continued into 1994. The perl5-porters mailing list was established in May 1994 to coordinate work on porting Perl 5 to different platforms. It remains the primary forum for development, maintenance, and porting of Perl 5.[28]

Perl 5.000 was released on October 17, 1994.[29] It was a nearly complete rewrite of the interpreter, and it added many new features to the language, including objects, references, lexical (my) variables, and modules. Importantly, modules provided a mechanism for extending the language without modifying the interpreter. This allowed the core interpreter to stabilize, even as it enabled ordinary Perl programmers to add new language features. Perl 5 has been in active development since then.

Perl 5.001 was released on March 13, 1995. Perl 5.002 was released on February 29, 1996 with the new prototypes feature. This allowed module authors to make subroutines that behaved like Perl builtins. Perl 5.003 was released June 25, 1996, as a security release.[30]

Perl has been referred to as "line noise" and a write-only language by its critics.

https://en.wikipedia.org/wiki/Perl

## Navigation
**[`^        back to top        ^`](#)**

**Decision to open GPS to civilian use and deployment of modern GPS satellites**

In 1983, after Soviet interceptor aircraft shot down the civilian airliner KAL 007 that strayed into prohibited airspace because of navigational errors, killing all 269 people on board, U.S. President Ronald Reagan announced that GPS would be made available for civilian uses once it was completed,[54][55] although it had been previously published in Navigation magazine, and that the CA code (Coarse/Acquisition code) would be available to civilian users.[citation needed]

By 1985, ten more experimental Block-I satellites had been launched to validate the concept.

Beginning in 1988, command and control of these satellites was moved from Onizuka AFS, California to the 2nd Satellite Control Squadron (2SCS) located at Falcon Air Force Station in Colorado Springs, Colorado.[56][57]

On February 14, 1989, the first modern Block-II satellite was launched.

https://en.wikipedia.org/wiki/Global_Positioning_System


