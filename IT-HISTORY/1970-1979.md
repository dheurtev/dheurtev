# 1970s

**In short:**
- [Inventions](#inventions) : first demonstration of TFT LCD and AM LCD displays, Yellow, red and orange-red LED, Blue-violet LED, DCT video compression, EEPROM, Earliest monochromatic flat panel LED television display, Inkjet printers
- [Electronics](#electronics) : Backplane system, Very large scale integration (VLSI), EPROM, first CPU, semi-conductor memory, BIOS
- [Energy](#energy) : 
- [Telecommunications](#telecommunications) : Conversion of the Bell backbone from analog to digital, Digital leased lines, Class-5 telephone switch / digital switches (Alcatel E10, Nortel DMS, Western Electric 5ESS), 1975 New York Telephone exchange fire, first long distance fiber network, X.25 standard, USENET
- [Networking](#networking) : Proprietary protocol suites, 1.2 kbps modem (VA3400), TCP
- [Cryptography](#cryptography) : birth of modern cryptography : symmetric key (DES), asymmetric key (DH, RSA)
- [Computers](#computers) :
  * [Form factor](#form-factor) : 
  * [OS](#os) : 
  * [Peripherals](#peripherals) : Dot matrix printer, DIN connector, Laser printers
  * [Storage](#storage) : Disk platter hard drive, hard drive swinging arm actuator, compact cassettes 
  * [Uses](#uses) :
  
      *Servers* : Timesharing (Multi-user), FTP servers, Community memory (BBS precursor), Relational database management system (Oracle database)
      
      *Clients* : interactive user terminals, timesharing computers, first spreadsheet, first document preparation program, 
      
      *CGI* : Early CGI in movies
- [Consumer Electronics](#consumer-electronics) :
  * [Gadgets](#gadgets) : First single chip calculator, TN LCD watches (Seiko, Casio), first scientific calculator, first programmable calculators, single-chip linear predictive coding speech synthesizer (Speak & Spell)
  * [Multimedia](#multimedia) : electronic remote controls, Technics SL-1200, Phonograph improvement (Shibata stylus), remote control for light and applicances
  * [Screens](#screens) : Panaplex display (gas-plasma display), Ceefax Teletext, TV as computer screens
  * [Broadcast](#broadcast) : First North American commercial satelitte to carry tv tranmissions (Anik1), beginning of the satellite TV industry
  * [Video games](#video-games) : arcade video games (Pong), introduction of cartridge-based home consoles, first and second generation video games consoles
- [Standards and protocols](#standards-and-protocols) : X.25, TCP, FTP, TAR compression, EDI (X12), X.10, Modbus
- [Programming languages and frameworks](#programming-languages-and-frameworks):
- [Navigation](#navigation) : military GPS system design and launch

## Inventions
**[`^        back to top        ^`](#)**

- **First demonstration of TFT LCD and AM LCD displays (1973-1974)**

In 1972, the concept of the active-matrix thin-film transistor (TFT) liquid-crystal display panel was prototyped in the United States by T. Peter Brody's team at Westinghouse, in Pittsburgh, Pennsylvania.[56] In 1973, Brody, J. A. Asars and G. D. Dixon at Westinghouse Research Laboratories demonstrated the first thin-film-transistor liquid-crystal display (TFT LCD).[57][58] As of 2013, all modern high-resolution and high-quality electronic visual display devices use TFT-based active matrix displays.[59] Brody and Fang-Chen Luo demonstrated the first flat active-matrix liquid-crystal display (AM LCD) in 1974, and then Brody coined the term "active matrix" in 1975.[52]

https://en.wikipedia.org/wiki/Liquid-crystal_display

In 1973, Brody, J. A. Asars and G. D. Dixon at Westinghouse Research Laboratories demonstrated the first thin-film-transistor liquid-crystal display (TFT LCD).[13][14] Brody and Fang-Chen Luo demonstrated the first flat active-matrix liquid-crystal display (AM LCD) using TFTs in 1974.[10]

https://en.wikipedia.org/wiki/Flat-panel_display

- **Yellow, red and orange-red LED (1972)**

M. George Craford,[27] a former graduate student of Holonyak, invented the first yellow LED and improved the brightness of red and red-orange LEDs by a factor of ten in 1972.[28]

https://en.wikipedia.org/wiki/Light-emitting_diode

- **Blue-violet LED (1972)**

The first blue-violet LED using magnesium-doped gallium nitride was made at Stanford University in 1972 by Herb Maruska and Wally Rhines, doctoral students in materials science and engineering.[39][40] At the time Maruska was on leave from RCA Laboratories, where he collaborated with Jacques Pankove on related work. In 1971, the year after Maruska left for Stanford, his RCA colleagues Pankove and Ed Miller demonstrated the first blue electroluminescence from zinc-doped gallium nitride, though the subsequent device Pankove and Miller built, the first actual gallium nitride light-emitting diode, emitted green light.[41][42] In 1974 the U.S. Patent Office awarded Maruska, Rhines and Stanford professor David Stevenson a patent for their work in 1972 (U.S. Patent US3819974 A). Today, magnesium-doping of gallium nitride remains the basis for all commercial blue LEDs and laser diodes. In the early 1970s, these devices were too dim for practical use, and research into gallium nitride devices slowed.

https://en.wikipedia.org/wiki/Light-emitting_diode

- **Discrete Cosine Transform (DCT) video compression (1972) - Nassir Ahmed**

Digital HDTV was made possible by the development of discrete cosine transform (DCT) video compression.[25][23] DCT coding is a lossy image compression technique that was first proposed by Nasir Ahmed in 1972,

https://en.wikipedia.org/wiki/High-definition_television

- **EEPROM - electrically re-programmable non-volatile memory (1972) Toshiba**
In 1972, a type of electrically re-programmable non-volatile memory was invented by Fujio Masuoka at Toshiba, who is also known as the inventor of flash memory.[12] Most of the major semiconductor manufactures, such as Toshiba,[12][6] Sanyo (later, ON Semiconductor),[13] IBM,[14] Intel,[15][16] NEC (later, Renesas Electronics),[17] Philips (later, NXP Semiconductors),[18] Siemens (later, Infineon Technologies),[19] Honeywell (later, Atmel),[20] Texas Instruments,[21] studied, invented, and manufactured some electrically re-programmable non-volatile devices until 1977.
https://en.wikipedia.org/wiki/EEPROM

- **Earliest monochromatic flat panel LED television display (1977)**

In 1977, James P Mitchell prototyped and later demonstrated what was perhaps the earliest monochromatic flat panel LED television display.

https://en.wikipedia.org/wiki/Flat-panel_display

- **Inkjet printers**

In the late 1970s, inkjet printers that could reproduce digital images generated by computers were developed, mainly by Epson, Hewlett-Packard (HP) and Canon. In the worldwide consumer market, four manufacturers account for the majority of inkjet printer sales: Canon, HP, Epson and Brother.[5]

https://en.wikipedia.org/wiki/Inkjet_printing

## Electronics
**[`^        back to top        ^`](#)**

- **Backplane system (1970s)**

Prior to the invention of the microprocessor, the digital computer consisted of multiple printed circuit boards in a card-cage case with components connected by a backplane, a set of interconnected sockets. In very old designs, copper wires were the discrete connections between card connector pins, but printed circuit boards soon became the standard practice. The central processing unit (CPU), memory, and peripherals were housed on individually printed circuit boards, which were plugged into the backplane. The ubiquitous S-100 bus of the 1970s is an example of this type of backplane system.

https://en.wikipedia.org/wiki/Motherboard

- **VLSI - Very large scale integration (1970s+)**

Very large-scale integration (VLSI) is the process of creating an integrated circuit (IC) by combining millions of MOS transistors onto a single chip. VLSI began in the 1970s when MOS integrated circuit (Metal Oxide Semiconductor) chips were widely adopted, enabling complex semiconductor and telecommunication technologies to be developed. The microprocessor and memory chips are VLSI devices. Before the introduction of VLSI technology, most ICs had a limited set of functions they could perform. An electronic circuit might consist of a CPU, ROM, RAM and other glue logic. VLSI enables IC designers to add all of these into one chip.

https://en.wikipedia.org/wiki/Very_Large_Scale_Integration

- **Intel Intel 1702A 2K (256 x 8) UV Erasable PROM - First EPROM (1971) - Intel**

An EPROM (rarely EROM), or erasable programmable read-only memory, is a type of programmable read-only memory (PROM) chip that retains its data when its power supply is switched off. Computer memory that can retrieve stored data after a power supply has been turned off and back on is called non-volatile. It is an array of floating-gate transistors individually programmed by an electronic device that supplies higher voltages than those normally used in digital circuits. Once programmed, an EPROM can be erased by exposing it to strong ultraviolet light source (such as from a mercury-vapor lamp). EPROMs are easily recognizable by the transparent fused quartz (or on later models resin) window on the top of the package, through which the silicon chip is visible, and which permits exposure to ultraviolet light during erasing

https://en.wikipedia.org/wiki/EPROM

- **Intel 4004 - Fist CPU (1971) - Intel** 

The Intel 4004 was the world's first microprocessor—a complete general-purpose CPU on a single chip

https://en.wikipedia.org/wiki/Intel_4004

When the first general purpose microprocessor was introduced in 1971 (Intel 4004) it immediately began chipping away at the low end of the computer market, replacing embedded minicomputers in many industrial devices.

https://en.wikipedia.org/wiki/Commodity_computing

- **IBM/7 - semiconductor memory (1970-1971) - IBM**

The IBM System/7 was a computer system designed for industrial control, announced on October 28, 1970 and first shipped in 1971. It was a 16-bit machine and one of the first made by IBM to use novel semiconductor memory, instead of magnetic core memory conventional at that date

https://en.wikipedia.org/wiki/IBM_System/7

- **Bios (1975) - CP/M operating system - Digital Research, Inc.**

In computing, BIOS (/ˈbaɪɒs, -oʊs/, BY-oss, -⁠ohss; Basic Input/Output System (also known as the System BIOS, ROM BIOS, BIOS ROM or PC BIOS) is firmware used to provide runtime services for operating systems and programs and to perform hardware initialization during the booting process (power-on startup).[1] The BIOS firmware comes pre-installed on an IBM PC or IBM PC compatible's system board and exists in UEFI-based systems too.[2][3] The name originates from the Basic Input/Output System used in the CP/M operating system in 1975.[4][5] The BIOS originally proprietary to the IBM PC has been reverse engineered by some companies (such as Phoenix Technologies) looking to create compatible systems. The interface of that original system serves as a de facto standard.

https://en.wikipedia.org/wiki/BIOS
https://en.wikipedia.org/wiki/CP/M

## Energy
**[`^        back to top        ^`](#)**
## Telecommunications
**[`^        back to top        ^`](#)**

- **Conversion of the Bell backbone from analog to digital, Digital leased lines**

Leased line services (or private line services) became digital in the 1970s with the conversion of the Bell backbone network from analog to digital circuits. This allowed AT&T to offer Dataphone Digital Services (later re-branded digital data services) that started the deployment of ISDN and T1 lines to customer premises to connect

https://en.wikipedia.org/wiki/Leased_line

- **Class-5 telephone switch (1970-2000s)**

A class-5 telephone switch is a telephone switch or telephone exchange in the public switched telephone network located at the local telephone company's central office, directly serving subscribers. Class-5 switch services include basic dial-tone, calling features, and additional digital and data services to subscribers connected to a local loop.

https://en.wikipedia.org/wiki/Class-5_telephone_switch

**Alcatel E10 - early deployment of digital switches (1972) - Alcatel**

E10A (E10N3)- Original switch introduced in 1972 one of the earliest deployments of TDM switching in the world.

Unlike other conventional systems at the time, the E10 was able to take its input signals in the form of a train of pulses. The conversion of the analog signals (which represent speech) to the pulse code modulated (PCM) digital form takes place in the local switch units, each of which serve around 500 subscribers. At the other end of the line, the digital signals are turned back into analog wave form by remote switches and then transmitted on to the receiving subscriber. The use of microelectronics made it much more efficient and lowered the cost of transmitting signals in this way. As the equipment only needed to recognise the presence or absence of the bits to be able to be able to reconstruct the original speech wave-form, the quality and loudness of the speech signals were made independent of the distance between the calling and called person.

Having digital PCM links between all local switched and the main E10 unit also allowed for setting up, ringing, and clearing down all telephone calls progressing through the local units serving the subscribers. Being able to control a complete telephone area with a single computer installation made it far easier for providers to provide customers with new facilities.

https://www.carritech.com/news/history-alcatel-1000-e10/

https://en.wikipedia.org/wiki/List_of_telephone_switches

**DMS - Digital Multiplex System  (1977) - Nortel**

Digital Multiplex System (DMS) is the name shared among several different telephony product lines from Nortel Networks for wireline and wireless operators. Among them are the DMS-1 (originally named the DMS-256[1]) Rural/Urban digital loop carrier, the DMS-10 telephone switch, the DMS SuperNode family of telephone switches (DMS-100, DMS-200, DMS-250, DMS-300, DMS-500, DMS-GSP, DMS-MSC, DMS-MTX), and the S/DMS optical transmission system.

Exploratory development on the technology began at Northern Telecom's Bell Northern Research Labs in Ottawa, Ontario in 1971. The first Class 5 switch, the DMS-10, began service on 21 October 1977 in Fort White, Florida and the first toll switch (Class 4), the DMS-200, entered service in 1979 in Ottawa. The DMS-10 was the first commercially successful Class 5 digital switch in the North American market and had a profound impact on the industry. 

https://en.wikipedia.org/wiki/Digital_Multiplex_System

**5ESS Switching System  (1982) - Western Electric**
The 5ESS Switching System is a Class 5 telephone electronic switching system developed by Western Electric for the American Telephone and Telegraph Company (AT&T) and the Bell System in the United States. It came into service in 1982 and the last unit was produced in 2003.[1]

https://en.wikipedia.org/wiki/5ESS_Switching_System

- **1975 New York Telephone exchange fire**

The New York Telephone exchange fire occurred on February 27, 1975, at the New York Telephone Company switching center at 204 Second Avenue and Thirteenth Street in the East Village of Manhattan, New York City. At this time, the building contained central offices for connecting local customer telephone lines, as well as toll switching systems. The fire disrupted service for 175,000 customers, connected within the building through 105,000 service loops. It was the worst single service disaster suffered by any single Bell operating company in the 20th century.

The events relating to the fire make it notable for several reasons, including the extent of the disruption, the large scale and speed of the recovery efforts, which were completed in 23 days,[1] and the succeeding influence on adoption of fire safety rules for installation of low-voltage wiring inside buildings, especially in areas that can spread fire or toxic fumes. Decades later the polyvinyl chloride (PVC) combustion products produced by the fire were identified as a reason for elevated rates of cancer in the firefighters at the scene.[2][3]

https://en.wikipedia.org/wiki/1975_New_York_Telephone_exchange_fire

- **First long distance fiber network (1977)**

In 1977, the first long-distance fiber network was deployed by GTE in Long Beach, California.

https://en.wikipedia.org/wiki/Computer_network

- **X.25 standard**

The X.25 standard gained political support in European countries and from the European Economic Community (EEC). For example, the European Informatics Network, which was based on datagrams was replaced with Euronet based on X.25.[38] Peter Kirstein wrote that European networks tended to be short-term projects with smaller numbers of computers and users. As a result, the European networking activities did not lead to any strong standards except X.25 which became the main European data protocol for fifteen to twenty years. 

https://en.wikipedia.org/wiki/Protocol_Wars

- **USENET - predecessor to dial-up Internet access (1979) - Duke University**

In 1979, Tom Truscott and Jim Ellis, graduates of Duke University, created an early predecessor to dial-up Internet access called the USENET. The USENET was a UNIX based system that used a dial-up connection to transfer data through telephone modems.

https://en.wikipedia.org/wiki/Dial-up_Internet_access

## Networking
**[`^        back to top        ^`](#)**

- **Proprietary protocol suites**

Computer manufacturers developed **proprietary protocol suites** such as IBM's Systems Network Architecture (SNA), Digital Equipment Corporation's DECnet, and Xerox's Xerox Network Systems (XNS). During the late 1970s and most of the 1980s, there remained a lack of open networking options. Therefore, proprietary standards, particularly SNA and DECnet, as well as some variants of XNS (e.g., Novell NetWare and Banyan VINES), were commonly used on private networks, becoming somewhat "de facto" industry standards.

https://en.wikipedia.org/wiki/Protocol_Wars

In 1976, John Murphy of Datapoint Corporation created **ARCNET**, a token-passing network first used to share storage devices.
In 1977, **Xerox Network Systems (XNS)** was developed by Robert Metcalfe and Yogen Dalal at Xerox.[24]

https://en.wikipedia.org/wiki/Computer_network

- **VA3400 - 1.2 kbps modem (1973) - Vadic**

The 201A Data-Phone was a synchronous modem using two-bit-per-symbol phase-shift keying (PSK) encoding, achieving 2,000 bit/s half-duplex over normal phone lines.[10] In this system the two tones for any one side of the connection are sent at similar frequencies as in the 300 bit/s systems, but slightly out of phase.

In early 1973, Vadic introduced the VA3400 which performed full-duplex at 1,200 bit/s over a normal phone line.[11]

In November 1976, AT&T introduced the 212A modem, similar in design, but using the lower frequency set for transmission. It was not compatible with the VA3400,[12] but it would operate with 103A modem at 300 bit/s.

In 1977, Vadic responded with the VA3467 triple modem, an answer-only modem sold to computer center operators that supported Vadic's 1,200-bit/s mode, AT&T's 212A mode, and 103A operation.[13]

https://en.wikipedia.org/wiki/Modem

- **TCP - Transmission Control Protocol (1974)**

transmission of packets (reception is certain). Essential to the internet. 

https://en.wikipedia.org/wiki/Transmission_Control_Protocol

## Cryptography
**[`^        back to top        ^`](#)**

- **DES - symmetric key (1975-1977)**

The Data Encryption Standard (DES /ˌdiːˌiːˈɛs, dɛz/) is a symmetric-key algorithm for the encryption of digital data. Although its short key length of 56 bits makes it too insecure for modern applications, it has been highly influential in the advancement of cryptography.
Developed in the early 1970s at IBM and based on an earlier design by Horst Feistel, the algorithm was submitted to the National Bureau of Standards (NBS) following the agency's invitation to propose a candidate for the protection of sensitive, unclassified electronic government data. In 1976, after consultation with the National Security Agency (NSA), the NBS selected a slightly modified version (strengthened against differential cryptanalysis, but weakened against brute-force attacks), which was published as an official Federal Information Processing Standard (FIPS) for the United States in 1977.[2]

https://en.wikipedia.org/wiki/Data_Encryption_Standard

- **Asymmetric public-private key (1976) - Whitfield Diffie and Martin Hellman**

The idea of an asymmetric public-private key cryptosystem is attributed to Whitfield Diffie and Martin Hellman, who published this concept in 1976.

https://en.wikipedia.org/wiki/RSA_(cryptosystem)

- **RSA (1977) - Rivest–Shamir–Adleman**

a public-key cryptosystem that is widely used for secure data transmission. It is also one of the oldest. The acronym "RSA" comes from the surnames of Ron Rivest, Adi Shamir and Leonard Adleman, who publicly described the algorithm in 1977. Patented in 1983.

https://en.wikipedia.org/wiki/RSA_(cryptosystem)

## Computers
**[`^        back to top        ^`](#)**
### Form factor
**[`^        back to top        ^`](#)**
### OS
**[`^        back to top        ^`](#)**
### Peripherals
**[`^        back to top        ^`](#)**

- **Dot matrix printer (1970) - Centronics**

An Wang, Robert Howard and Prentice Robinson began development of a low-cost printer at Centronics, a subsidiary of Wang Laboratories that produced specialty computer terminals. The printer used the dot matrix printing principle, with a print head consisting of a vertical row of seven metal pins connected to solenoids. When power was applied to the solenoids, the pin was pushed forward to strike the paper and leave a dot. To make a complete character glyph, the print head would receive power to specified pins to create a single vertical pattern, then the print head would move to the right by a small amount, and the process repeated. On their original design, a typical glyph was printed as a matrix seven high and five wide, while the "A" models used a print head with 9 pins and formed glyphs that were 9 by 7.[2]
This left the problem of sending the ASCII data to the printer. While a serial port does so with the minimum of pins and wires, it requires the device to buffer up the data as it arrives bit by bit and turn it back into multi-bit values. A parallel port makes this simpler; the entire ASCII value is presented on the pins in complete form. In addition to the eight data pins, the system also needed various control pins as well as electrical grounds. Wang happened to have a surplus stock of 20,000 Amphenol 36-pin micro ribbon connectors that were originally used for one of their early calculators. The interface only required 21 of these pins, the rest were grounded or not connected. The connector has become so closely associated with Centronics that it is now popularly known as the "Centronics connector".[3]
The Centronics Model 101 printer, featuring this connector, was released in 1970.[3]
The printer side of the interface quickly became an industry de facto standard, but manufacturers used various connectors on the system side, so a variety of cables were required. For example, NCR used the 36-pin micro ribbon connector on both ends of the connection, early VAX systems used a DC-37 connector, Texas Instruments used a 25-pin card edge connector and Data General used a 50-pin micro ribbon connector. When IBM implemented the parallel interface on the IBM PC, they used the DB-25F connector at the PC-end of the interface, creating the now familiar parallel cable with a DB25M at one end and a 36-pin micro ribbon connector at the other.

https://en.wikipedia.org/wiki/Parallel_port

- **DIN connector (1974)**

The DIN connector is an electrical connector that was originally standardized in the early 1970s[1] by the Deutsches Institut für Normung (DIN), the German national standards organization. There are DIN standards for various different connectors.

https://en.wikipedia.org/wiki/DIN_connector

- **Laser printers (1970s) - Xerox, IBM, Canon**
In the 1960s, the Xerox Corporation held a dominant position in the photocopier market.[2] In 1969, Gary Starkweather, who worked in Xerox's product development department, had the idea of using a laser beam to "draw" an image of what was to be copied directly onto the copier drum. After transferring to the recently formed Palo Alto Research Center (Xerox PARC) in 1971, Starkweather adapted a Xerox 7000 copier to make SLOT (Scanned Laser Output Terminal). In 1972, Starkweather worked with Butler Lampson and Ronald Rider to add a control system and character generator, resulting in a printer called EARS (Ethernet, Alto Research character generator, Scanned laser output terminal)—which later became the Xerox 9700 laser printer.[3][4][5]

1976: The **first commercial implementation of a laser printer**, the **IBM 3800**, was released. It was designed for data centers, where it replaced line printers attached to mainframe computers. The IBM 3800 was used for high-volume printing on continuous stationery, and achieved speeds of 215 pages per minute (ppm), at a resolution of 240 dots per inch (dpi). Over 8,000 of these printers were sold.[6]

1977: The Xerox 9700 was brought to market. Unlike the IBM 3800, the Xerox 9700 was not targeted to replace any particular existing printers; however, it did have limited support for the loading of fonts. The Xerox 9700 excelled at printing high-value documents on cut-sheet paper with varying content (e.g. insurance policies).[6]

**1979**: Inspired by the Xerox 9700's commercial success, Japanese camera and optics company Canon developed the **Canon LBP-10, a low-cost desktop laser printer**. Canon then began work on a much-improved print engine, the Canon CX, resulting in the LBP-CX printer. Having no experience in selling to computer users, Canon sought partnerships with three Silicon Valley companies: Diablo Data Systems (who rejected the offer), Hewlett-Packard (HP), and Apple Computer.[7][8]

https://en.wikipedia.org/wiki/Laser_printing

### Storage
**[`^        back to top        ^`](#)**

- **Winchester - disk platter hard drive (1973) - IBM**

In 1973, IBM introduced a new type of HDD code-named "Winchester". Its primary distinguishing feature was that the disk heads were not withdrawn completely from the stack of disk platters when the drive was powered down. Instead, the heads were allowed to "land" on a special area of the disk surface upon spin-down, "taking off" again when the disk was later powered on. This greatly reduced the cost of the head actuator mechanism, but precluded removing just the disks from the drive as was done with the disk packs of the day. Instead, the first models of "Winchester technology" drives featured a removable disk module, which included both the disk pack and the head assembly, leaving the actuator motor in the drive upon removal. Later "Winchester" drives abandoned the removable media concept and returned to non-removable platters.

https://en.wikipedia.org/wiki/Hard_disk_drive

- **Hard drive swinging arm actuator (1974) - IBM**

In 1974 IBM introduced the swinging arm actuator, made feasible because the Winchester recording heads function well when skewed to the recorded tracks. The simple design of the IBM GV (Gulliver) drive,[44] invented at IBM's UK Hursley Labs, became IBM's most licensed electro-mechanical invention[45] of all time, the actuator and filtration system being adopted in the 1980s eventually for all HDDs, and still universal nearly 40 years and 10 Billion arms later.

https://en.wikipedia.org/wiki/Hard_disk_drive

- **Storage on cassettes (late 70s, early 80s)**

Many late 1970s and early 1980s home computers used Compact Cassettes, encoded with the Kansas City standard, or alternate encodings. Modern cartridge formats include LTO, DLT, and DAT/DDC.

https://en.wikipedia.org/wiki/Magnetic_tape

### Uses
**[`^        back to top        ^`](#)**

## Consumer Electronics
**[`^        back to top        ^`](#)**
### Gadgets
**[`^        back to top        ^`](#)**

- **First single chip calculator (1970) - Pico Electronics**

In 1970, a group of engineers started a company in Glenrothes, Scotland called Pico Electronics.[2] The company developed the first single chip calculator

https://en.wikipedia.org/wiki/X10_(industry_standard)

- **TN LCD watches (1970s)**

On December 4, 1970, the twisted nematic field effect (TN) in liquid crystals was filed for patent by Hoffmann-LaRoche in Switzerland, (Swiss patent No. 532 261) with Wolfgang Helfrich and Martin Schadt (then working for the Central Research Laboratories) listed as inventors.[46] Hoffmann-La Roche licensed the invention to Swiss manufacturer Brown, Boveri & Cie, its joint venture partner at that time, which produced TN displays for wristwatches and other applications during the 1970s for the international markets including the Japanese electronics industry, which soon produced the first digital quartz wristwatches with TN-LCDs and numerous other products. James Fergason, while working with Sardari Arora and Alfred Saupe at Kent State University Liquid Crystal Institute, filed an identical patent in the United States on April 22, 1971.[54] In 1971, the company of Fergason, ILIXCO (now LXD Incorporated), produced LCDs based on the TN-effect, which soon superseded the poor-quality DSM types due to improvements of lower operating voltages and lower power consumption. Tetsuro Hama and Izuhiko Nishimura of **Seiko** received a US patent dated February 1971, for an electronic wristwatch incorporating a TN-LCD.[55] In 1972, the first wristwatch with TN-LCD was launched on the market: The Gruen Teletime which was a four digit display watch.

In 1972 North American Rockwell Microelectronics Corp introduced the use of DSM LCDs for calculators for marketing by Lloyds Electronics Inc, though these required an internal light source for illumination.[60] Sharp Corporation followed with DSM LCDs for pocket-sized calculators in 1973[61] and then mass-produced TN LCDs for watches in 1975.[62] Other Japanese companies soon took a leading position in the wristwatch market, like Seiko and its first 6-digit TN-LCD quartz wristwatch, and **Casio**’s ‘Casiotron’. Color LCDs based on Guest-Host interaction were invented by a team at RCA in 1968.[63] A particular type of such a color LCD was developed by Japan's Sharp Corporation in the 1970s, receiving patents for their inventions, such as a patent by Shinji Kato and Takaaki Miyazaki in May 1975,[64] and then improved by Fumiaki Funada and Masataka Matsuura in December 1975.[65] TFT LCDs similar to the prototypes developed by a Westinghouse team in 1972 were patented in 1976 by a team at Sharp consisting of Fumiaki Funada, Masataka Matsuura, and Tomio Wada,[66] then improved in 1977 by a Sharp team consisting of Kohei Kishi, Hirosaku Nonomura, Keiichiro Shimizu, and Tomio Wada.[67] However, these TFT-LCDs were not yet ready for use in products, as problems with the materials for the TFTs were not yet solved.

https://en.wikipedia.org/wiki/Liquid-crystal_display

- **First scientific calculator (1972) - HP**

Meanwhile, Hewlett-Packard (HP) had been developing a pocket calculator. Launched in early 1972, it was unlike the other basic four-function pocket calculators then available in that it was the first pocket calculator with scientific functions that could replace a slide rule. The $395 HP-35, along with nearly all later HP engineering calculators, uses reverse Polish notation (RPN), also called postfix notation. A calculation like "8 plus 5" is, using RPN, performed by pressing 8, Enter↑, 5, and +; instead of the algebraic infix notation: 8, +, 5, =. It had 35 buttons and was based on Mostek Mk6020 chip.

https://en.wikipedia.org/wiki/Calculator

- **First programmable calculators (1974+) - HP**

The first programmable pocket calculator was the HP-65, in 1974; it had a capacity of 100 instructions, and could store and retrieve programs with a built-in magnetic card reader. Two years later the HP-25C introduced continuous memory, i.e., programs and data were retained in CMOS memory during power-off. In 1979, HP released the first alphanumeric, programmable, expandable calculator, the HP-41C. It could be expanded with random-access memory (RAM, for memory) and read-only memory (ROM, for software) modules, and peripherals like bar code readers, microcassette and floppy disk drives, paper-roll thermal printers, and miscellaneous communication interfaces (RS-232, HP-IL, HP-IB).

https://en.wikipedia.org/wiki/Calculator

- **Speak & Spell - single-chip linear predictive coding speech synthesizer (1978) - TI**

In 1978, Texas Instruments introduced the first single-chip linear predictive coding speech synthesizer.[36] In 1976, TI began a feasibility study of memory-intensive applications for bubble memory then being developed. They soon focused on speech applications. This resulted in the development the TMC0280 one-chip linear predictive coding speech synthesizer, which was the first time a single silicon chip had electronically replicated the human voice.[37][38] This was used in several TI commercial products beginning with Speak & Spell, which was introduced at the Summer Consumer Electronics Show in June 1978. 

https://en.wikipedia.org/wiki/Texas_Instruments

### Multimedia
**[`^        back to top        ^`](#)**

- **Electronic remote controls (1970) - RCA**

In 1970, RCA introduced an all-electronic remote control that uses digital signals and metal–oxide–semiconductor field-effect transistor (MOSFET) memory. This was widely adopted for color television, replacing motor-driven tuning controls.[23]

https://en.wikipedia.org/wiki/Remote_control#Infrared,_line_of_sight_and_operating_angle

- **Technics SL-1200 (1972) - Matsushita**

Technics SL-1200 is a series of direct-drive turntables originally manufactured from October 1972 until 2010, and resumed in 2016, by Matsushita Electric (now Panasonic Corporation) under the brand name of Technics. S means "Stereo", L means "Player". Originally released as a high fidelity consumer record player, it quickly became adopted among radio and disco club disc jockeys, thanks to the direct drive, high torque motor design, making it initially suitable for pushbutton cueing and starting of tracks on radio and in dance clubs. It is still extremely popular with audiophiles.[citation needed]

When the use of slip-mats for cueing and beat-mixing (and scratching) became popular in hip hop music, the quartz-controlled high torque motor system enabled records to be mixed with consistency and accuracy. A primary design goal was for hi-fidelity, but having good build quality, control over wow and flutter, and minimized resonance made the equipment particularly suitable for use in nightclubs and other public-address applications. Since its release in 1979, SL-1200MK2 and its successors were the most common turntable for DJing and scratching.[citation needed] Producers, DJs and MCs refer to the Technics turntable as "the 1s and 2s" and the "Wheels of Steel".

1200s are commonly used in recording studios and for non-electronic live music performance. More than 3 million units were sold. It is widely regarded as one of the most durable and reliable turntables ever produced.[weasel words] Many 1970s units are still in heavy use.[citation needed] In the autumn of 2010, Panasonic announced that the series was to be discontinued.[1][2] However, at the 2016 Consumer Electronics Show, Panasonic announced that they would return in two models named "Grand Class": one a limited run of 1200 globally (1200GAE), and the other a consumer product (1200G). A lighter and less expensive 1200GR model was announced.

At the London Science Museum, a Technics SL-1210[3] is on display[4] as one of the pieces of technology that have "shaped the world we live in".[5]

The SL-1200 was the most influential turntable.[7] It was developed in 1971 by a team led by Shuichi Obata at Matsushita, which then released it onto the market in 1972.[8] It was adopted by New York City hip hop DJs in the 1970s. As they experimented with the SL-1200 decks, they developed scratching techniques when they found that the motor would continue to spin at the correct RPM even if the DJ wiggled the record back and forth on the platter.[7]

The most influential turntable was the Technics SL-1200.[61] Since then, turntablism spread widely in hip hop culture, and the SL-1200 remained the most widely used turntable in DJ culture for the next several decades.[61]

https://en.wikipedia.org/wiki/Phonograph

- **Phonograph improvement : Shibata stylus (1972) - JVC**

A development in stylus form came about by the attention to the CD-4 quadraphonic sound modulation process, which requires up to 50 kHz frequency response, with cartridges like Technics EPC-100CMK4 capable of playback on frequencies up to 100 kHz. This requires a stylus with a narrow side radius, such as 5 µm (or 0.2 mil). A narrow-profile elliptical stylus is able to read the higher frequencies (greater than 20 kHz), but at an increased wear, since the contact surface is narrower. For overcoming this problem, the Shibata stylus was invented around 1972 in Japan by Norio Shibata of JVC.[67]

The Shibata-designed stylus offers a greater contact surface with the groove, which in turn means less pressure over the vinyl surface and thus less wear. A positive side effect is that the greater contact surface also means the stylus will read sections of the vinyl that were not touched (or "worn") by the common spherical stylus. In a demonstration by JVC[68] records "worn" after 500 plays at a relatively very high 4.5 gf tracking force with a spherical stylus, played "as new" with the Shibata profile.[citation needed]

Other advanced stylus shapes appeared following the same goal of increasing contact surface, improving on the Shibata. Chronologically: "Hughes" Shibata variant (1975),[69] "Ogura" (1978),[70] Van den Hul (1982).[71] Such a stylus may be marketed as "Hyperelliptical" (Shure), "Alliptic", "Fine Line" (Ortofon), "Line contact" (Audio Technica), "Polyhedron", "LAC", or "Stereohedron" (Stanton).[72]

A keel-shaped diamond stylus appeared as a byproduct of the invention of the CED Videodisc. This, together with laser-diamond-cutting technologies, made possible the "ridge" shaped stylus, such as the Namiki (1985)[73] design, and Fritz Gyger (1989)[74] design. This type of stylus is marketed as "MicroLine" (Audio technica), "Micro-Ridge" (Shure), or "Replicant" (Ortofon).[72]

https://en.wikipedia.org/wiki/Phonograph

- **Remote control for light and applicances (1974) -  Pico Electronics / Birmingham Sound Reproducers** 

In 1974, the Pico engineers jointly developed a LP record turntable, the ADC Accutrac 4000, with Birmingham Sound Reproducers, at the time the largest manufacturer of record changers in the world. It could be programmed to play selected tracks, and could be operated by a remote control using ultrasound signals, which sparked the idea of remote control for lights and appliances. By 1975, the X10 project was conceived, so named because it was the tenth project. In 1978, X10 products started to appear in RadioShack and Sears stores. Together with BSR a partnership was formed, with the name X10 Ltd. At that time the system consisted of a 16 channel command console, a lamp module, and an appliance module. Soon after came the wall switch module and the first X10 timer.

https://en.wikipedia.org/wiki/X10_(industry_standard)

### Screens
**[`^        back to top        ^`](#)**

- **Panaplex display - gas-discharge / gas-plasma display (early 1970s)**

Burroughs Corporation, a maker of adding machines and computers, developed the Panaplex display in the early 1970s. The Panaplex display, generically referred to as a gas-discharge or gas-plasma display,[58] uses the same technology as later plasma video displays, but began life as a seven-segment display for use in adding machines. They became popular for their bright orange luminous look and found nearly ubiquitous use throughout the late 1970s and into the 1990s in cash registers, calculators, pinball machines, aircraft avionics such as radios, navigational instruments, and stormscopes; test equipment such as frequency counters and multimeters; and generally anything that previously used nixie tube or numitron displays with a high digit-count. These displays were eventually replaced by LEDs because of their low current-draw and module-flexibility, but are still found in some applications where their high brightness is desired, such as pinball machines and avionics.

https://en.wikipedia.org/wiki/Plasma_display

- **Ceefax teletex service (1973) - BBC**

The impetus for a more complex type of television remote control came in 1973, with the development of the Ceefax teletext service by the BBC. Most commercial remote controls at that time had a limited number of functions, sometimes as few as three: next channel, previous channel, and volume/off. This type of control did not meet the needs of Teletext sets, where pages were identified with three-digit numbers. A remote control that selects Teletext pages would need buttons for each numeral from zero to nine, as well as other control functions, such as switching from text to picture, and the normal television controls of volume, channel, brightness, color intensity, etc. Early Teletext sets used wired remote controls to select pages, but the continuous use of the remote control required for Teletext quickly indicated the need for a wireless device. So BBC engineers began talks with one or two television manufacturers, which led to early prototypes in around 1977–1978 that could control many more functions. ITT was one of the companies and later gave its name to the ITT protocol of infrared communication.[24]

https://en.wikipedia.org/wiki/Remote_control#Infrared,_line_of_sight_and_operating_angle

- **TV screens used as computer screens**

Many personal computers introduced in the late 1970s and the 1980s were designed to use television receivers as their display devices, making the resolutions dependent on the television standards in use, including PAL and NTSC. Picture sizes were usually limited to ensure the visibility of all the pixels in the major television standards and the broad range of television sets with varying amounts of over scan. The actual drawable picture area was, therefore, somewhat smaller than the whole screen, and was usually surrounded by a static-colored border (see image to right). Also, the interlace scanning was usually omitted in order to provide more stability to the picture, effectively halving the vertical resolution in progress. 160 × 200, 320 × 200 and 640 × 200 on NTSC were relatively common resolutions in the era (224, 240 or 256 scanlines were also common). In the IBM PC world, these resolutions came to be used by 16-color EGA video cards.

One of the drawbacks of using a classic television is that the computer display resolution is higher than the television could decode. Chroma resolution for NTSC/PAL televisions are bandwidth-limited to a maximum 1.5 MHz, or approximately 160 pixels wide, which led to blurring of the color for 320- or 640-wide signals, and made text difficult to read (see example image below). Many users upgraded to higher-quality televisions with S-Video or RGBI inputs that helped eliminate chroma blur and produce more legible displays. The earliest, lowest cost solution to the chroma problem was offered in the Atari 2600 Video Computer System and the Apple II+, both of which offered the option to disable the color and view a legacy black-and-white signal. On the Commodore 64, the GEOS mirrored the Mac OS method of using black-and-white to improve readability.

https://en.wikipedia.org/wiki/Display_resolution

### Broadcast
**[`^        back to top        ^`](#)**

- **Anik1 - First North American commercial satelitte to carry tv tranmissions (1972)**

The first commercial North American satellite to carry television transmissions was Canada's geostationary Anik 1, which was launched on 9 November 1972.[48] ATS-6, the world's first experimental educational and direct broadcast satellite (DBS), was launched on 30 May 1974.[49] It transmitted at 860 MHz using wideband FM modulation and had two sound channels. The transmissions were focused on the Indian subcontinent but experimenters were able to receive the signal in Western Europe using home constructed equipment that drew on UHF television design techniques already in use.[50]

The first in a series of Soviet geostationary satellites to carry direct-to-home television, Ekran 1, was launched on 26 October 1976.[51] It used a 714 MHz UHF downlink frequency so that the transmissions could be received with existing UHF television technology rather than microwave technology.[52]

https://en.wikipedia.org/wiki/Satellite_television

- **Beginning of the satellite TV industry (1976–1980)**

The satellite television industry developed first in the US from the cable television industry as communication satellites were being used to distribute television programming to remote cable television headends. Home Box Office (HBO), Turner Broadcasting System (TBS), and Christian Broadcasting Network (CBN, later The Family Channel) were among the first to use satellite television to deliver programming. Taylor Howard of San Andreas, California, became the first person to receive C-band satellite signals with his home-built system in 1976.[53]

In the US, PBS, a non-profit public broadcasting service, began to distribute its television programming by satellite in 1978.[54]

In 1979, Soviet engineers developed the Moskva (or Moscow) system of broadcasting and delivering of TV signals via satellites. They launched the Gorizont communication satellites later that same year. These satellites used geostationary orbits.[55] They were equipped with powerful on-board transponders, so the size of receiving parabolic antennas of downlink stations was reduced to 4 and 2.5 metres.[55] On October 18, 1979, the Federal Communications Commission (FCC) began allowing people to have home satellite earth stations without a federal government license.[56] The front cover of the 1979 Neiman-Marcus Christmas catalogue featured the first home satellite TV stations on sale for $36,500.[57] The dishes were nearly 20 feet (6.1 m) in diameter[58] and were remote controlled.[59] The price went down by half soon after that, but there were only eight more channels.[60] The Society for Private and Commercial Earth Stations (SPACE), an organisation which represented consumers and satellite TV system owners, was established in 1980.[61]

Early satellite television systems were not very popular due to their expense and large dish size.[62] The satellite television dishes of the systems in the late 1970s and early 1980s were 10 to 16 feet (3.0 to 4.9 m) in diameter,[63] made of fibreglass or solid aluminum or steel,[64] and in the United States cost more than $5,000, sometimes as much as $10,000.[65] Programming sent from ground stations was relayed from eighteen satellites in geostationary orbit located 22,300 miles (35,900 km) above the Earth.[66][67]

By 1980, satellite television was well established in the USA and Europe.

https://en.wikipedia.org/wiki/Satellite_television

### Video games
**[`^        back to top        ^`](#)**

- **Magnavox Odyssey - Arcade video games (1972) - Sanders Associates / Magnavox**

In 1966, while working at Sanders Associates, Ralph Baer came up with an idea for an entertainment device that could be hooked up to a television monitor. Presenting this to his superiors at Sanders and getting their approval, he along with William Harrison and William Rusch refined Baer's concept into the "Brown Box" prototype of a home video game console that could play a simple table tennis game. The three patented the technology, and Sanders, not in the business of commercialization, sold licenses to the patents to Magnavox to commercialize. With Baer's help, Magnavox developed the Magnavox Odyssey, the first commercial home console, in 1972.

https://en.wikipedia.org/wiki/History_of_video_games

- **Pong - influencial arcade video game (1972) - Atari**

Concurrently, Nolan Bushnell and Ted Dabney had the idea of making a coin-operated cabinet housing a small, low-cost microcomputer to run Spacewar! By 1971, the two had developed Computer Space with Nutting Associates, the first recognized arcade video game.[7] Bushnell and Dabney struck out on their own and formed Atari. Bushnell, inspired by the table tennis game on the Odyssey, hired Allan Alcorn to develop an arcade version of the game, this time using discrete transistor–transistor logic (TTL) electronic circuitry. Atari's Pong was released in late 1972 and is considered the first successful arcade video game. It ignited the growth of the arcade game industry in the United States from both established coin-operated game manufacturers like Williams, Chicago Coin, and the Midway subsidiary of Bally Manufacturing, and new startups such as Ramtek and Allied Leisure. Many of these were Pong clones using ball-and-paddle controls, and led to saturation of the market in 1974, forcing arcade game makers to try to innovate new games in 1975. Many of the newer companies created in the wake of Pong failed to innovate on their own and shut down, and by the end of 1975, the arcade market had fallen by about 50% based on new game sale revenues.[8] Further, Magnavox took Atari and several other of these arcade game makers to court over violations of Baer's patents. Bushnell settled the suit for Atari, gaining perpetual rights for the patents for Atari as part of the settlement.[9] Others failed to settle, and Magnavox won around $100 million in damages from these patent infringement suits before the patents expired in 1990.[10]

Arcade video games caught on quickly in Japan due to partnerships between American and Japanese corporations that kept the Japan companies abreast of technology developments within the United States. The Nakamura Amusement Machine Manufacturing Company (Namco) partnered with Atari to import Pong into Japan in late 1973. Within the year, Taito and Sega released Pong clones in Japan by mid-1973. Japanese companies began developing novel games and exporting or licensing them through partners in 1974.[11] Among these included Taito's Gun Fight (originally Western Gun in its Japanese release), which was licensed to Midway. Midway's version, released in 1975, was the first arcade video game to use a microprocessor rather than discrete TLL components.[12] This innovation drastically reduced the complexity and time to design of arcade games and the number of physical components required to achieve more advanced gameplay.[13]

https://en.wikipedia.org/wiki/History_of_video_games

- **First generation of video games consoles (1972-1983)**

In the history of video games, the first-generation era refers to the video games, video game consoles, and handheld video game consoles available from 1972 to 1983. Notable consoles of the first generation include the **Odyssey series** (excluding the Magnavox Odyssey 2), the **Atari Home Pong**,[1] the **Coleco Telstar series** and the Color TV-Game series. The generation ended with the Computer TV-Game in 1980, but many manufacturers had left the market prior due to the market decline in 1977 and the start of the second generation of video game consoles.

https://en.wikipedia.org/wiki/First_generation_of_video_game_consoles

- **Dedicated console market (1975-1978)**

The Magnavox Odyssey never caught on with the public, due largely to the limited functionality of its primitive discrete electronic component technology.[8] By mid-1975 large-scale integration (LSI) microchips had become inexpensive enough to be incorporated into a consumer product.[8] In 1975, Magnavox reduced the part count of the Odyssey using a three-chip set created by Texas Instruments and released two new systems that only played ball-and-paddle games, the Magnavox Odyssey 100 and Magnavox Odyssey 200. Atari, meanwhile, entered the consumer market that same year with the single-chip Home Pong system. The next year, General Instrument released a "Pong-on-a-chip" LSI and made it available at a low price to any interested company. Toy company Coleco Industries used this chip to create the million-selling Telstar console model series (1976–77).

These initial home video game consoles were popular, leading to a large influx of companies releasing Pong and other video game clones to satisfy consumer demand. While there were only seven companies that were releasing home consoles in 1975, there were at least 82 by 1977, with more than 160 different models that year alone that were easily documented. A large number of these consoles were created in East Asia, and it is estimated that over 500 Pong-type home console models were made during this period.[8] As with the prior paddle-and-ball saturation in the arcade game field by 1975 due to consumer weariness, dedicated console sales dropped sharply in 1978, disrupted by the introduction of programmable systems and Handheld electronic games.[8]

https://en.wikipedia.org/wiki/History_of_video_games

- **Fairchild Channel F - Introduction of cartridge-based home consoles (1976) - Fairchild Camera and Instrument**

Development costs of dedicated game hardware for arcade and home consoles based on discrete component circuitry and application-specific integrated circuits (ASICs) with only limited consumer lifespans drove engineers to find alternatives. Microprocessors had dropped far enough in price by 1975 to make these a viable option for developing programmable consoles that could load in game software from a form of swappable media.[20]

The Fairchild Channel F by Fairchild Camera and Instrument was released in 1976. It is the first home console to use programmable ROM cartridges - allowing players to swap games - as well as being the first home console to use a microprocessor which read instructions from the ROM cartridge. Atari and Magnavox followed suit in 1977 with the release of the Atari Video Computer System (VCS, later known as the Atari 2600) and the Magnavox Odyssey 2, both systems also introducing the use of cartridges. As to complete the Atari VCS quickly, Bushnell sold Atari to Warner Communications $28 million, providing the necessary cash infusion to complete the system's design by the end of 1977.[13] The initial market for these new consoles were initially modest as consumers were still wary after the saturation of dedicated home consoles.[21] 

https://en.wikipedia.org/wiki/History_of_video_games

- **Second generation video games consoles (1976-1992)**
In the history of video games, the second-generation era refers to computer and video games, video game consoles, and handheld video game consoles available from 1976 to 1992. Notable platforms of the second generation include the **Fairchild Channel F**, **Atari 2600**, **Intellivision**, **Odyssey 2**, and **ColecoVision**. The generation began in November 1976 with the release of the Fairchild Channel F.[1] This was followed by the Atari 2600 in 1977,[2] Magnavox Odyssey² in 1978,[3] Intellivision in 1980[4] and then the Emerson Arcadia 2001, ColecoVision, Atari 5200, and Vectrex,[5] all in 1982. By the end of the era, there were over 15 different consoles. It coincided with, and was partly fuelled by, the golden age of arcade video games. This peak era of popularity and innovation for the medium resulted in many games for second generation home consoles being ports of arcade games. Space Invaders, the first "killer app" arcade game to be ported, was released in 1980 for the Atari 2600, though earlier Atari-published arcade games were ported to the 2600 previously.[6] Coleco packaged Nintendo's Donkey Kong with the ColecoVision when it was released in August 1982.
The primary driver of the second generation of consoles was the introduction of the low-cost microprocessor. 
https://en.wikipedia.org/wiki/Second_generation_of_video_game_consoles

## Standards and protocols
**[`^        back to top        ^`](#)**
### Network layer

- **X.25 (1976)**

ITU-T standard protocol suite for **packet-switched data communication in wide area networks (WAN)**. It was originally defined by the International Telegraph and Telephone Consultative Committee (CCITT, now ITU-T) in a series of drafts and finalized in a publication known as The Orange Book in 1976.
Networks using X.25 were popular during the late 1970s and 1980s with **telecommunications companies** and in **financial transaction systems** such as automated teller machines. An X.25 WAN consists of packet-switching exchange (PSE) nodes as the networking hardware, and leased lines, plain old telephone service connections, or ISDN connections as physical links. However, most users have moved to Internet Protocol (IP) systems instead. X.25 was used up to 2015 (e.g. by the credit card payment industry)[7] and is still used by aviation, purchasable from telecoms companies

https://en.wikipedia.org/wiki/X.25

### Transport layer
- **NCP - Network Control Protocol (1970)**

First implementation for the ARPANET. Transport layer protocol used during the early ARPANET. Remained in use until 1982.

https://en.wikipedia.org/wiki/Network_Control_Protocol_(ARPANET)

- **TCP - Transmission Control Protocol (1974)**

transmission of packets (reception is certain). Essential to the internet. 

https://en.wikipedia.org/wiki/Transmission_Control_Protocol

### Application layer
- **FTP - File Transfer Protocol (1971)**
standard communication protocol used for the transfer of computer files from a server to a client on a computer network. FTP is built on a client–server model architecture using separate control and data connections between the client and the server.
Until 1980, FTP ran on NCP, the predecessor of TCP/IP. Superseded by FTPS and SFTP.

https://en.wikipedia.org/wiki/File_Transfer_Protocol

- **G.711 (1972) : a narrowband audio codec**

originally designed for use in telephony that provides toll-quality audio at 64 kbit/s. G.711 passes audio signals in the range of 300–3400 Hz and samples them at the rate of 8,000 samples per second, with the tolerance on that rate of 50 parts per million (ppm). Non-uniform (logarithmic) quantization with 8 bits is used to represent each sample, resulting in a 64 kbit/s bit rate. There are two slightly different versions: μ-law, which is used primarily in North America and Japan, and A-law, which is in use in most other countries outside North America.
G.711 is an ITU-T standard (Recommendation) for audio companding, titled **Pulse code modulation (PCM)** of voice frequencies released for use in 1972. It is a required standard in many technologies, such as in the H.320 and H.323 standards.[1] It can also be used for fax communication over IP networks (as defined in T.38 specification).

https://en.wikipedia.org/wiki/G.711

-  **NAME/FINGER - Finger Protocol (1977)**
simple network protocols for the exchange of human-oriented status and user information (insecure, obsolete) https://en.wikipedia.org/wiki/Finger_(protocol)

### Electronic data exchange

Electronic data interchange (EDI) is the concept of businesses electronically communicating information that was traditionally communicated on paper, such as purchase orders and invoices. Technical standards for EDI exist to facilitate parties transacting such instruments without having to make special arrangements.

EDI has existed at least since the early 70s, and there are many EDI standards (including X12, EDIFACT, ODETTE, etc.), some of which address the needs of specific industries or regions. It also refers specifically to a family of standards. In 1996, the National Institute of Standards and Technology defined electronic data interchange as "the computer-to-computer interchange of a standardised format for data exchange. EDI implies a sequence of messages between two parties, either of whom may serve as originator or recipient. The formatted data representing the documents may be transmitted from originator to recipient via telecommunications or physically transported on electronic storage media." It distinguished mere electronic communication or data exchange, specifying that "in EDI, the usual processing of received messages is by computer only. Human intervention in the processing of a received message is typically intended only for error conditions, for quality review, and for special situations. For example, the transmission of binary or textual data is not EDI as defined here unless the data are treated as one or more data elements of an EDI message and are not normally intended for human interpretation as part of online data processing."[1] In short, EDI can be defined as the transfer of structured data, by agreed message standards, from one computer system to another without human intervention.

https://en.wikipedia.org/wiki/Electronic_data_interchange

- **ASC X12 (1979)**

The Accredited Standards Committee X12 (also known as ASC X12) is a standards organization. Chartered by the American National Standards Institute (ANSI) in 1979,[2] it develops and maintains the X12 Electronic data interchange (EDI) and Context Inspired Component Architecture (CICA) standards along with XML schemas which drive business processes globally. The membership of ASC X12 includes technologists and business process experts, encompassing health care, insurance, transportation, finance, government, supply chain and other industries.

https://en.wikipedia.org/wiki/ASC_X12

### File compression

- **TAR - Tape archive (1979) - Version 7 UNIX - AT&T Bell Laboratories**

In computing, tar is a computer software utility for collecting many files into one archive file, often referred to as a tarball, for distribution or backup purposes. The name is derived from "tape archive", as it was originally developed to write data to sequential I/O devices with no file system of their own. The archive data sets created by tar contain various file system parameters, such as name, timestamps, ownership, file-access permissions, and directory organization. POSIX abandoned tar in favor of pax, yet tar sees continued widespread use.

The command-line utility was first introduced in the Version 7 Unix in January 1979, replacing the tp program (which in turn replaced "tap").[7] The file structure to store this information was standardized in POSIX.1-1988[8] and later POSIX.1-2001,[9] and became a format supported by most modern file archiving systems. The tar command was abandoned in POSIX.1-2001 in favor of pax command, which was to support ustar file format; the tar command was indicated for withdrawal in favor of pax command at least since 1994.

Today, Unix-like operating systems usually include tools to support tar files, as well as utilities commonly used to compress them, such as gzip and bzip2.

https://en.wikipedia.org/wiki/Tar_(computing)

### Automation 
- **X.10 (1975)**

X10 is a protocol for **communication among electronic devices used for home automation (domotics)**. It primarily uses power line wiring for signaling and control, where the signals involve brief radio frequency bursts representing digital information. A wireless radio-based protocol transport is also defined. X10 was developed in 1975 by Pico Electronics of Glenrothes, Scotland, in order to allow remote control of home devices and appliances. It was the first general purpose domotic network technology and remains the most widely available[citation needed].[1]
Although a number of higher-bandwidth alternatives exist, X10 remains popular in the home environment with millions of units in use worldwide, and inexpensive availability of new components.

- **Modbus (1979)**

Modbus is a data communications protocol originally published by Modicon (now Schneider Electric) in 1979 for use with its programmable logic controllers (PLCs). Modbus has become a de facto standard communication protocol and is now a commonly available means of connecting industrial electronic devices.[1]

Modbus is popular in industrial environments because it is openly published and royalty-free. It was developed for industrial applications, is relatively easy to deploy and maintain compared to other standards, and places few restrictions on the format of the data to be transmitted.

The Modbus protocol uses character serial communication lines, Ethernet, or the Internet protocol suite as a transport layer. Modbus supports communication to and from multiple devices connected to the same cable or Ethernet network. For example, there can be a device that measures temperature and another device to measure humidity connected to the same cable, both communicating measurements to the same computer, via Modbus.

Modbus is often used to connect a plant/system supervisory computer with a remote terminal unit (RTU) in supervisory control and data acquisition (SCADA) systems. Many of the data types are named from industrial control of factory devices, such as ladder logic because of its use in driving relays: a single-bit physical output is called a coil, and a single-bit physical input is called a discrete input or a contact.

The development and update of Modbus protocols have been managed by the Modbus Organization[2] since April 2004, when Schneider Electric transferred rights to that organization.[3] The Modbus Organization is an association of users and suppliers of Modbus-compliant devices that advocates for the continued use of the technology.[4] Modbus Organization, Inc. is a trade association for the promotion and development of the Modbus protocol.[2]

https://en.wikipedia.org/wiki/Modbus

## Programming languages and frameworks
**[`^        back to top        ^`](#)**
## Navigation
**[`^        back to top        ^`](#)**
- **Military GPS design and launch (1970-1978)**

A team led by Harold L Jury of Pan Am Aerospace Division in Florida from 1970 to 1973, used real-time data assimilation and recursive estimation to do so, reducing systematic and residual errors to a manageable level to permit accurate navigation.[35]

During Labor Day weekend in 1973, a meeting of about twelve military officers at the Pentagon discussed the creation of a Defense Navigation Satellite System (DNSS). It was at this meeting that the real synthesis that became GPS was created.

In 1972, the USAF Central Inertial Guidance Test Facility (Holloman AFB) conducted developmental flight tests of four prototype GPS receivers in a Y configuration over White Sands Missile Range, using ground-based pseudo-satellites.[53]

In 1978, the first experimental Block-I GPS satellite was launched.[39]

https://en.wikipedia.org/wiki/Global_Positioning_System

------------



- Computers : 
  * Hardware : First computers designed for individual use (Xerox Alto), First PCs (Apple II)
- Uses :

## Computers
**[`^        back to top        ^`](#)**

### Evolution of mainframes

By the early 1970s, many mainframes acquired **interactive user terminals**[NB 1] operating as **timesharing computers**, supporting **hundreds of users simultaneously along with batch processing**. Users gained access through **keyboard/typewriter terminals** and **specialized text terminal CRT displays with integral keyboards**, or **later from personal computers equipped with terminal emulation software**

https://en.wikipedia.org/wiki/Mainframe_computer

### Evolution of hardware

- **Datapoint 2200 (1970) - Datapoint**

Released in June 1970, the programmable terminal called the Datapoint 2200 is among the **earliest known devices that bears significant resemblance to the modern personal computer, with a CRT screen, keyboard, programmability, and program storage**

https://en.wikipedia.org/wiki/History_of_personal_computers


- **Xerox Alto - First computers designed for individual use (1973) - Xerox**

The Xerox Alto was one of the first computers designed for individual use in 1973 and is regarded as the **first modern computer to use a mouse**.[48] Inspired by PARC's Alto, the Lilith, a computer which had been developed by a team around Niklaus Wirth at ETH Zürich between 1978 and 1980, provided a mouse as well. 

https://en.wikipedia.org/wiki/Computer_mouse

In the 1970s, Engelbart's ideas were further refined and extended to graphics by researchers at Xerox PARC and specifically Alan Kay, who went beyond text-based hyperlinks and used a GUI as the main interface for the Smalltalk programming language, which ran on the Xerox Alto computer, released in 1973. Most modern general-purpose GUIs are derived from this system.

https://en.wikipedia.org/wiki/Graphical_user_interface

- **Interdata 7/32 and 8/32 - first 32-bit minicomputers under $10,000 (1973) - Interdata** 

Interdata computers are primarily remembered for being the first 32-bit minicomputers under $10,000 
The 7/32 and 8/32 became the computers of choice in large scale embedded systems, such as FFT machines used in real-time seismic analysis, CAT scanners, and flight simulator systems. They were also often used as non-IBM peripherals in IBM networks, serving the role of HASP workstations and spooling systems, so called RJE (Remote Job Entry) stations. For example, the computers behind the first Space Shuttle simulator consisted of thirty-six 32-bit minis inputting and/or outputting data to networked mainframe computers (both IBM and Univac), all in real-time.

https://en.wikipedia.org/wiki/Interdata_7/32_and_8/32



- **Apple II - First commodity-like microcomputer, uses a 5 1/4 floppy disk(1977) - Apple**

By 1976, there were several firms racing to introduce the first truly successful commercial personal computers**. Three machines, the Apple II, PET 2001 and TRS-80 were all released in 1977,[34] becoming the most popular by late 1978.[35] Byte magazine later referred to Commodore, Apple, and Tandy as the "1977 Trinity".[36] Also in 1977, Sord Computer Corporation released the Sord M200 Smart Home Computer in Japan.[37]

  * **Apple** is founded in 1976.

https://en.wikipedia.org/wiki/Apple_Inc

While early Apple II models use ordinary cassette tapes as storage devices, they were superseded in 1978 by the introduction of a **5+1⁄4-inch floppy disk drive** and interface called the Disk II.

https://en.wikipedia.org/wiki/Commodity_computing

https://en.wikipedia.org/wiki/Apple_II

- **PERQ - first commercially available computer with a GUI (1979) - Three Rivers Computer Corporation (3RCC)**

The first commercially available computer with a GUI was 1979 PERQ workstation, manufactured by Three Rivers Computer Corporation. Its design was heavily influenced by the work at Xerox PARC.

https://en.wikipedia.org/wiki/Graphical_user_interface

The design was heavily influenced by the original workstation computer, the Xerox Alto, which was never commercially produced.

https://en.wikipedia.org/wiki/PERQ




### Servers
- **File Transfer Protocol (1971)**

https://en.wikipedia.org/wiki/File_Transfer_Protocol

- **Community Memory - BBS precursor (1973-1975) - Berkeley, California**

A precursor to the public bulletin board system was Community Memory, started in August 1973 in Berkeley, California. Useful microcomputers did not exist at that time, and modems were both expensive and slow. Community Memory therefore ran on a mainframe computer and was accessed through terminals located in several San Francisco Bay Area neighborhoods.[4][5] The poor quality of the original modem connecting the terminals to the mainframe prompted Community Memory hardware person, Lee Felsenstein, to invent the Pennywhistle modem, whose design was highly influential in the mid-1970s.

Community Memory allowed the user to type messages into a computer terminal after inserting a coin, and offered a "pure" bulletin board experience with public messages only (no email or other features). It did offer the ability to tag messages with keywords, which the user could use in searches. The system acted primarily in the form of a buy and sell system with the tags taking the place of the more traditional classifications. But users found ways to express themselves outside these bounds, and the system spontaneously created stories, poetry and other forms of communications. The system was expensive to operate, and when their host machine became unavailable and a new one could not be found, the system closed in January 1975.

https://en.wikipedia.org/wiki/Bulletin_board_system

- **Oracle Database - Relational database management systems (1977) - Software Development Laboratories (SDL) to Oracle Corporation**

Larry Ellison co-founded Oracle Corporation in 1977 with Bob Miner and Ed Oates under the name Software Development Laboratories (SDL).[2] Ellison took inspiration[8] from the 1970 paper written by Edgar F. Codd on relational database management systems (RDBMS) named "A Relational Model of Data for Large Shared Data Banks."[9] He heard about the IBM System R database from an article in the IBM Research Journal provided by Oates. Ellison wanted to make Oracle's product compatible with System R, but failed to do so as IBM kept the error codes for their DBMS a secret. SDL changed its name to Relational Software, Inc (RSI) in 1979,[10] then again to Oracle Systems Corporation in 1983,[11] to align itself more closely with its flagship product Oracle Database. The name also drew from the 1977 CIA project codename, which was also Oracle's first customer.[12] At this stage Bob Miner served as the company's senior programmer. On March 12, 1986, the company had its initial public offering.[13]

https://en.wikipedia.org/wiki/Oracle_Corporation

### Clients

- **Bravo - First WYSIWYG document preparation program (1974) - Xerox Parc**

It provided multi-font capability using the bitmap displays on the Xerox Alto personal computer. It was produced at Xerox PARC by Butler Lampson, Charles Simonyi and colleagues in 1974.

https://en.wikipedia.org/wiki/Bravo_(editor)

The first version of Microsoft Word was developed by Charles Simonyi and Richard Brodie, former Xerox programmers hired by Bill Gates and Paul Allen in 1981. Both programmers worked on Xerox Bravo, the first WYSIWYG (What You See Is What You Get) word processor.

https://en.wikipedia.org/wiki/History_of_Microsoft_Word

- **Visicalc - First spreadsheet (1979) - Software Arts**

With the development of the VisiCalc application in 1979, microcomputers broke out of the factory and began entering office suites in large quantities, but still through the back door.
Appeared on Apple II.

https://en.wikipedia.org/wiki/Commodity_computing

https://en.wikipedia.org/wiki/VisiCalc

https://en.wikipedia.org/wiki/Software_Arts

- **Early computer-generated imagery (CGI) - Westworld (1973)**

Computer-generated imagery (CGI) is the use of computer graphics to create or contribute to images in art, printed media, video games, simulators, computer animation and VFX in films, television programs, shorts, commercials, and videos. The images may be dynamic or static, and may be two-dimensional (2D), although the term "CGI" is most commonly used to refer to the 3-D computer graphics used for creating characters, scenes and special effects in films and television, which is described as "CGI animation".

The first feature film to make use of CGI was the 1973 film Westworld.[1] Other early films that incorporated CGI include Star Wars (1977),[1] Tron (1982),[1] Golgo 13: The Professional (1983),[2] The Last Starfighter (1984),[3] Young Sherlock Holmes (1985)[1] and Flight of the Navigator (1986).[4] The first music video to use CGI was Dire Straits' award-winning "Money for Nothing" (1985), whose success was instrumental in giving the process mainstream exposure.[5]

The evolution of CGI led to the emergence of virtual cinematography in the 1990s, where the vision of the simulated camera is not constrained by the laws of physics. Availability of CGI software and increased computer speeds have allowed individual artists and small companies to produce professional-grade films, games, and fine art from their home computers.[citation needed]

The term virtual world refers to agent-based, interactive environments, which can be created with CGI.

https://en.wikipedia.org/wiki/Computer-generated_imagery

**Westworld** was the first feature film to use digital image processing. Crichton originally went to the Jet Propulsion Laboratory in Pasadena, but after learning that two minutes of animation would take nine months and cost $200,000, he contacted John Whitney Sr., who in turn recommended his son John Whitney Jr. The latter went to Information International, Inc., where they could work at night and complete the animation both faster and much more cheaply.[18] Whitney Jr. digitally processed motion picture photography at Information International, Inc. to appear pixelized in order to portray the Gunslinger android's point of view.[4] The approximately 2 minutes and 31 seconds' worth of cinegraphic block portraiture was accomplished by color-separating (three basic color separations plus black mask) each frame of source 70 mm film images, scanning each of these elements to convert into rectangular blocks, then adding basic color according to the tone values developed.[19] The resulting coarse pixel matrix was output back to film.[20] The process was covered in the American Cinematographer article "Behind the scenes of Westworld"[21] and in a 2013 New Yorker online article.[22]

https://en.wikipedia.org/wiki/Westworld_(film)

**Information International Inc - Motion Pictures Product Group**

In the early 1960s, Information International Inc. contributed several articles by Ed Fredkin, Malcolm Pivar, and Elaine Gord, and others, in a major book on the programming language **LISP** and its applications.

Triple-I's commercially successful technology was centered around very high precision CRTs, capable of recording to film; which for a while were the publishing industry's gold standard for digital-to-film applications. The company also manufactured film scanners using special cameras fitted with photomultiplier tubes as the image sensor, for digitizing existing films and paper documents. One such successful product of theirs using their precision CRT technology was their FR-80 film recorder introduced in 1968. It was capable of recording black and white (and later color as an option) digital imagery to motion picture or still transparency film at a maximum resolution of 16384x16384, making it an ideal system for generating either Computer Output Microfilm (COM), computer-to-film negatives for making printing plates, and other computer-generated graphics.

However, Triple-I is most notable for its commercially unsuccessful ventures; a number of one-or-two of a kind systems which included CRT based computer displays used at the Stanford AI Lab, an OCR system based on PDP-10's (two were sold), and The Foonly F-1 - which was used for movie special effects.

Triple-I's work in computer animation done by the **Motion Pictures Product Group**, is probably the most notable first from Triple-I, at least if measured by the eventual success of the technology. They created some of the first computer-generated special effects for major motion pictures, and employed a number of computer graphics pioneers.

Computer animators Gary Demos and John Whitney Jr. began using equipment at Triple-I in the early 1970s for animation, including the first use of computer imaging in a feature film — the "android vision" effect in Westworld. In 1974, Demos and Whitney convinced Triple-I to establish the Motion Pictures Product Group. In 1976, they scanned and animated Peter Fonda's head for Futureworld, the first appearance of 3D computer graphics in a film. They created an early demo animation called "Adam Powers, The Juggler"; this animation was later used in Miramar's short film All Shapes and Sizes as well as referenced by Pixar's short film Red's Dream. They were also responsible for effects in the film Looker, and animation tests for films such as Close Encounters of the Third Kind and Star Wars.[5]

Circa 1976, prior to becoming an artist-in-residence at the Jet Propulsion Laboratory, pioneering computer artist David Em spent nights at Triple-I for eighteen months, learning to use their systems and create his first 3D, shaded, digital imagery.[6]

When Disney began production of the film Tron, they hired four companies to create the computer graphics — Triple-I, MAGI, Robert Abel & Associates, and Digital Effects. Triple-I and MAGI were responsible for the majority of the roughly thirty minutes of computer animation. Triple-I created the Master Control Program, the Solar Sailer, and Sark's Carrier. Whitney and Demos left before the end of work on Tron, to found Digital Productions. Partly due to their departure, Triple-I was unable to complete as much of the effects as planned, and MAGI took over some of the work.[7][8]

Triple-I sponsored the construction of the Foonly F-1, the fastest PDP-10 ever made. Jim Blinn, Frank Crow, and others developed the company's rendering software TRANEW for the Foonly. Craig Reynolds created the Actor/Scriptor Animation System (ASAS), a procedural animation language based on LISP, at the MIT Architecture Machine Group, and then at Triple-I integrated it into their Digital Scene Simulation System. Larry Malone developed 3D modeling software for the Tektronix 4014 display. Tom McMahon developed a memory-mapped thousand line RGB framebuffer for the Foonly, one of the earliest framebuffers in that class.

In 1982, the management of Triple-I decided to shut down the Motion Pictures Product Group.

https://en.wikipedia.org/wiki/Information_International,_Inc.

**NYIT Computer Graphics Lab (CGL), Lucasfilm (1974-1982)**

Pixar got its start in 1974, when New York Institute of Technology's (NYIT) founder, Alexander Schure, who was also the owner of a traditional animation studio, established the Computer Graphics Lab (CGL) and recruited computer scientists who shared his ambitions about creating the world's first computer-animated film. Edwin Catmull and Malcolm Blanchard were the first to be hired and were soon joined by Alvy Ray Smith and David DiFrancesco some months later, which were the four original members of the Computer Graphics Lab, located in a converted two-story garage acquired from the former Vanderbilt-Whitney estate.[8][9] Schure kept pouring money into the computer graphics lab, an estimated $15 million, giving the group everything they desired and driving NYIT into serious financial troubles.[10] Eventually, the group realized they needed to work in a real film studio in order to reach their goal. Francis Ford Coppola then invited Smith to his house for a three-day media conference, where Coppola and George Lucas shared their visions for the future of digital moviemaking.[11]

When Lucas approached them and offered them a job at his studio, six employees moved to Lucasfilm. During the following months, they gradually resigned from CGL, found temporary jobs for about a year to avoid making Schure suspicious, and joined the Graphics Group at Lucasfilm.[12][13] The Graphics Group, which was one-third of the Computer Division of Lucasfilm, was launched in 1979 with the hiring of Catmull from NYIT,[14] where he was in charge of the Computer Graphics Lab. He was then reunited with Smith, who also made the journey from NYIT to Lucasfilm, and was made the director of the Graphics Group. At NYIT, the researchers pioneered many of the CG foundation techniques—in particular, the invention of the alpha channel by Catmull and Smith.[15] Over the next several years, the CGL would produce a few frames of an experimental film called The Works. After moving to Lucasfilm, the team worked on creating the precursor to RenderMan, called REYES (for "renders everything you ever saw") and developed several critical technologies for CG—including particle effects and various animation tools.[16]

https://en.wikipedia.org/wiki/Pixar#Early_history










